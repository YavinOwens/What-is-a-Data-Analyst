[
  {
    "objectID": "team_wiki.html",
    "href": "team_wiki.html",
    "title": "Team Wiki",
    "section": "",
    "text": "Our development environment uses Docker containers for various services. Here’s how to access them:\n\n\n\nURL: http://localhost:8080\nPassword: andi_password\nFeatures:\n\nFull VS Code experience in browser\nPre-configured extensions\nIntegrated terminal\nGit integration\nDebugging support\n\n\n\n\n\n\nHost: localhost\nPort: 5432\nDatabase: gdpr_fines\nUsername: andi_user\nPassword: andi_password\n\n\n\n\n\nURL: http://localhost:5050\nUsername: admin@admin.com\nPassword: admin\n\n\n\n\n\nHost: localhost\nPort: 6379\nPassword: andi_password\n\n\n\n\n\n\nClone the repository:\n\ngit clone https://github.com/YavinOwens/What-is-a-Data-Analyst.git\ncd What-is-a-Data-Analyst\n\nStart the development environment:\n\ncd _DevelopmentEnvironment\ndocker-compose up -d\n\nAccess the services:\n\n\nVS Code: http://localhost:8080\nPgAdmin: http://localhost:5050\nPostgreSQL: localhost:5432\nRedis: localhost:6379\n\n\n\n\n\nVersion Control\n\nUse feature branches for new work\nWrite clear commit messages\nKeep branches up to date\nReview code before merging\n\nDocumentation\n\nUpdate documentation as you work\nUse the provided templates\nKeep documentation in sync with code\nDocument any environment changes\n\nDevelopment Workflow\n\nStart with a clear plan\nWrite tests for new features\nFollow coding standards\nReview your work before committing\n\nEnvironment Management\n\nKeep dependencies updated\nDocument environment changes\nUse virtual environments\nFollow security best practices\n\n\n\n\n\nlean_analytics/\n├── _DevelopmentEnvironment/  # Docker configuration\n├── docs/                     # Documentation and website\n├── src/                      # Source code\n└── data/                     # Data files\n\n\n\nIf you encounter port conflicts: 1. Check running containers: docker ps 2. Stop all containers: docker-compose down 3. Start containers again: docker-compose up -d\nFor database connection issues: 1. Verify PostgreSQL container is running: docker ps | grep postgres 2. Check container logs: docker logs andi_postgres 3. Ensure correct credentials are being used"
  },
  {
    "objectID": "team_wiki.html#docker-services",
    "href": "team_wiki.html#docker-services",
    "title": "Team Wiki",
    "section": "",
    "text": "Our development environment uses Docker containers for various services. Here’s how to access them:\n\n\n\nURL: http://localhost:8080\nPassword: andi_password\nFeatures:\n\nFull VS Code experience in browser\nPre-configured extensions\nIntegrated terminal\nGit integration\nDebugging support\n\n\n\n\n\n\nHost: localhost\nPort: 5432\nDatabase: gdpr_fines\nUsername: andi_user\nPassword: andi_password\n\n\n\n\n\nURL: http://localhost:5050\nUsername: admin@admin.com\nPassword: admin\n\n\n\n\n\nHost: localhost\nPort: 6379\nPassword: andi_password"
  },
  {
    "objectID": "team_wiki.html#getting-started",
    "href": "team_wiki.html#getting-started",
    "title": "Team Wiki",
    "section": "",
    "text": "Clone the repository:\n\ngit clone https://github.com/YavinOwens/What-is-a-Data-Analyst.git\ncd What-is-a-Data-Analyst\n\nStart the development environment:\n\ncd _DevelopmentEnvironment\ndocker-compose up -d\n\nAccess the services:\n\n\nVS Code: http://localhost:8080\nPgAdmin: http://localhost:5050\nPostgreSQL: localhost:5432\nRedis: localhost:6379"
  },
  {
    "objectID": "team_wiki.html#best-practices",
    "href": "team_wiki.html#best-practices",
    "title": "Team Wiki",
    "section": "",
    "text": "Version Control\n\nUse feature branches for new work\nWrite clear commit messages\nKeep branches up to date\nReview code before merging\n\nDocumentation\n\nUpdate documentation as you work\nUse the provided templates\nKeep documentation in sync with code\nDocument any environment changes\n\nDevelopment Workflow\n\nStart with a clear plan\nWrite tests for new features\nFollow coding standards\nReview your work before committing\n\nEnvironment Management\n\nKeep dependencies updated\nDocument environment changes\nUse virtual environments\nFollow security best practices"
  },
  {
    "objectID": "team_wiki.html#project-structure",
    "href": "team_wiki.html#project-structure",
    "title": "Team Wiki",
    "section": "",
    "text": "lean_analytics/\n├── _DevelopmentEnvironment/  # Docker configuration\n├── docs/                     # Documentation and website\n├── src/                      # Source code\n└── data/                     # Data files"
  },
  {
    "objectID": "team_wiki.html#troubleshooting",
    "href": "team_wiki.html#troubleshooting",
    "title": "Team Wiki",
    "section": "",
    "text": "If you encounter port conflicts: 1. Check running containers: docker ps 2. Stop all containers: docker-compose down 3. Start containers again: docker-compose up -d\nFor database connection issues: 1. Verify PostgreSQL container is running: docker ps | grep postgres 2. Check container logs: docker logs andi_postgres 3. Ensure correct credentials are being used"
  },
  {
    "objectID": "analysis_notebooks.html",
    "href": "analysis_notebooks.html",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "This document provides templates for standardized analysis notebooks in the GDPR Fines Analysis project.\n\n\n# ---\n# title: \"Exploratory Data Analysis: [Dataset Name]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Exploratory Data Analysis: [Dataset Name]\n# \n# ## Objective\n# \n# [Brief description of the analysis objectives]\n# \n# ## Data Description\n# \n# - **Source**: [Where the data comes from]\n# - **Time Period**: [Time period covered by the data]\n# - **Size**: [Number of records, file size]\n# - **Format**: [File format, e.g., CSV, JSON, database]\n\n# %% [markdown]\n# ## Setup and Configuration\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom datetime import datetime\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('Set2')\n\n# %% [markdown]\n# ## Data Loading\n\n# %%\n# Load the data\ndf = pd.read_csv('path/to/data.csv')\n\n# Display basic information\nprint(f\"Data Shape: {df.shape}\")\nprint(\"\\nData Types:\")\nprint(df.dtypes)\nprint(\"\\nMemory Usage:\")\nprint(df.memory_usage(deep=True))\n\n# %% [markdown]\n# ## Data Overview\n# \n# Let's look at the first few rows to understand the structure.\n\n# %%\n# Display sample data\ndf.head()\n\n# %% [markdown]\n# ## Data Quality Assessment\n# \n# Let's check for missing values, duplicates, and other quality issues.\n\n# %%\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_pct = (missing_values / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_pct\n})\nmissing_df = missing_df[missing_df['Missing Values'] &gt; 0].sort_values('Percentage', ascending=False)\n\nif not missing_df.empty:\n    print(\"Columns with missing values:\")\n    display(missing_df)\nelse:\n    print(\"No missing values found.\")\n\n# %%\n# Check for duplicates\nduplicate_count = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count} ({duplicate_count/len(df):.2%} of total)\")\n\n# %%\n# Check for unusual values or outliers\nnumeric_columns = df.select_dtypes(include=['number']).columns\nfor column in numeric_columns:\n    stats = df[column].describe()\n    print(f\"\\nSummary statistics for {column}:\")\n    display(stats)\n    \n    # Box plot for outlier visualization\n    plt.figure(figsize=(10, 4))\n    sns.boxplot(x=df[column])\n    plt.title(f'Box plot of {column}')\n    plt.show()\n\n# %% [markdown]\n# ## Univariate Analysis\n# \n# Let's analyze each variable independently.\n\n# %%\n# Categorical variables analysis\ncategorical_columns = df.select_dtypes(include=['object', 'category']).columns\nfor column in categorical_columns:\n    value_counts = df[column].value_counts()\n    value_pct = df[column].value_counts(normalize=True) * 100\n    \n    print(f\"\\nValue counts for {column}:\")\n    display(pd.DataFrame({\n        'Count': value_counts,\n        'Percentage': value_pct\n    }).head(10))\n    \n    # Bar chart\n    plt.figure(figsize=(12, 6))\n    sns.countplot(y=column, data=df, order=value_counts.index[:10])\n    plt.title(f'Count of {column} (Top 10)')\n    plt.xlabel('Count')\n    plt.ylabel(column)\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Numerical variables analysis\nfor column in numeric_columns:\n    plt.figure(figsize=(12, 6))\n    \n    # Histogram and KDE\n    plt.subplot(1, 2, 1)\n    sns.histplot(df[column], kde=True)\n    plt.title(f'Distribution of {column}')\n    \n    # Cumulative distribution\n    plt.subplot(1, 2, 2)\n    sns.ecdfplot(df[column])\n    plt.title(f'Cumulative Distribution of {column}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# %% [markdown]\n# ## Bivariate Analysis\n# \n# Let's explore relationships between pairs of variables.\n\n# %%\n# Correlation matrix for numerical variables\nif len(numeric_columns) &gt; 1:\n    corr_matrix = df[numeric_columns].corr()\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Categorical vs. Numerical variables\nif len(categorical_columns) &gt; 0 and len(numeric_columns) &gt; 0:\n    for cat_col in categorical_columns[:3]:  # Limit to first 3 categorical columns to avoid too many plots\n        top_categories = df[cat_col].value_counts().head(5).index  # Top 5 categories\n        for num_col in numeric_columns[:3]:  # Limit to first 3 numerical columns\n            plt.figure(figsize=(12, 6))\n            \n            # Box plot\n            sns.boxplot(x=cat_col, y=num_col, data=df[df[cat_col].isin(top_categories)])\n            plt.title(f'{num_col} by {cat_col} (Top 5 categories)')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.show()\n\n# %% [markdown]\n# ## Time Series Analysis (if applicable)\n# \n# If the data has a time component, let's analyze trends over time.\n\n# %%\n# Check if there's a date column\ndate_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n\nif date_columns:\n    date_col = date_columns[0]  # Use the first date column found\n    \n    # Convert to datetime if not already\n    if df[date_col].dtype != 'datetime64[ns]':\n        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n    \n    # Set time period for aggregation (e.g., monthly)\n    df['period'] = df[date_col].dt.to_period('M')\n    \n    # Aggregate by time period\n    for num_col in numeric_columns[:3]:  # First 3 numeric columns\n        time_series = df.groupby('period')[num_col].agg(['mean', 'median', 'sum', 'count'])\n        \n        plt.figure(figsize=(15, 6))\n        time_series['sum'].plot(kind='line')\n        plt.title(f'Sum of {num_col} Over Time')\n        plt.xlabel('Time Period')\n        plt.ylabel(f'Sum of {num_col}')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\n# %% [markdown]\n# ## Multivariate Analysis\n# \n# Let's explore relationships between multiple variables.\n\n# %%\n# Scatter plot matrix for numerical variables\nif len(numeric_columns) &gt;= 2:\n    selected_num_cols = numeric_columns[:4]  # Limit to first 4 numerical columns\n    sns.pairplot(df[selected_num_cols], diag_kind='kde')\n    plt.suptitle('Scatter Plot Matrix', y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Multivariate analysis with categorical variables\nif len(categorical_columns) &gt; 0 and len(numeric_columns) &gt; 1:\n    cat_col = categorical_columns[0]  # Choose first categorical column\n    num_col1 = numeric_columns[0]  # First numerical column\n    num_col2 = numeric_columns[1]  # Second numerical column\n    \n    top_categories = df[cat_col].value_counts().head(5).index  # Top 5 categories\n    \n    plt.figure(figsize=(12, 8))\n    sns.scatterplot(x=num_col1, y=num_col2, hue=cat_col, data=df[df[cat_col].isin(top_categories)])\n    plt.title(f'Scatter Plot of {num_col2} vs {num_col1} by {cat_col}')\n    plt.tight_layout()\n    plt.show()\n\n# %% [markdown]\n# ## Key Insights\n# \n# [Document key findings from the exploratory analysis]\n# \n# 1. [Insight 1]\n# 2. [Insight 2]\n# 3. [Insight 3]\n# \n# ## Next Steps\n# \n# [Document recommended actions based on the analysis]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]\n\n\n\n# ---\n# title: \"Data Cleaning and Preprocessing: [Dataset Name]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Data Cleaning and Preprocessing: [Dataset Name]\n# \n# ## Objective\n# \n# [Brief description of the cleaning and preprocessing objectives]\n# \n# ## Data Source\n# \n# - **Source**: [Where the data comes from]\n# - **Raw Data Path**: [Path to raw data]\n# - **Output Path**: [Path where processed data will be saved]\n\n# %% [markdown]\n# ## Setup and Configuration\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport re\nimport os\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# %% [markdown]\n# ## Data Loading\n\n# %%\n# Load the raw data\nraw_data_path = 'path/to/raw_data.csv'\ndf_raw = pd.read_csv(raw_data_path)\n\n# Display basic information\nprint(f\"Raw Data Shape: {df_raw.shape}\")\nprint(\"\\nData Types:\")\nprint(df_raw.dtypes)\n\n# Make a copy for processing\ndf = df_raw.copy()\n\n# %% [markdown]\n# ## Data Quality Assessment\n# \n# Let's assess the quality of the raw data and identify issues that need to be addressed.\n\n# %%\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_pct = (missing_values / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_pct\n})\nmissing_df = missing_df[missing_df['Missing Values'] &gt; 0].sort_values('Percentage', ascending=False)\n\nif not missing_df.empty:\n    print(\"Columns with missing values:\")\n    display(missing_df)\nelse:\n    print(\"No missing values found.\")\n\n# %%\n# Check for duplicates\nduplicate_count = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count} ({duplicate_count/len(df):.2%} of total)\")\n\n# %%\n# Check for unusual values or outliers\nnumeric_columns = df.select_dtypes(include=['number']).columns\nfor column in numeric_columns:\n    stats = df[column].describe()\n    print(f\"\\nSummary statistics for {column}:\")\n    display(stats)\n\n# %% [markdown]\n# ## Step 1: Handling Missing Values\n\n# %%\n# Define handling strategy for each column with missing values\n# Example strategies:\n# 1. Drop rows with missing values in critical columns\n# 2. Fill missing values with mean/median/mode\n# 3. Use more sophisticated imputation methods\n\n# Strategy 1: Drop rows with missing values in critical columns\ncritical_columns = []  # List critical columns here\nif critical_columns:\n    df_clean = df.dropna(subset=critical_columns)\n    print(f\"Rows remaining after dropping missing values in critical columns: {len(df_clean)} ({len(df_clean)/len(df):.2%} of original)\")\nelse:\n    df_clean = df.copy()\n\n# Strategy 2: Impute missing values in numerical columns\nnumeric_columns_with_missing = [col for col in numeric_columns if df_clean[col].isnull().sum() &gt; 0]\nif numeric_columns_with_missing:\n    # Simple imputation with median\n    for col in numeric_columns_with_missing:\n        median_value = df_clean[col].median()\n        df_clean[col] = df_clean[col].fillna(median_value)\n        print(f\"Filled missing values in {col} with median: {median_value:.2f}\")\n\n# Strategy 3: Impute missing values in categorical columns\ncategorical_columns = df_clean.select_dtypes(include=['object', 'category']).columns\ncategorical_columns_with_missing = [col for col in categorical_columns if df_clean[col].isnull().sum() &gt; 0]\nif categorical_columns_with_missing:\n    # Simple imputation with mode\n    for col in categorical_columns_with_missing:\n        mode_value = df_clean[col].mode()[0]\n        df_clean[col] = df_clean[col].fillna(mode_value)\n        print(f\"Filled missing values in {col} with mode: {mode_value}\")\n\n# Verify all missing values are handled\nmissing_after = df_clean.isnull().sum().sum()\nprint(f\"\\nTotal missing values remaining: {missing_after}\")\n\n# %% [markdown]\n# ## Step 2: Handling Duplicates\n\n# %%\n# Check for duplicates after handling missing values\nduplicate_count_after = df_clean.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count_after} ({duplicate_count_after/len(df_clean):.2%} of total)\")\n\n# Remove duplicates if any\nif duplicate_count_after &gt; 0:\n    df_clean = df_clean.drop_duplicates()\n    print(f\"Rows remaining after removing duplicates: {len(df_clean)}\")\n\n# %% [markdown]\n# ## Step 3: Data Type Conversions\n\n# %%\n# Convert data types as needed\n# Example conversions:\n\n# Convert date columns to datetime\ndate_columns = []  # List date columns here\nfor col in date_columns:\n    df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n    print(f\"Converted {col} to datetime type\")\n\n# Convert categorical columns to category type\nfor col in categorical_columns:\n    df_clean[col] = df_clean[col].astype('category')\n    print(f\"Converted {col} to category type\")\n\n# %% [markdown]\n# ## Step 4: Handling Outliers\n\n# %%\n# Identify and handle outliers in numerical columns\nfor col in numeric_columns:\n    # Calculate IQR\n    Q1 = df_clean[col].quantile(0.25)\n    Q3 = df_clean[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    # Define bounds\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # Identify outliers\n    outliers = df_clean[(df_clean[col] &lt; lower_bound) | (df_clean[col] &gt; upper_bound)]\n    outlier_count = len(outliers)\n    \n    if outlier_count &gt; 0:\n        print(f\"\\nOutliers in {col}: {outlier_count} ({outlier_count/len(df_clean):.2%} of data)\")\n        \n        # Strategy options:\n        # 1. Cap outliers at bounds (winsorization)\n        # 2. Remove outliers\n        # 3. Transform data (e.g., log transformation)\n        # 4. Keep outliers if they are valid data points\n        \n        # Example: Winsorization\n        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n        print(f\"Applied winsorization to {col}\")\n\n# %% [markdown]\n# ## Step 5: Feature Engineering\n\n# %%\n# Create new features based on existing ones\n# Examples:\n\n# 1. Extract components from date fields\nif date_columns:\n    for col in date_columns:\n        df_clean[f'{col}_year'] = df_clean[col].dt.year\n        df_clean[f'{col}_month'] = df_clean[col].dt.month\n        df_clean[f'{col}_day'] = df_clean[col].dt.day\n        df_clean[f'{col}_dayofweek'] = df_clean[col].dt.dayofweek\n        print(f\"Created date components from {col}\")\n\n# 2. Create interaction features\n# Example: Multiply two numeric features\nif len(numeric_columns) &gt;= 2:\n    col1 = numeric_columns[0]\n    col2 = numeric_columns[1]\n    df_clean[f'{col1}_{col2}_interaction'] = df_clean[col1] * df_clean[col2]\n    print(f\"Created interaction feature between {col1} and {col2}\")\n\n# 3. Binning numerical features\n# Example: Convert a continuous feature into categorical bins\nif len(numeric_columns) &gt; 0:\n    col = numeric_columns[0]\n    df_clean[f'{col}_binned'] = pd.qcut(df_clean[col], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n    print(f\"Created binned version of {col}\")\n\n# %% [markdown]\n# ## Step 6: Feature Scaling and Encoding\n\n# %%\n# Define preprocessing pipeline for numerical and categorical features\n\n# Numerical features preprocessing\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical features preprocessing\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ])\n\n# Create example of preprocessed data for demonstration\n# In a real workflow, you might save the preprocessor to use with new data\nX = df_clean[list(numeric_columns) + list(categorical_columns)]\ntry:\n    preprocessor.fit(X)\n    print(\"Preprocessing pipeline successfully fitted.\")\nexcept Exception as e:\n    print(f\"Error fitting preprocessing pipeline: {str(e)}\")\n\n# %% [markdown]\n# ## Step 7: Final Quality Checks\n\n# %%\n# Perform final data quality checks\n\n# Check for missing values\nfinal_missing = df_clean.isnull().sum().sum()\nprint(f\"Final missing values: {final_missing}\")\n\n# Check data shape\nprint(f\"Original data shape: {df_raw.shape}\")\nprint(f\"Clean data shape: {df_clean.shape}\")\nprint(f\"Rows retained: {len(df_clean)/len(df_raw):.2%}\")\n\n# Check column data types\nprint(\"\\nFinal data types:\")\nprint(df_clean.dtypes)\n\n# Sample of final clean data\nprint(\"\\nSample of clean data:\")\ndisplay(df_clean.head())\n\n# %% [markdown]\n# ## Step 8: Save Processed Data\n\n# %%\n# Save the cleaned and preprocessed data\noutput_path = 'path/to/processed_data.csv'\ndf_clean.to_csv(output_path, index=False)\nprint(f\"Saved processed data to {output_path}\")\n\n# %% [markdown]\n# ## Summary of Processing Steps\n# \n# 1. **Loading and Initial Assessment**:\n#    - Loaded raw data with shape {df_raw.shape}\n#    - Identified {missing_df.shape[0]} columns with missing values\n#    - Found {duplicate_count} duplicate rows\n# \n# 2. **Handling Missing Values**:\n#    - [List strategies used for handling missing values]\n# \n# 3. **Removing Duplicates**:\n#    - Removed {duplicate_count} duplicate rows\n# \n# 4. **Data Type Conversions**:\n#    - [List data type conversions performed]\n# \n# 5. **Handling Outliers**:\n#    - [List outlier handling strategies]\n# \n# 6. **Feature Engineering**:\n#    - [List new features created]\n# \n# 7. **Feature Scaling and Encoding**:\n#    - [List preprocessing steps applied]\n# \n# 8. **Final Dataset**:\n#    - Final shape: {df_clean.shape}\n#    - Retention rate: {len(df_clean)/len(df_raw):.2%}\n# \n# ## Next Steps\n# \n# [Document recommended actions for using this processed data]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]\n\n\n\n# ---\n# title: \"Analysis Report: [Analysis Title]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Analysis Report: [Analysis Title]\n# \n# ## Executive Summary\n# \n# [Brief summary of the analysis, key findings, and recommendations (2-3 paragraphs)]\n# \n# **Key Findings:**\n# \n# 1. [Key finding 1]\n# 2. [Key finding 2]\n# 3. [Key finding 3]\n# \n# **Recommendations:**\n# \n# 1. [Recommendation 1]\n# 2. [Recommendation 2]\n# 3. [Recommendation 3]\n\n# %% [markdown]\n# ## 1. Introduction\n# \n# ### 1.1 Background\n# \n# [Context and background information]\n# \n# ### 1.2 Objectives\n# \n# [Specific objectives of this analysis]\n# \n# ### 1.3 Research Questions\n# \n# 1. [Research question 1]\n# 2. [Research question 2]\n# 3. [Research question 3]\n\n# %% [markdown]\n# ## 2. Methodology\n# \n# ### 2.1 Data Sources\n# \n# [Description of data sources used]\n# \n# | Dataset | Source | Time Period | Size | Description |\n# |---------|--------|-------------|------|-------------|\n# | [Dataset 1] | [Source] | [Time Period] | [Size] | [Description] |\n# | [Dataset 2] | [Source] | [Time Period] | [Size] | [Description] |\n# \n# ### 2.2 Analysis Approach\n# \n# [Description of the analytical approach, including methods, models, and techniques used]\n# \n# ### 2.3 Tools and Technologies\n# \n# - Python [version]\n# - pandas [version]\n# - scikit-learn [version]\n# - [Other tools/libraries]\n\n# %% [markdown]\n# ## 3. Data Preparation\n# \n# ### 3.1 Data Cleaning\n# \n# [Summary of data cleaning steps]\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Load the cleaned data\ndf = pd.read_csv('path/to/cleaned_data.csv')\n\n# Display basic information\nprint(f\"Data Shape: {df.shape}\")\nprint(\"\\nData Types:\")\nprint(df.dtypes)\n\n# Display sample data\ndf.head()\n\n# %% [markdown]\n# ### 3.2 Feature Engineering\n# \n# [Description of features created or transformed]\n\n# %%\n# Display the engineered features\nengineered_features = ['feature_1', 'feature_2', 'feature_3']  # List your engineered features\ndf[engineered_features].head()\n\n# %% [markdown]\n# ## 4. Analysis Results\n# \n# ### 4.1 Descriptive Statistics\n# \n# [Summary of descriptive statistics]\n\n# %%\n# Display summary statistics\ndf.describe()\n\n# %%\n# Visualize distribution of key variables\nkey_variables = ['variable_1', 'variable_2', 'variable_3']  # List your key variables\n\nfor var in key_variables:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[var], kde=True)\n    plt.title(f'Distribution of {var}')\n    plt.show()\n\n# %% [markdown]\n# ### 4.2 Key Finding 1: [Title]\n# \n# [Detailed explanation of finding 1]\n\n# %%\n# Visualization supporting finding 1\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 1')\nplt.show()\n\n# %% [markdown]\n# ### 4.3 Key Finding 2: [Title]\n# \n# [Detailed explanation of finding 2]\n\n# %%\n# Visualization supporting finding 2\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 2')\nplt.show()\n\n# %% [markdown]\n# ### 4.4 Key Finding 3: [Title]\n# \n# [Detailed explanation of finding 3]\n\n# %%\n# Visualization supporting finding 3\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 3')\nplt.show()\n\n# %% [markdown]\n# ## 5. Discussion\n# \n# ### 5.1 Interpretation of Results\n# \n# [Interpretation of the analysis results in the context of the research questions]\n# \n# ### 5.2 Limitations\n# \n# [Discussion of limitations of the analysis]\n# \n# ### 5.3 Implications\n# \n# [Discussion of implications of the findings]\n\n# %% [markdown]\n# ## 6. Recommendations\n# \n# ### 6.1 Recommendation 1: [Title]\n# \n# [Detailed explanation of recommendation 1, including implementation considerations]\n# \n# ### 6.2 Recommendation 2: [Title]\n# \n# [Detailed explanation of recommendation 2, including implementation considerations]\n# \n# ### 6.3 Recommendation 3: [Title]\n# \n# [Detailed explanation of recommendation 3, including implementation considerations]\n\n# %% [markdown]\n# ## 7. Conclusion\n# \n# [Summary of the analysis, findings, and recommendations]\n# \n# ## 8. Next Steps\n# \n# [Suggested next steps for further analysis or implementation]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]\n\n# %% [markdown]\n# ## Appendix\n# \n# ### A. Additional Visualizations\n# \n# [Additional visualizations supporting the analysis]\n\n# %%\n# Additional visualization 1\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Additional Visualization 1')\nplt.show()\n\n# %% [markdown]\n# ### B. Technical Details\n# \n# [Technical details of the analysis, such as model parameters, validation methods, etc.]\n# \n# ### C. References\n# \n# [References to data sources, methodologies, or other resources used]\n\n\n\nThese notebook templates are designed to provide a standardized structure for data analysis work. To use them effectively:\n\nCopy the Template: Select the appropriate template based on your analysis needs.\nCustomize the Template: Replace placeholders with your specific information.\nDocument as You Go: Keep documentation current as you perform your analysis.\nInclude Visualizations: Add relevant visualizations to support your findings.\nSave in Standard Format: Save your notebook in a standard format (e.g., Jupyter Notebook, Quarto Document).\nVersion Control: Commit your notebook to version control for tracking changes.\n\n\n\n\n\nStructure and Organization:\n\nUse clear section headings\nInclude a table of contents\nNumber sections and subsections\n\nDocumentation:\n\nExplain your reasoning for key decisions\nDocument assumptions and limitations\nInclude sources of data and methodologies\n\nCode Quality:\n\nFollow the project’s coding standards\nUse comments to explain complex code\nOptimize for readability\n\nReproducibility:\n\nInclude all necessary import statements\nUse relative paths for file references\nDocument environment requirements\n\nVisualizations:\n\nUse appropriate chart types for your data\nInclude clear titles and labels\nConsider colorblind-friendly color palettes\n\nConclusion and Next Steps:\n\nSummarize key findings\nPropose actionable recommendations\nSuggest areas for further investigation"
  },
  {
    "objectID": "analysis_notebooks.html#exploratory-data-analysis-eda-template",
    "href": "analysis_notebooks.html#exploratory-data-analysis-eda-template",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "# ---\n# title: \"Exploratory Data Analysis: [Dataset Name]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Exploratory Data Analysis: [Dataset Name]\n# \n# ## Objective\n# \n# [Brief description of the analysis objectives]\n# \n# ## Data Description\n# \n# - **Source**: [Where the data comes from]\n# - **Time Period**: [Time period covered by the data]\n# - **Size**: [Number of records, file size]\n# - **Format**: [File format, e.g., CSV, JSON, database]\n\n# %% [markdown]\n# ## Setup and Configuration\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom datetime import datetime\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('Set2')\n\n# %% [markdown]\n# ## Data Loading\n\n# %%\n# Load the data\ndf = pd.read_csv('path/to/data.csv')\n\n# Display basic information\nprint(f\"Data Shape: {df.shape}\")\nprint(\"\\nData Types:\")\nprint(df.dtypes)\nprint(\"\\nMemory Usage:\")\nprint(df.memory_usage(deep=True))\n\n# %% [markdown]\n# ## Data Overview\n# \n# Let's look at the first few rows to understand the structure.\n\n# %%\n# Display sample data\ndf.head()\n\n# %% [markdown]\n# ## Data Quality Assessment\n# \n# Let's check for missing values, duplicates, and other quality issues.\n\n# %%\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_pct = (missing_values / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_pct\n})\nmissing_df = missing_df[missing_df['Missing Values'] &gt; 0].sort_values('Percentage', ascending=False)\n\nif not missing_df.empty:\n    print(\"Columns with missing values:\")\n    display(missing_df)\nelse:\n    print(\"No missing values found.\")\n\n# %%\n# Check for duplicates\nduplicate_count = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count} ({duplicate_count/len(df):.2%} of total)\")\n\n# %%\n# Check for unusual values or outliers\nnumeric_columns = df.select_dtypes(include=['number']).columns\nfor column in numeric_columns:\n    stats = df[column].describe()\n    print(f\"\\nSummary statistics for {column}:\")\n    display(stats)\n    \n    # Box plot for outlier visualization\n    plt.figure(figsize=(10, 4))\n    sns.boxplot(x=df[column])\n    plt.title(f'Box plot of {column}')\n    plt.show()\n\n# %% [markdown]\n# ## Univariate Analysis\n# \n# Let's analyze each variable independently.\n\n# %%\n# Categorical variables analysis\ncategorical_columns = df.select_dtypes(include=['object', 'category']).columns\nfor column in categorical_columns:\n    value_counts = df[column].value_counts()\n    value_pct = df[column].value_counts(normalize=True) * 100\n    \n    print(f\"\\nValue counts for {column}:\")\n    display(pd.DataFrame({\n        'Count': value_counts,\n        'Percentage': value_pct\n    }).head(10))\n    \n    # Bar chart\n    plt.figure(figsize=(12, 6))\n    sns.countplot(y=column, data=df, order=value_counts.index[:10])\n    plt.title(f'Count of {column} (Top 10)')\n    plt.xlabel('Count')\n    plt.ylabel(column)\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Numerical variables analysis\nfor column in numeric_columns:\n    plt.figure(figsize=(12, 6))\n    \n    # Histogram and KDE\n    plt.subplot(1, 2, 1)\n    sns.histplot(df[column], kde=True)\n    plt.title(f'Distribution of {column}')\n    \n    # Cumulative distribution\n    plt.subplot(1, 2, 2)\n    sns.ecdfplot(df[column])\n    plt.title(f'Cumulative Distribution of {column}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# %% [markdown]\n# ## Bivariate Analysis\n# \n# Let's explore relationships between pairs of variables.\n\n# %%\n# Correlation matrix for numerical variables\nif len(numeric_columns) &gt; 1:\n    corr_matrix = df[numeric_columns].corr()\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Categorical vs. Numerical variables\nif len(categorical_columns) &gt; 0 and len(numeric_columns) &gt; 0:\n    for cat_col in categorical_columns[:3]:  # Limit to first 3 categorical columns to avoid too many plots\n        top_categories = df[cat_col].value_counts().head(5).index  # Top 5 categories\n        for num_col in numeric_columns[:3]:  # Limit to first 3 numerical columns\n            plt.figure(figsize=(12, 6))\n            \n            # Box plot\n            sns.boxplot(x=cat_col, y=num_col, data=df[df[cat_col].isin(top_categories)])\n            plt.title(f'{num_col} by {cat_col} (Top 5 categories)')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.show()\n\n# %% [markdown]\n# ## Time Series Analysis (if applicable)\n# \n# If the data has a time component, let's analyze trends over time.\n\n# %%\n# Check if there's a date column\ndate_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n\nif date_columns:\n    date_col = date_columns[0]  # Use the first date column found\n    \n    # Convert to datetime if not already\n    if df[date_col].dtype != 'datetime64[ns]':\n        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n    \n    # Set time period for aggregation (e.g., monthly)\n    df['period'] = df[date_col].dt.to_period('M')\n    \n    # Aggregate by time period\n    for num_col in numeric_columns[:3]:  # First 3 numeric columns\n        time_series = df.groupby('period')[num_col].agg(['mean', 'median', 'sum', 'count'])\n        \n        plt.figure(figsize=(15, 6))\n        time_series['sum'].plot(kind='line')\n        plt.title(f'Sum of {num_col} Over Time')\n        plt.xlabel('Time Period')\n        plt.ylabel(f'Sum of {num_col}')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\n# %% [markdown]\n# ## Multivariate Analysis\n# \n# Let's explore relationships between multiple variables.\n\n# %%\n# Scatter plot matrix for numerical variables\nif len(numeric_columns) &gt;= 2:\n    selected_num_cols = numeric_columns[:4]  # Limit to first 4 numerical columns\n    sns.pairplot(df[selected_num_cols], diag_kind='kde')\n    plt.suptitle('Scatter Plot Matrix', y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n# %%\n# Multivariate analysis with categorical variables\nif len(categorical_columns) &gt; 0 and len(numeric_columns) &gt; 1:\n    cat_col = categorical_columns[0]  # Choose first categorical column\n    num_col1 = numeric_columns[0]  # First numerical column\n    num_col2 = numeric_columns[1]  # Second numerical column\n    \n    top_categories = df[cat_col].value_counts().head(5).index  # Top 5 categories\n    \n    plt.figure(figsize=(12, 8))\n    sns.scatterplot(x=num_col1, y=num_col2, hue=cat_col, data=df[df[cat_col].isin(top_categories)])\n    plt.title(f'Scatter Plot of {num_col2} vs {num_col1} by {cat_col}')\n    plt.tight_layout()\n    plt.show()\n\n# %% [markdown]\n# ## Key Insights\n# \n# [Document key findings from the exploratory analysis]\n# \n# 1. [Insight 1]\n# 2. [Insight 2]\n# 3. [Insight 3]\n# \n# ## Next Steps\n# \n# [Document recommended actions based on the analysis]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]"
  },
  {
    "objectID": "analysis_notebooks.html#data-cleaning-and-preprocessing-template",
    "href": "analysis_notebooks.html#data-cleaning-and-preprocessing-template",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "# ---\n# title: \"Data Cleaning and Preprocessing: [Dataset Name]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Data Cleaning and Preprocessing: [Dataset Name]\n# \n# ## Objective\n# \n# [Brief description of the cleaning and preprocessing objectives]\n# \n# ## Data Source\n# \n# - **Source**: [Where the data comes from]\n# - **Raw Data Path**: [Path to raw data]\n# - **Output Path**: [Path where processed data will be saved]\n\n# %% [markdown]\n# ## Setup and Configuration\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport re\nimport os\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# %% [markdown]\n# ## Data Loading\n\n# %%\n# Load the raw data\nraw_data_path = 'path/to/raw_data.csv'\ndf_raw = pd.read_csv(raw_data_path)\n\n# Display basic information\nprint(f\"Raw Data Shape: {df_raw.shape}\")\nprint(\"\\nData Types:\")\nprint(df_raw.dtypes)\n\n# Make a copy for processing\ndf = df_raw.copy()\n\n# %% [markdown]\n# ## Data Quality Assessment\n# \n# Let's assess the quality of the raw data and identify issues that need to be addressed.\n\n# %%\n# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_pct = (missing_values / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_pct\n})\nmissing_df = missing_df[missing_df['Missing Values'] &gt; 0].sort_values('Percentage', ascending=False)\n\nif not missing_df.empty:\n    print(\"Columns with missing values:\")\n    display(missing_df)\nelse:\n    print(\"No missing values found.\")\n\n# %%\n# Check for duplicates\nduplicate_count = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count} ({duplicate_count/len(df):.2%} of total)\")\n\n# %%\n# Check for unusual values or outliers\nnumeric_columns = df.select_dtypes(include=['number']).columns\nfor column in numeric_columns:\n    stats = df[column].describe()\n    print(f\"\\nSummary statistics for {column}:\")\n    display(stats)\n\n# %% [markdown]\n# ## Step 1: Handling Missing Values\n\n# %%\n# Define handling strategy for each column with missing values\n# Example strategies:\n# 1. Drop rows with missing values in critical columns\n# 2. Fill missing values with mean/median/mode\n# 3. Use more sophisticated imputation methods\n\n# Strategy 1: Drop rows with missing values in critical columns\ncritical_columns = []  # List critical columns here\nif critical_columns:\n    df_clean = df.dropna(subset=critical_columns)\n    print(f\"Rows remaining after dropping missing values in critical columns: {len(df_clean)} ({len(df_clean)/len(df):.2%} of original)\")\nelse:\n    df_clean = df.copy()\n\n# Strategy 2: Impute missing values in numerical columns\nnumeric_columns_with_missing = [col for col in numeric_columns if df_clean[col].isnull().sum() &gt; 0]\nif numeric_columns_with_missing:\n    # Simple imputation with median\n    for col in numeric_columns_with_missing:\n        median_value = df_clean[col].median()\n        df_clean[col] = df_clean[col].fillna(median_value)\n        print(f\"Filled missing values in {col} with median: {median_value:.2f}\")\n\n# Strategy 3: Impute missing values in categorical columns\ncategorical_columns = df_clean.select_dtypes(include=['object', 'category']).columns\ncategorical_columns_with_missing = [col for col in categorical_columns if df_clean[col].isnull().sum() &gt; 0]\nif categorical_columns_with_missing:\n    # Simple imputation with mode\n    for col in categorical_columns_with_missing:\n        mode_value = df_clean[col].mode()[0]\n        df_clean[col] = df_clean[col].fillna(mode_value)\n        print(f\"Filled missing values in {col} with mode: {mode_value}\")\n\n# Verify all missing values are handled\nmissing_after = df_clean.isnull().sum().sum()\nprint(f\"\\nTotal missing values remaining: {missing_after}\")\n\n# %% [markdown]\n# ## Step 2: Handling Duplicates\n\n# %%\n# Check for duplicates after handling missing values\nduplicate_count_after = df_clean.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count_after} ({duplicate_count_after/len(df_clean):.2%} of total)\")\n\n# Remove duplicates if any\nif duplicate_count_after &gt; 0:\n    df_clean = df_clean.drop_duplicates()\n    print(f\"Rows remaining after removing duplicates: {len(df_clean)}\")\n\n# %% [markdown]\n# ## Step 3: Data Type Conversions\n\n# %%\n# Convert data types as needed\n# Example conversions:\n\n# Convert date columns to datetime\ndate_columns = []  # List date columns here\nfor col in date_columns:\n    df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n    print(f\"Converted {col} to datetime type\")\n\n# Convert categorical columns to category type\nfor col in categorical_columns:\n    df_clean[col] = df_clean[col].astype('category')\n    print(f\"Converted {col} to category type\")\n\n# %% [markdown]\n# ## Step 4: Handling Outliers\n\n# %%\n# Identify and handle outliers in numerical columns\nfor col in numeric_columns:\n    # Calculate IQR\n    Q1 = df_clean[col].quantile(0.25)\n    Q3 = df_clean[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    # Define bounds\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # Identify outliers\n    outliers = df_clean[(df_clean[col] &lt; lower_bound) | (df_clean[col] &gt; upper_bound)]\n    outlier_count = len(outliers)\n    \n    if outlier_count &gt; 0:\n        print(f\"\\nOutliers in {col}: {outlier_count} ({outlier_count/len(df_clean):.2%} of data)\")\n        \n        # Strategy options:\n        # 1. Cap outliers at bounds (winsorization)\n        # 2. Remove outliers\n        # 3. Transform data (e.g., log transformation)\n        # 4. Keep outliers if they are valid data points\n        \n        # Example: Winsorization\n        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n        print(f\"Applied winsorization to {col}\")\n\n# %% [markdown]\n# ## Step 5: Feature Engineering\n\n# %%\n# Create new features based on existing ones\n# Examples:\n\n# 1. Extract components from date fields\nif date_columns:\n    for col in date_columns:\n        df_clean[f'{col}_year'] = df_clean[col].dt.year\n        df_clean[f'{col}_month'] = df_clean[col].dt.month\n        df_clean[f'{col}_day'] = df_clean[col].dt.day\n        df_clean[f'{col}_dayofweek'] = df_clean[col].dt.dayofweek\n        print(f\"Created date components from {col}\")\n\n# 2. Create interaction features\n# Example: Multiply two numeric features\nif len(numeric_columns) &gt;= 2:\n    col1 = numeric_columns[0]\n    col2 = numeric_columns[1]\n    df_clean[f'{col1}_{col2}_interaction'] = df_clean[col1] * df_clean[col2]\n    print(f\"Created interaction feature between {col1} and {col2}\")\n\n# 3. Binning numerical features\n# Example: Convert a continuous feature into categorical bins\nif len(numeric_columns) &gt; 0:\n    col = numeric_columns[0]\n    df_clean[f'{col}_binned'] = pd.qcut(df_clean[col], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n    print(f\"Created binned version of {col}\")\n\n# %% [markdown]\n# ## Step 6: Feature Scaling and Encoding\n\n# %%\n# Define preprocessing pipeline for numerical and categorical features\n\n# Numerical features preprocessing\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical features preprocessing\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ])\n\n# Create example of preprocessed data for demonstration\n# In a real workflow, you might save the preprocessor to use with new data\nX = df_clean[list(numeric_columns) + list(categorical_columns)]\ntry:\n    preprocessor.fit(X)\n    print(\"Preprocessing pipeline successfully fitted.\")\nexcept Exception as e:\n    print(f\"Error fitting preprocessing pipeline: {str(e)}\")\n\n# %% [markdown]\n# ## Step 7: Final Quality Checks\n\n# %%\n# Perform final data quality checks\n\n# Check for missing values\nfinal_missing = df_clean.isnull().sum().sum()\nprint(f\"Final missing values: {final_missing}\")\n\n# Check data shape\nprint(f\"Original data shape: {df_raw.shape}\")\nprint(f\"Clean data shape: {df_clean.shape}\")\nprint(f\"Rows retained: {len(df_clean)/len(df_raw):.2%}\")\n\n# Check column data types\nprint(\"\\nFinal data types:\")\nprint(df_clean.dtypes)\n\n# Sample of final clean data\nprint(\"\\nSample of clean data:\")\ndisplay(df_clean.head())\n\n# %% [markdown]\n# ## Step 8: Save Processed Data\n\n# %%\n# Save the cleaned and preprocessed data\noutput_path = 'path/to/processed_data.csv'\ndf_clean.to_csv(output_path, index=False)\nprint(f\"Saved processed data to {output_path}\")\n\n# %% [markdown]\n# ## Summary of Processing Steps\n# \n# 1. **Loading and Initial Assessment**:\n#    - Loaded raw data with shape {df_raw.shape}\n#    - Identified {missing_df.shape[0]} columns with missing values\n#    - Found {duplicate_count} duplicate rows\n# \n# 2. **Handling Missing Values**:\n#    - [List strategies used for handling missing values]\n# \n# 3. **Removing Duplicates**:\n#    - Removed {duplicate_count} duplicate rows\n# \n# 4. **Data Type Conversions**:\n#    - [List data type conversions performed]\n# \n# 5. **Handling Outliers**:\n#    - [List outlier handling strategies]\n# \n# 6. **Feature Engineering**:\n#    - [List new features created]\n# \n# 7. **Feature Scaling and Encoding**:\n#    - [List preprocessing steps applied]\n# \n# 8. **Final Dataset**:\n#    - Final shape: {df_clean.shape}\n#    - Retention rate: {len(df_clean)/len(df_raw):.2%}\n# \n# ## Next Steps\n# \n# [Document recommended actions for using this processed data]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]"
  },
  {
    "objectID": "analysis_notebooks.html#analysis-report-template",
    "href": "analysis_notebooks.html#analysis-report-template",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "# ---\n# title: \"Analysis Report: [Analysis Title]\"\n# author: \"[Author Name]\"\n# date: \"[YYYY-MM-DD]\"\n# ---\n\n# %% [markdown]\n# # Analysis Report: [Analysis Title]\n# \n# ## Executive Summary\n# \n# [Brief summary of the analysis, key findings, and recommendations (2-3 paragraphs)]\n# \n# **Key Findings:**\n# \n# 1. [Key finding 1]\n# 2. [Key finding 2]\n# 3. [Key finding 3]\n# \n# **Recommendations:**\n# \n# 1. [Recommendation 1]\n# 2. [Recommendation 2]\n# 3. [Recommendation 3]\n\n# %% [markdown]\n# ## 1. Introduction\n# \n# ### 1.1 Background\n# \n# [Context and background information]\n# \n# ### 1.2 Objectives\n# \n# [Specific objectives of this analysis]\n# \n# ### 1.3 Research Questions\n# \n# 1. [Research question 1]\n# 2. [Research question 2]\n# 3. [Research question 3]\n\n# %% [markdown]\n# ## 2. Methodology\n# \n# ### 2.1 Data Sources\n# \n# [Description of data sources used]\n# \n# | Dataset | Source | Time Period | Size | Description |\n# |---------|--------|-------------|------|-------------|\n# | [Dataset 1] | [Source] | [Time Period] | [Size] | [Description] |\n# | [Dataset 2] | [Source] | [Time Period] | [Size] | [Description] |\n# \n# ### 2.2 Analysis Approach\n# \n# [Description of the analytical approach, including methods, models, and techniques used]\n# \n# ### 2.3 Tools and Technologies\n# \n# - Python [version]\n# - pandas [version]\n# - scikit-learn [version]\n# - [Other tools/libraries]\n\n# %% [markdown]\n# ## 3. Data Preparation\n# \n# ### 3.1 Data Cleaning\n# \n# [Summary of data cleaning steps]\n\n# %%\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Load the cleaned data\ndf = pd.read_csv('path/to/cleaned_data.csv')\n\n# Display basic information\nprint(f\"Data Shape: {df.shape}\")\nprint(\"\\nData Types:\")\nprint(df.dtypes)\n\n# Display sample data\ndf.head()\n\n# %% [markdown]\n# ### 3.2 Feature Engineering\n# \n# [Description of features created or transformed]\n\n# %%\n# Display the engineered features\nengineered_features = ['feature_1', 'feature_2', 'feature_3']  # List your engineered features\ndf[engineered_features].head()\n\n# %% [markdown]\n# ## 4. Analysis Results\n# \n# ### 4.1 Descriptive Statistics\n# \n# [Summary of descriptive statistics]\n\n# %%\n# Display summary statistics\ndf.describe()\n\n# %%\n# Visualize distribution of key variables\nkey_variables = ['variable_1', 'variable_2', 'variable_3']  # List your key variables\n\nfor var in key_variables:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[var], kde=True)\n    plt.title(f'Distribution of {var}')\n    plt.show()\n\n# %% [markdown]\n# ### 4.2 Key Finding 1: [Title]\n# \n# [Detailed explanation of finding 1]\n\n# %%\n# Visualization supporting finding 1\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 1')\nplt.show()\n\n# %% [markdown]\n# ### 4.3 Key Finding 2: [Title]\n# \n# [Detailed explanation of finding 2]\n\n# %%\n# Visualization supporting finding 2\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 2')\nplt.show()\n\n# %% [markdown]\n# ### 4.4 Key Finding 3: [Title]\n# \n# [Detailed explanation of finding 3]\n\n# %%\n# Visualization supporting finding 3\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Visualization Supporting Finding 3')\nplt.show()\n\n# %% [markdown]\n# ## 5. Discussion\n# \n# ### 5.1 Interpretation of Results\n# \n# [Interpretation of the analysis results in the context of the research questions]\n# \n# ### 5.2 Limitations\n# \n# [Discussion of limitations of the analysis]\n# \n# ### 5.3 Implications\n# \n# [Discussion of implications of the findings]\n\n# %% [markdown]\n# ## 6. Recommendations\n# \n# ### 6.1 Recommendation 1: [Title]\n# \n# [Detailed explanation of recommendation 1, including implementation considerations]\n# \n# ### 6.2 Recommendation 2: [Title]\n# \n# [Detailed explanation of recommendation 2, including implementation considerations]\n# \n# ### 6.3 Recommendation 3: [Title]\n# \n# [Detailed explanation of recommendation 3, including implementation considerations]\n\n# %% [markdown]\n# ## 7. Conclusion\n# \n# [Summary of the analysis, findings, and recommendations]\n# \n# ## 8. Next Steps\n# \n# [Suggested next steps for further analysis or implementation]\n# \n# 1. [Next step 1]\n# 2. [Next step 2]\n# 3. [Next step 3]\n\n# %% [markdown]\n# ## Appendix\n# \n# ### A. Additional Visualizations\n# \n# [Additional visualizations supporting the analysis]\n\n# %%\n# Additional visualization 1\nplt.figure(figsize=(12, 8))\n# Add your visualization code here\nplt.title('Additional Visualization 1')\nplt.show()\n\n# %% [markdown]\n# ### B. Technical Details\n# \n# [Technical details of the analysis, such as model parameters, validation methods, etc.]\n# \n# ### C. References\n# \n# [References to data sources, methodologies, or other resources used]"
  },
  {
    "objectID": "analysis_notebooks.html#how-to-use-these-templates",
    "href": "analysis_notebooks.html#how-to-use-these-templates",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "These notebook templates are designed to provide a standardized structure for data analysis work. To use them effectively:\n\nCopy the Template: Select the appropriate template based on your analysis needs.\nCustomize the Template: Replace placeholders with your specific information.\nDocument as You Go: Keep documentation current as you perform your analysis.\nInclude Visualizations: Add relevant visualizations to support your findings.\nSave in Standard Format: Save your notebook in a standard format (e.g., Jupyter Notebook, Quarto Document).\nVersion Control: Commit your notebook to version control for tracking changes."
  },
  {
    "objectID": "analysis_notebooks.html#best-practices-for-analysis-notebooks",
    "href": "analysis_notebooks.html#best-practices-for-analysis-notebooks",
    "title": "Analysis Notebooks",
    "section": "",
    "text": "Structure and Organization:\n\nUse clear section headings\nInclude a table of contents\nNumber sections and subsections\n\nDocumentation:\n\nExplain your reasoning for key decisions\nDocument assumptions and limitations\nInclude sources of data and methodologies\n\nCode Quality:\n\nFollow the project’s coding standards\nUse comments to explain complex code\nOptimize for readability\n\nReproducibility:\n\nInclude all necessary import statements\nUse relative paths for file references\nDocument environment requirements\n\nVisualizations:\n\nUse appropriate chart types for your data\nInclude clear titles and labels\nConsider colorblind-friendly color palettes\n\nConclusion and Next Steps:\n\nSummarize key findings\nPropose actionable recommendations\nSuggest areas for further investigation"
  },
  {
    "objectID": "issues_and_bugs.html",
    "href": "issues_and_bugs.html",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "This document provides templates for tracking and managing issues in the GDPR Fines Analysis project.\n\n\n# Issue Report: [Issue ID]\n\n## Basic Information\n- **Issue ID**: [e.g., BUG-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Severity**: [Critical/Major/Minor/Trivial]\n- **Status**: [Open/In Progress/Under Review/Resolved/Closed]\n- **Assigned To**: [Name]\n\n## Description\n[Detailed description of the issue, including background information and context]\n\n## Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n4. ...\n\n## Expected Behavior\n[Description of what should happen if there was no issue]\n\n## Actual Behavior\n[Description of what actually happens]\n\n## Impact\n- **Business Impact**: [Description of how this affects business operations]\n- **Affected Users**: [Number or category of users affected]\n- **Affected Systems**: [List of systems affected]\n\n## Environment\n- **System Version**: [Version information]\n- **Browser/Client**: [Browser/client name and version]\n- **Operating System**: [OS name and version]\n- **Additional Environment Details**: [Any other relevant environment details]\n\n## Attachments\n- [Error logs]\n- [Screenshots]\n- [Videos]\n- [Other relevant files]\n\n## Related Issues\n- [Link to related issue 1]\n- [Link to related issue 2]\n\n## Root Cause Analysis\n[Preliminary assessment of what caused the issue, to be updated as investigation proceeds]\n\n## Resolution\n- **Fix Description**: [Description of the solution implemented]\n- **Fixed By**: [Name]\n- **Fixed Date**: [YYYY-MM-DD]\n- **Code Changes**: [Links to relevant code commits]\n\n## Verification\n- **Verified By**: [Name]\n- **Verification Date**: [YYYY-MM-DD]\n- **Verification Method**: [Description of testing performed to verify the fix]\n\n## Lessons Learned\n- [Key takeaways]\n- [Preventive measures for the future]\n\n\n\n# Bug Report: [Bug ID]\n\n## Bug Information\n- **Bug ID**: [e.g., BUG-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Component**: [System component where the bug was found]\n- **Version**: [Software version where the bug was found]\n- **Priority**: [P1/P2/P3/P4]\n- **Severity**: [Blocker/Critical/Major/Minor/Trivial]\n- **Status**: [New/Assigned/In Progress/Fixed/Verified/Closed]\n- **Assigned To**: [Name]\n- **Due Date**: [YYYY-MM-DD]\n\n## Description\n[Detailed description of the bug]\n\n## Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n4. ...\n\n## Expected Result\n[What should happen when steps are followed]\n\n## Actual Result\n[What actually happens when steps are followed]\n\n## Reproducibility\n[Always/Sometimes/Rarely/Unable to reproduce]\n\n## Environment\n- **Operating System**: [OS name and version]\n- **Browser**: [Browser name and version, if applicable]\n- **Database**: [Database name and version, if applicable]\n- **Screen Resolution**: [If relevant to the bug]\n- **Other Environment Details**: [Any other relevant environment details]\n\n## Attachments\n- [Screenshot 1]\n- [Screenshot 2]\n- [Error logs]\n- [Videos demonstrating the bug]\n\n## Regression Information\n- **Previous Working Version**: [Version where this worked correctly]\n- **Introduced In Version**: [Version where this bug first appeared, if known]\n\n## Workaround\n[Temporary solution until the bug is fixed, if available]\n\n## Resolution\n- **Resolution Type**: [Fixed/Cannot Reproduce/Duplicate/Won't Fix/By Design]\n- **Resolution Description**: [Details of how the bug was fixed or why it won't be fixed]\n- **Fixed By**: [Name]\n- **Fixed Date**: [YYYY-MM-DD]\n- **Fix Version**: [Version where the fix will be/was implemented]\n- **Code Changes**: [Links to relevant code commits]\n\n## Testing Notes\n- **Tested By**: [Name]\n- **Test Date**: [YYYY-MM-DD]\n- **Test Cases**: [References to test cases used]\n- **Test Results**: [Summary of test results]\n\n## Related Items\n- **Related Bugs**: [Links to related bugs]\n- **Related Features**: [Links to related features]\n- **Related Documents**: [Links to related documentation]\n\n\n\n# Data Quality Issue: [Issue ID]\n\n## Basic Information\n- **Issue ID**: [e.g., DQI-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Severity**: [Critical/Major/Minor/Trivial]\n- **Status**: [Open/In Progress/Under Review/Resolved/Closed]\n- **Assigned To**: [Name]\n\n## Issue Description\n[Detailed description of the data quality issue]\n\n## Data Sources Affected\n- **Primary Source**: [Name of the primary data source]\n- **Additional Sources**: [Names of other affected data sources]\n- **Tables/Fields**: [Specific tables and fields affected]\n- **Data Volume Affected**: [Amount or percentage of data affected]\n\n## Data Quality Dimension\n- **Type**: [Completeness/Accuracy/Consistency/Timeliness/Validity/Uniqueness/Integrity]\n- **Metrics**: [Specific metrics showing the issue, e.g., \"15% of records have missing values\"]\n\n## Impact Assessment\n- **Business Impact**: [How this affects business operations or decision-making]\n- **Downstream Systems**: [Systems that may be affected by this issue]\n- **Reporting Impact**: [How this affects reports or dashboards]\n- **Compliance Impact**: [Any compliance concerns raised by this issue]\n\n## Root Cause Analysis\n- **Primary Cause**: [Main reason for the data quality issue]\n- **Contributing Factors**: [Other factors that contributed to the issue]\n- **Process Failures**: [Process breakdowns that allowed the issue to occur]\n\n## Remediation Plan\n- **Short-term Fix**: [Immediate steps to address the issue]\n- **Long-term Solution**: [Comprehensive solution to prevent recurrence]\n- **Validation Method**: [How the fix will be validated]\n- **Timeline**: [Expected timeline for resolution]\n\n## Resolution\n- **Actions Taken**: [Description of actions taken to resolve the issue]\n- **Data Corrections**: [Specific corrections made to the data]\n- **Process Improvements**: [Changes to processes to prevent recurrence]\n- **Resolved By**: [Name]\n- **Resolution Date**: [YYYY-MM-DD]\n\n## Verification\n- **Verification Method**: [How the resolution was verified]\n- **Verification Results**: [Results of verification testing]\n- **Verified By**: [Name]\n- **Verification Date**: [YYYY-MM-DD]\n\n## Preventive Measures\n- **Monitoring Controls**: [New monitoring implemented]\n- **Process Changes**: [Changes to processes]\n- **System Enhancements**: [Changes to systems]\n- **Training/Documentation**: [Training or documentation updates]\n\n## Lessons Learned\n- [Key takeaways from this issue]\n- [Recommendations for future data management]\n\n\n\n# Feature Request: [Feature ID]\n\n## Basic Information\n- **Feature ID**: [e.g., FR-001]\n- **Title**: [Brief descriptive title]\n- **Requested By**: [Name]\n- **Requested Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Status**: [New/Under Review/Approved/In Development/Implemented/Rejected]\n- **Assigned To**: [Name]\n- **Target Release**: [Version or sprint where this should be implemented]\n\n## Description\n[Detailed description of the feature being requested]\n\n## Justification\n[Business justification for the feature, including benefits and value]\n\n## User Stories\n- As a [type of user], I want [goal] so that [benefit].\n- As a [type of user], I want [goal] so that [benefit].\n- ...\n\n## Acceptance Criteria\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n- [ ] [Criterion 3]\n- ...\n\n## Technical Considerations\n- **Implementation Approach**: [High-level description of how this might be implemented]\n- **Dependencies**: [Other features or components this depends on]\n- **Technical Risks**: [Potential technical challenges or risks]\n- **Performance Considerations**: [Performance impacts to consider]\n- **Security Considerations**: [Security aspects to consider]\n\n## UI/UX Design\n[Wireframes, mockups, or descriptions of user interface changes]\n\n## Alternatives Considered\n- **Alternative 1**: [Description and pros/cons]\n- **Alternative 2**: [Description and pros/cons]\n- ...\n\n## Stakeholders\n[List of stakeholders who should be consulted or informed about this feature]\n\n## Resource Estimate\n- **Development Effort**: [Estimated person-days/hours]\n- **QA Effort**: [Estimated person-days/hours]\n- **Documentation Effort**: [Estimated person-days/hours]\n- **Total Effort**: [Total estimated effort]\n\n## Approval\n- **Approved By**: [Name]\n- **Approval Date**: [YYYY-MM-DD]\n- **Approval Notes**: [Any notes regarding the approval decision]\n\n## Implementation\n- **Implemented By**: [Name]\n- **Implementation Date**: [YYYY-MM-DD]\n- **Implementation Details**: [Brief overview of the implementation]\n- **Code Changes**: [Links to relevant code commits]\n\n## Testing\n- **Tested By**: [Name]\n- **Test Date**: [YYYY-MM-DD]\n- **Test Results**: [Summary of test results]\n\n## Documentation\n- **Documentation Required**: [Yes/No]\n- **Documentation Location**: [Link to documentation]\n- **Documentation Status**: [Not Started/In Progress/Completed]\n\n\n\n\nCopy the relevant template markdown.\nFill in the details specific to your issue or bug.\nSave the completed document in the project’s issue tracking system.\nUpdate the document as the issue progresses through its lifecycle.\nReference the issue ID in any related code commits or documentation.\n\nThese templates should be customized based on your team’s specific needs and processes."
  },
  {
    "objectID": "issues_and_bugs.html#issue-tracking-template",
    "href": "issues_and_bugs.html#issue-tracking-template",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "# Issue Report: [Issue ID]\n\n## Basic Information\n- **Issue ID**: [e.g., BUG-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Severity**: [Critical/Major/Minor/Trivial]\n- **Status**: [Open/In Progress/Under Review/Resolved/Closed]\n- **Assigned To**: [Name]\n\n## Description\n[Detailed description of the issue, including background information and context]\n\n## Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n4. ...\n\n## Expected Behavior\n[Description of what should happen if there was no issue]\n\n## Actual Behavior\n[Description of what actually happens]\n\n## Impact\n- **Business Impact**: [Description of how this affects business operations]\n- **Affected Users**: [Number or category of users affected]\n- **Affected Systems**: [List of systems affected]\n\n## Environment\n- **System Version**: [Version information]\n- **Browser/Client**: [Browser/client name and version]\n- **Operating System**: [OS name and version]\n- **Additional Environment Details**: [Any other relevant environment details]\n\n## Attachments\n- [Error logs]\n- [Screenshots]\n- [Videos]\n- [Other relevant files]\n\n## Related Issues\n- [Link to related issue 1]\n- [Link to related issue 2]\n\n## Root Cause Analysis\n[Preliminary assessment of what caused the issue, to be updated as investigation proceeds]\n\n## Resolution\n- **Fix Description**: [Description of the solution implemented]\n- **Fixed By**: [Name]\n- **Fixed Date**: [YYYY-MM-DD]\n- **Code Changes**: [Links to relevant code commits]\n\n## Verification\n- **Verified By**: [Name]\n- **Verification Date**: [YYYY-MM-DD]\n- **Verification Method**: [Description of testing performed to verify the fix]\n\n## Lessons Learned\n- [Key takeaways]\n- [Preventive measures for the future]"
  },
  {
    "objectID": "issues_and_bugs.html#bug-report-template",
    "href": "issues_and_bugs.html#bug-report-template",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "# Bug Report: [Bug ID]\n\n## Bug Information\n- **Bug ID**: [e.g., BUG-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Component**: [System component where the bug was found]\n- **Version**: [Software version where the bug was found]\n- **Priority**: [P1/P2/P3/P4]\n- **Severity**: [Blocker/Critical/Major/Minor/Trivial]\n- **Status**: [New/Assigned/In Progress/Fixed/Verified/Closed]\n- **Assigned To**: [Name]\n- **Due Date**: [YYYY-MM-DD]\n\n## Description\n[Detailed description of the bug]\n\n## Steps to Reproduce\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n4. ...\n\n## Expected Result\n[What should happen when steps are followed]\n\n## Actual Result\n[What actually happens when steps are followed]\n\n## Reproducibility\n[Always/Sometimes/Rarely/Unable to reproduce]\n\n## Environment\n- **Operating System**: [OS name and version]\n- **Browser**: [Browser name and version, if applicable]\n- **Database**: [Database name and version, if applicable]\n- **Screen Resolution**: [If relevant to the bug]\n- **Other Environment Details**: [Any other relevant environment details]\n\n## Attachments\n- [Screenshot 1]\n- [Screenshot 2]\n- [Error logs]\n- [Videos demonstrating the bug]\n\n## Regression Information\n- **Previous Working Version**: [Version where this worked correctly]\n- **Introduced In Version**: [Version where this bug first appeared, if known]\n\n## Workaround\n[Temporary solution until the bug is fixed, if available]\n\n## Resolution\n- **Resolution Type**: [Fixed/Cannot Reproduce/Duplicate/Won't Fix/By Design]\n- **Resolution Description**: [Details of how the bug was fixed or why it won't be fixed]\n- **Fixed By**: [Name]\n- **Fixed Date**: [YYYY-MM-DD]\n- **Fix Version**: [Version where the fix will be/was implemented]\n- **Code Changes**: [Links to relevant code commits]\n\n## Testing Notes\n- **Tested By**: [Name]\n- **Test Date**: [YYYY-MM-DD]\n- **Test Cases**: [References to test cases used]\n- **Test Results**: [Summary of test results]\n\n## Related Items\n- **Related Bugs**: [Links to related bugs]\n- **Related Features**: [Links to related features]\n- **Related Documents**: [Links to related documentation]"
  },
  {
    "objectID": "issues_and_bugs.html#data-quality-issue-template",
    "href": "issues_and_bugs.html#data-quality-issue-template",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "# Data Quality Issue: [Issue ID]\n\n## Basic Information\n- **Issue ID**: [e.g., DQI-001]\n- **Title**: [Brief descriptive title]\n- **Reported By**: [Name]\n- **Reported Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Severity**: [Critical/Major/Minor/Trivial]\n- **Status**: [Open/In Progress/Under Review/Resolved/Closed]\n- **Assigned To**: [Name]\n\n## Issue Description\n[Detailed description of the data quality issue]\n\n## Data Sources Affected\n- **Primary Source**: [Name of the primary data source]\n- **Additional Sources**: [Names of other affected data sources]\n- **Tables/Fields**: [Specific tables and fields affected]\n- **Data Volume Affected**: [Amount or percentage of data affected]\n\n## Data Quality Dimension\n- **Type**: [Completeness/Accuracy/Consistency/Timeliness/Validity/Uniqueness/Integrity]\n- **Metrics**: [Specific metrics showing the issue, e.g., \"15% of records have missing values\"]\n\n## Impact Assessment\n- **Business Impact**: [How this affects business operations or decision-making]\n- **Downstream Systems**: [Systems that may be affected by this issue]\n- **Reporting Impact**: [How this affects reports or dashboards]\n- **Compliance Impact**: [Any compliance concerns raised by this issue]\n\n## Root Cause Analysis\n- **Primary Cause**: [Main reason for the data quality issue]\n- **Contributing Factors**: [Other factors that contributed to the issue]\n- **Process Failures**: [Process breakdowns that allowed the issue to occur]\n\n## Remediation Plan\n- **Short-term Fix**: [Immediate steps to address the issue]\n- **Long-term Solution**: [Comprehensive solution to prevent recurrence]\n- **Validation Method**: [How the fix will be validated]\n- **Timeline**: [Expected timeline for resolution]\n\n## Resolution\n- **Actions Taken**: [Description of actions taken to resolve the issue]\n- **Data Corrections**: [Specific corrections made to the data]\n- **Process Improvements**: [Changes to processes to prevent recurrence]\n- **Resolved By**: [Name]\n- **Resolution Date**: [YYYY-MM-DD]\n\n## Verification\n- **Verification Method**: [How the resolution was verified]\n- **Verification Results**: [Results of verification testing]\n- **Verified By**: [Name]\n- **Verification Date**: [YYYY-MM-DD]\n\n## Preventive Measures\n- **Monitoring Controls**: [New monitoring implemented]\n- **Process Changes**: [Changes to processes]\n- **System Enhancements**: [Changes to systems]\n- **Training/Documentation**: [Training or documentation updates]\n\n## Lessons Learned\n- [Key takeaways from this issue]\n- [Recommendations for future data management]"
  },
  {
    "objectID": "issues_and_bugs.html#feature-request-template",
    "href": "issues_and_bugs.html#feature-request-template",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "# Feature Request: [Feature ID]\n\n## Basic Information\n- **Feature ID**: [e.g., FR-001]\n- **Title**: [Brief descriptive title]\n- **Requested By**: [Name]\n- **Requested Date**: [YYYY-MM-DD]\n- **Priority**: [High/Medium/Low]\n- **Status**: [New/Under Review/Approved/In Development/Implemented/Rejected]\n- **Assigned To**: [Name]\n- **Target Release**: [Version or sprint where this should be implemented]\n\n## Description\n[Detailed description of the feature being requested]\n\n## Justification\n[Business justification for the feature, including benefits and value]\n\n## User Stories\n- As a [type of user], I want [goal] so that [benefit].\n- As a [type of user], I want [goal] so that [benefit].\n- ...\n\n## Acceptance Criteria\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n- [ ] [Criterion 3]\n- ...\n\n## Technical Considerations\n- **Implementation Approach**: [High-level description of how this might be implemented]\n- **Dependencies**: [Other features or components this depends on]\n- **Technical Risks**: [Potential technical challenges or risks]\n- **Performance Considerations**: [Performance impacts to consider]\n- **Security Considerations**: [Security aspects to consider]\n\n## UI/UX Design\n[Wireframes, mockups, or descriptions of user interface changes]\n\n## Alternatives Considered\n- **Alternative 1**: [Description and pros/cons]\n- **Alternative 2**: [Description and pros/cons]\n- ...\n\n## Stakeholders\n[List of stakeholders who should be consulted or informed about this feature]\n\n## Resource Estimate\n- **Development Effort**: [Estimated person-days/hours]\n- **QA Effort**: [Estimated person-days/hours]\n- **Documentation Effort**: [Estimated person-days/hours]\n- **Total Effort**: [Total estimated effort]\n\n## Approval\n- **Approved By**: [Name]\n- **Approval Date**: [YYYY-MM-DD]\n- **Approval Notes**: [Any notes regarding the approval decision]\n\n## Implementation\n- **Implemented By**: [Name]\n- **Implementation Date**: [YYYY-MM-DD]\n- **Implementation Details**: [Brief overview of the implementation]\n- **Code Changes**: [Links to relevant code commits]\n\n## Testing\n- **Tested By**: [Name]\n- **Test Date**: [YYYY-MM-DD]\n- **Test Results**: [Summary of test results]\n\n## Documentation\n- **Documentation Required**: [Yes/No]\n- **Documentation Location**: [Link to documentation]\n- **Documentation Status**: [Not Started/In Progress/Completed]"
  },
  {
    "objectID": "issues_and_bugs.html#how-to-use-these-templates",
    "href": "issues_and_bugs.html#how-to-use-these-templates",
    "title": "Issues and Bugs Tracking",
    "section": "",
    "text": "Copy the relevant template markdown.\nFill in the details specific to your issue or bug.\nSave the completed document in the project’s issue tracking system.\nUpdate the document as the issue progresses through its lifecycle.\nReference the issue ID in any related code commits or documentation.\n\nThese templates should be customized based on your team’s specific needs and processes."
  },
  {
    "objectID": "meeting_templates.html",
    "href": "meeting_templates.html",
    "title": "Meeting Templates",
    "section": "",
    "text": "This document provides templates for various team meetings in the GDPR Fines Analysis project.\n\n\n# Weekly Team Meeting: [Date]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Absent\n- [Name, Role]\n- [Name, Role]\n\n## Agenda\n1. **Review Previous Action Items** (10 mins)\n2. **Project Status Updates** (15 mins)\n3. **Key Discussion Topics** (20 mins)\n   - [Topic 1]\n   - [Topic 2]\n   - [Topic 3]\n4. **Roadblocks/Issues** (10 mins)\n5. **Next Steps and Action Items** (5 mins)\n\n## Previous Action Item Follow-up\n| Action Item | Owner | Status | Notes |\n|-------------|-------|--------|-------|\n| [Item 1]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n| [Item 2]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n| [Item 3]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n\n## Discussion Notes\n### Project Status Updates\n- [Update 1]\n- [Update 2]\n- [Update 3]\n\n### Key Discussion Topic 1: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Key Discussion Topic 2: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Key Discussion Topic 3: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Roadblocks/Issues\n- [Issue 1]\n  - **Impact**: [Impact]\n  - **Proposed Solution**: [Solution]\n- [Issue 2]\n  - **Impact**: [Impact]\n  - **Proposed Solution**: [Solution]\n\n## New Action Items\n| Action Item | Description | Owner | Due Date | Priority |\n|-------------|-------------|-------|----------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 2]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 3]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n\n## Next Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Agenda Items**:\n  - [Item 1]\n  - [Item 2]\n  - [Item 3]\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n# Sprint Planning Meeting: Sprint [#]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Sprint Duration**: [Start Date] to [End Date]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Sprint Goal\n[Clear, concise statement of what the team aims to achieve in this sprint]\n\n## Sprint Backlog\n| User Story/Task | Description | Acceptance Criteria | Story Points | Assignee |\n|-----------------|-------------|---------------------|--------------|----------|\n| [ID-001]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n| [ID-002]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n| [ID-003]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n\n## Team Capacity\n- **Total Team Members**: [Number]\n- **Available Working Days**: [Number] days\n- **Total Team Capacity**: [Number] story points\n- **Committed Story Points**: [Number]\n- **Buffer**: [Number] story points ([Percentage]%)\n\n## Dependencies\n- [ID-001] depends on [External Dependency]\n- [ID-002] depends on [ID-003]\n- [Team Member] will be out on [Date]\n\n## Risks and Concerns\n- [Risk 1]\n  - **Mitigation**: [Strategy]\n- [Risk 2]\n  - **Mitigation**: [Strategy]\n\n## Definition of Done\nFor a story to be considered complete:\n- Code is written and meets established coding standards\n- Unit tests are written and passing\n- Code is reviewed by at least one team member\n- Feature is tested in a development environment\n- Documentation is updated\n- [Additional criteria]\n\n## Action Items\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n# Sprint Retrospective: Sprint [#]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Sprint Duration**: [Start Date] to [End Date]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Sprint Overview\n- **Sprint Goal**: [Goal]\n- **Committed Story Points**: [Number]\n- **Completed Story Points**: [Number]\n- **Completion Rate**: [Percentage]\n\n## What Went Well\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## What Could Be Improved\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## What We Learned\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## Action Items for Improvement\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n| [Item 3]    | [Description] | [Name] | [Date] |\n\n## Team Mood\n- **Overall Team Mood**: [Description]\n- **Team Morale Indicators**:\n  - Communication: [Good/Neutral/Needs Improvement]\n  - Collaboration: [Good/Neutral/Needs Improvement]\n  - Motivation: [Good/Neutral/Needs Improvement]\n  - Workload: [Good/Neutral/Needs Improvement]\n\n## Follow-up on Previous Retrospective Actions\n| Action Item | Status | Outcome | Next Steps |\n|-------------|--------|---------|------------|\n| [Item 1]    | [Complete/In Progress/Not Started] | [Outcome] | [Next Steps] |\n| [Item 2]    | [Complete/In Progress/Not Started] | [Outcome] | [Next Steps] |\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n# Data Review Meeting: [Topic]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Agenda\n1. **Introduction and Meeting Objectives** (5 mins)\n2. **Data Overview** (10 mins)\n3. **Data Quality Assessment** (15 mins)\n4. **Analysis Findings** (20 mins)\n5. **Discussion and Interpretation** (15 mins)\n6. **Next Steps and Action Items** (10 mins)\n\n## Data Overview\n- **Data Sources**: [List of data sources reviewed]\n- **Time Period**: [Time period covered by the data]\n- **Data Volume**: [Amount of data reviewed]\n- **Key Metrics**: [Primary metrics being analyzed]\n\n## Data Quality Assessment\n| Quality Dimension | Status | Issues | Recommendations |\n|-------------------|--------|--------|-----------------|\n| Completeness      | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Accuracy          | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Consistency       | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Timeliness        | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Uniqueness        | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n\n## Analysis Findings\n### Finding 1: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n### Finding 2: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n### Finding 3: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n## Discussion and Interpretation\n- **Key Insights**:\n  - [Insight 1]\n  - [Insight 2]\n  - [Insight 3]\n- **Questions Raised**:\n  - [Question 1]\n  - [Question 2]\n  - [Question 3]\n- **Alternative Interpretations**:\n  - [Interpretation 1]\n  - [Interpretation 2]\n\n## Action Items\n| Action Item | Description | Owner | Due Date | Priority |\n|-------------|-------------|-------|----------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 2]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 3]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n\n## Next Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Focus Areas**:\n  - [Area 1]\n  - [Area 2]\n  - [Area 3]\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n# Stakeholder Update Meeting: [Project Name]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n### Project Team\n- [Name, Role]\n- [Name, Role]\n\n### Stakeholders\n- [Name, Role, Organization]\n- [Name, Role, Organization]\n- [Name, Role, Organization]\n\n## Agenda\n1. **Project Overview and Status** (10 mins)\n2. **Accomplishments Since Last Update** (15 mins)\n3. **Key Metrics and Results** (15 mins)\n4. **Challenges and Mitigations** (10 mins)\n5. **Next Steps and Timeline** (10 mins)\n6. **Q&A and Discussion** (15 mins)\n\n## Project Overview and Status\n- **Project Goal**: [Brief reminder of project objectives]\n- **Overall Status**: [On Track / At Risk / Delayed]\n- **Current Phase**: [Phase name and brief description]\n- **Timeline Status**: [X weeks/months complete, Y weeks/months remaining]\n- **Budget Status**: [On Budget / Under Budget / Over Budget]\n\n## Accomplishments Since Last Update\n- [Accomplishment 1]\n- [Accomplishment 2]\n- [Accomplishment 3]\n- [Accomplishment 4]\n\n## Key Metrics and Results\n| Metric | Target | Current | Status | Trend |\n|--------|--------|---------|--------|-------|\n| [Metric 1] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n| [Metric 2] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n| [Metric 3] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n\n### Key Findings\n- [Finding 1]\n- [Finding 2]\n- [Finding 3]\n\n## Challenges and Mitigations\n| Challenge | Impact | Mitigation Strategy | Status |\n|-----------|--------|---------------------|--------|\n| [Challenge 1] | [Impact] | [Strategy] | [Status] |\n| [Challenge 2] | [Impact] | [Strategy] | [Status] |\n| [Challenge 3] | [Impact] | [Strategy] | [Status] |\n\n## Next Steps and Timeline\n- **Short-term Milestones** (Next 2-4 weeks):\n  - [Milestone 1]: [Due date]\n  - [Milestone 2]: [Due date]\n- **Medium-term Goals** (Next 1-3 months):\n  - [Goal 1]: [Timeframe]\n  - [Goal 2]: [Timeframe]\n- **Decisions Needed from Stakeholders**:\n  - [Decision 1]\n  - [Decision 2]\n\n## Questions and Discussion Notes\n- [Question 1]\n  - **Answer/Discussion**: [Notes]\n- [Question 2]\n  - **Answer/Discussion**: [Notes]\n- [Question 3]\n  - **Answer/Discussion**: [Notes]\n\n## Action Items\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n| [Item 3]    | [Description] | [Name] | [Date] |\n\n## Next Update Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Focus Areas**:\n  - [Area 1]\n  - [Area 2]\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n\nCopy the template markdown for the appropriate meeting type.\nReplace all placeholders (text within [brackets]) with meeting-specific information.\nDistribute the agenda to all participants before the meeting.\nTake notes during the meeting.\nAfter the meeting, update the template with notes and distribute to all participants.\nStore in the project documentation repository for future reference.\n\nThese templates are designed to be flexible and can be adapted to fit the specific needs of your meetings."
  },
  {
    "objectID": "meeting_templates.html#weekly-team-meeting-template",
    "href": "meeting_templates.html#weekly-team-meeting-template",
    "title": "Meeting Templates",
    "section": "",
    "text": "# Weekly Team Meeting: [Date]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Absent\n- [Name, Role]\n- [Name, Role]\n\n## Agenda\n1. **Review Previous Action Items** (10 mins)\n2. **Project Status Updates** (15 mins)\n3. **Key Discussion Topics** (20 mins)\n   - [Topic 1]\n   - [Topic 2]\n   - [Topic 3]\n4. **Roadblocks/Issues** (10 mins)\n5. **Next Steps and Action Items** (5 mins)\n\n## Previous Action Item Follow-up\n| Action Item | Owner | Status | Notes |\n|-------------|-------|--------|-------|\n| [Item 1]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n| [Item 2]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n| [Item 3]    | [Name] | [Complete/In Progress/Not Started] | [Notes] |\n\n## Discussion Notes\n### Project Status Updates\n- [Update 1]\n- [Update 2]\n- [Update 3]\n\n### Key Discussion Topic 1: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Key Discussion Topic 2: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Key Discussion Topic 3: [Topic]\n- [Point 1]\n- [Point 2]\n- [Point 3]\n\n### Roadblocks/Issues\n- [Issue 1]\n  - **Impact**: [Impact]\n  - **Proposed Solution**: [Solution]\n- [Issue 2]\n  - **Impact**: [Impact]\n  - **Proposed Solution**: [Solution]\n\n## New Action Items\n| Action Item | Description | Owner | Due Date | Priority |\n|-------------|-------------|-------|----------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 2]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 3]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n\n## Next Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Agenda Items**:\n  - [Item 1]\n  - [Item 2]\n  - [Item 3]\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "meeting_templates.html#sprint-planning-meeting-template",
    "href": "meeting_templates.html#sprint-planning-meeting-template",
    "title": "Meeting Templates",
    "section": "",
    "text": "# Sprint Planning Meeting: Sprint [#]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Sprint Duration**: [Start Date] to [End Date]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Sprint Goal\n[Clear, concise statement of what the team aims to achieve in this sprint]\n\n## Sprint Backlog\n| User Story/Task | Description | Acceptance Criteria | Story Points | Assignee |\n|-----------------|-------------|---------------------|--------------|----------|\n| [ID-001]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n| [ID-002]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n| [ID-003]        | [Description] | - [Criterion 1]&lt;br&gt;- [Criterion 2]&lt;br&gt;- [Criterion 3] | [Points] | [Name] |\n\n## Team Capacity\n- **Total Team Members**: [Number]\n- **Available Working Days**: [Number] days\n- **Total Team Capacity**: [Number] story points\n- **Committed Story Points**: [Number]\n- **Buffer**: [Number] story points ([Percentage]%)\n\n## Dependencies\n- [ID-001] depends on [External Dependency]\n- [ID-002] depends on [ID-003]\n- [Team Member] will be out on [Date]\n\n## Risks and Concerns\n- [Risk 1]\n  - **Mitigation**: [Strategy]\n- [Risk 2]\n  - **Mitigation**: [Strategy]\n\n## Definition of Done\nFor a story to be considered complete:\n- Code is written and meets established coding standards\n- Unit tests are written and passing\n- Code is reviewed by at least one team member\n- Feature is tested in a development environment\n- Documentation is updated\n- [Additional criteria]\n\n## Action Items\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "meeting_templates.html#sprint-retrospective-template",
    "href": "meeting_templates.html#sprint-retrospective-template",
    "title": "Meeting Templates",
    "section": "",
    "text": "# Sprint Retrospective: Sprint [#]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Sprint Duration**: [Start Date] to [End Date]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Sprint Overview\n- **Sprint Goal**: [Goal]\n- **Committed Story Points**: [Number]\n- **Completed Story Points**: [Number]\n- **Completion Rate**: [Percentage]\n\n## What Went Well\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## What Could Be Improved\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## What We Learned\n- [Item 1]\n- [Item 2]\n- [Item 3]\n\n## Action Items for Improvement\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n| [Item 3]    | [Description] | [Name] | [Date] |\n\n## Team Mood\n- **Overall Team Mood**: [Description]\n- **Team Morale Indicators**:\n  - Communication: [Good/Neutral/Needs Improvement]\n  - Collaboration: [Good/Neutral/Needs Improvement]\n  - Motivation: [Good/Neutral/Needs Improvement]\n  - Workload: [Good/Neutral/Needs Improvement]\n\n## Follow-up on Previous Retrospective Actions\n| Action Item | Status | Outcome | Next Steps |\n|-------------|--------|---------|------------|\n| [Item 1]    | [Complete/In Progress/Not Started] | [Outcome] | [Next Steps] |\n| [Item 2]    | [Complete/In Progress/Not Started] | [Outcome] | [Next Steps] |\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "meeting_templates.html#data-review-meeting-template",
    "href": "meeting_templates.html#data-review-meeting-template",
    "title": "Meeting Templates",
    "section": "",
    "text": "# Data Review Meeting: [Topic]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n- [Name, Role]\n\n## Agenda\n1. **Introduction and Meeting Objectives** (5 mins)\n2. **Data Overview** (10 mins)\n3. **Data Quality Assessment** (15 mins)\n4. **Analysis Findings** (20 mins)\n5. **Discussion and Interpretation** (15 mins)\n6. **Next Steps and Action Items** (10 mins)\n\n## Data Overview\n- **Data Sources**: [List of data sources reviewed]\n- **Time Period**: [Time period covered by the data]\n- **Data Volume**: [Amount of data reviewed]\n- **Key Metrics**: [Primary metrics being analyzed]\n\n## Data Quality Assessment\n| Quality Dimension | Status | Issues | Recommendations |\n|-------------------|--------|--------|-----------------|\n| Completeness      | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Accuracy          | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Consistency       | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Timeliness        | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n| Uniqueness        | [Good/Fair/Poor] | [Issues identified] | [Recommendations] |\n\n## Analysis Findings\n### Finding 1: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n### Finding 2: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n### Finding 3: [Brief Description]\n- **Observation**: [Detailed description]\n- **Supporting Data**: [Specific data points or trends]\n- **Significance**: [Why this matters]\n- **Visualizations**: [Reference to visualizations]\n\n## Discussion and Interpretation\n- **Key Insights**:\n  - [Insight 1]\n  - [Insight 2]\n  - [Insight 3]\n- **Questions Raised**:\n  - [Question 1]\n  - [Question 2]\n  - [Question 3]\n- **Alternative Interpretations**:\n  - [Interpretation 1]\n  - [Interpretation 2]\n\n## Action Items\n| Action Item | Description | Owner | Due Date | Priority |\n|-------------|-------------|-------|----------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 2]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n| [Item 3]    | [Description] | [Name] | [Date] | [High/Medium/Low] |\n\n## Next Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Focus Areas**:\n  - [Area 1]\n  - [Area 2]\n  - [Area 3]\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "meeting_templates.html#stakeholder-update-meeting-template",
    "href": "meeting_templates.html#stakeholder-update-meeting-template",
    "title": "Meeting Templates",
    "section": "",
    "text": "# Stakeholder Update Meeting: [Project Name]\n\n## Meeting Information\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Location**: [Location/Link]\n- **Meeting Organizer**: [Name]\n- **Note Taker**: [Name]\n\n## Attendees\n### Project Team\n- [Name, Role]\n- [Name, Role]\n\n### Stakeholders\n- [Name, Role, Organization]\n- [Name, Role, Organization]\n- [Name, Role, Organization]\n\n## Agenda\n1. **Project Overview and Status** (10 mins)\n2. **Accomplishments Since Last Update** (15 mins)\n3. **Key Metrics and Results** (15 mins)\n4. **Challenges and Mitigations** (10 mins)\n5. **Next Steps and Timeline** (10 mins)\n6. **Q&A and Discussion** (15 mins)\n\n## Project Overview and Status\n- **Project Goal**: [Brief reminder of project objectives]\n- **Overall Status**: [On Track / At Risk / Delayed]\n- **Current Phase**: [Phase name and brief description]\n- **Timeline Status**: [X weeks/months complete, Y weeks/months remaining]\n- **Budget Status**: [On Budget / Under Budget / Over Budget]\n\n## Accomplishments Since Last Update\n- [Accomplishment 1]\n- [Accomplishment 2]\n- [Accomplishment 3]\n- [Accomplishment 4]\n\n## Key Metrics and Results\n| Metric | Target | Current | Status | Trend |\n|--------|--------|---------|--------|-------|\n| [Metric 1] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n| [Metric 2] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n| [Metric 3] | [Target] | [Current value] | [On Track/At Risk/Off Track] | [Up/Down/Stable] |\n\n### Key Findings\n- [Finding 1]\n- [Finding 2]\n- [Finding 3]\n\n## Challenges and Mitigations\n| Challenge | Impact | Mitigation Strategy | Status |\n|-----------|--------|---------------------|--------|\n| [Challenge 1] | [Impact] | [Strategy] | [Status] |\n| [Challenge 2] | [Impact] | [Strategy] | [Status] |\n| [Challenge 3] | [Impact] | [Strategy] | [Status] |\n\n## Next Steps and Timeline\n- **Short-term Milestones** (Next 2-4 weeks):\n  - [Milestone 1]: [Due date]\n  - [Milestone 2]: [Due date]\n- **Medium-term Goals** (Next 1-3 months):\n  - [Goal 1]: [Timeframe]\n  - [Goal 2]: [Timeframe]\n- **Decisions Needed from Stakeholders**:\n  - [Decision 1]\n  - [Decision 2]\n\n## Questions and Discussion Notes\n- [Question 1]\n  - **Answer/Discussion**: [Notes]\n- [Question 2]\n  - **Answer/Discussion**: [Notes]\n- [Question 3]\n  - **Answer/Discussion**: [Notes]\n\n## Action Items\n| Action Item | Description | Owner | Due Date |\n|-------------|-------------|-------|----------|\n| [Item 1]    | [Description] | [Name] | [Date] |\n| [Item 2]    | [Description] | [Name] | [Date] |\n| [Item 3]    | [Description] | [Name] | [Date] |\n\n## Next Update Meeting\n- **Date**: [YYYY-MM-DD]\n- **Time**: [HH:MM] - [HH:MM]\n- **Focus Areas**:\n  - [Area 1]\n  - [Area 2]\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "meeting_templates.html#how-to-use-these-templates",
    "href": "meeting_templates.html#how-to-use-these-templates",
    "title": "Meeting Templates",
    "section": "",
    "text": "Copy the template markdown for the appropriate meeting type.\nReplace all placeholders (text within [brackets]) with meeting-specific information.\nDistribute the agenda to all participants before the meeting.\nTake notes during the meeting.\nAfter the meeting, update the template with notes and distribute to all participants.\nStore in the project documentation repository for future reference.\n\nThese templates are designed to be flexible and can be adapted to fit the specific needs of your meetings."
  },
  {
    "objectID": "analytical_engineering_docs.html",
    "href": "analytical_engineering_docs.html",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "Welcome to the Analytical Engineering documentation for the GDPR Fines Analysis project. This documentation serves as a comprehensive guide for the team’s processes, standards, and best practices.\n\n\nThe GDPR Fines Analysis project aims to analyze and visualize GDPR fine data to provide insights into compliance trends, geographic distribution, and industry impacts. This documentation supports the team in maintaining high standards of data quality, analytical rigor, and project management.\n\n\n\n\n\n\nProject Planning - Templates and guidelines for project management\nMeeting Templates - Templates for various team meetings\nFindings Templates - Templates for documenting analysis results\nIssues and Bugs - Templates for tracking and managing issues\n\n\n\n\n\nReference Materials - Technical standards and best practices\nTeam Wiki - Development environment setup and team resources\nAnalysis Notebooks - Templates for data analysis\nData Quality Checks - Templates for data validation\n\n\n\n\n\n\nProject Overview\nDocumentation Structure\nGetting Started\nDocumentation Updates\nContact\n\n\n\n\nNew team members should:\n\nReview the Project Overview\nSet up their development environment using the Team Wiki\nFamiliarize themselves with the Documentation Structure\nGo through the Analysis Notebooks to understand our analytical approach\n\n\n\n\nThis documentation is maintained by the Data Analytics team. Updates are made on a regular basis to ensure all information remains current and relevant.\nLast updated: 2023-05-01\n\n\n\nFor questions or suggestions regarding this documentation, please contact:\n\nData Analytics Team: data.analytics@example.com\nProject Lead: project.lead@example.com"
  },
  {
    "objectID": "analytical_engineering_docs.html#project-overview",
    "href": "analytical_engineering_docs.html#project-overview",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "The GDPR Fines Analysis project aims to analyze and visualize GDPR fine data to provide insights into compliance trends, geographic distribution, and industry impacts. This documentation supports the team in maintaining high standards of data quality, analytical rigor, and project management."
  },
  {
    "objectID": "analytical_engineering_docs.html#documentation-structure",
    "href": "analytical_engineering_docs.html#documentation-structure",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "Project Planning - Templates and guidelines for project management\nMeeting Templates - Templates for various team meetings\nFindings Templates - Templates for documenting analysis results\nIssues and Bugs - Templates for tracking and managing issues\n\n\n\n\n\nReference Materials - Technical standards and best practices\nTeam Wiki - Development environment setup and team resources\nAnalysis Notebooks - Templates for data analysis\nData Quality Checks - Templates for data validation"
  },
  {
    "objectID": "analytical_engineering_docs.html#quick-links",
    "href": "analytical_engineering_docs.html#quick-links",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "Project Overview\nDocumentation Structure\nGetting Started\nDocumentation Updates\nContact"
  },
  {
    "objectID": "analytical_engineering_docs.html#getting-started",
    "href": "analytical_engineering_docs.html#getting-started",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "New team members should:\n\nReview the Project Overview\nSet up their development environment using the Team Wiki\nFamiliarize themselves with the Documentation Structure\nGo through the Analysis Notebooks to understand our analytical approach"
  },
  {
    "objectID": "analytical_engineering_docs.html#documentation-updates",
    "href": "analytical_engineering_docs.html#documentation-updates",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "This documentation is maintained by the Data Analytics team. Updates are made on a regular basis to ensure all information remains current and relevant.\nLast updated: 2023-05-01"
  },
  {
    "objectID": "analytical_engineering_docs.html#contact",
    "href": "analytical_engineering_docs.html#contact",
    "title": "Analytical Engineering Documentation",
    "section": "",
    "text": "For questions or suggestions regarding this documentation, please contact:\n\nData Analytics Team: data.analytics@example.com\nProject Lead: project.lead@example.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lean Analytics Journey",
    "section": "",
    "text": "“If you write a problem down clearly, then the matter is half solved.”\n— Kidlens Law"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Lean Analytics Journey",
    "section": "Books",
    "text": "Books\n\nBook 1 - Kidlens Law\nExplore the fundamentals of data quality and analysis through the lens of Kidlens Law.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nData Quality Assessment\nGDPR Fines Analysis\nProblem Definition\n\n\n\n\n\nBook 2 - Documentation Journey\nLearn about the importance of documentation and knowledge management in data projects.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nDMBOK2 Principles\nHub and Spoke Model\nDocumentation Best Practices\n\n\n\n\n\nBook 3 - Getting Your Hands Dirty\nDive into practical implementation and hands-on data engineering.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nImplementation Guide\nCode Examples\nBest Practices\n\n\n\n\n\nBook 4 - ARGH Framework\nDiscover the ARGH framework for defining and achieving excellence in data projects.\n\n\n\n\n\n\nFramework Components\n\n\n\n\nActionable: Insights that drive decisions\nReliable: Trustworthy and consistent data\nGoverned: Controlled and compliant processes\nHarmonized: Integrated and synchronized systems\n\n\n\n\n\nAndi’s Complete Story\nFollow Andi’s journey through all four books in a single narrative."
  },
  {
    "objectID": "index.html#andis-templates",
    "href": "index.html#andis-templates",
    "title": "Lean Analytics Journey",
    "section": "Andi’s Templates",
    "text": "Andi’s Templates\nAccess our comprehensive documentation and templates:\n\nProject Management\n\nEngineering Documentation - Main documentation page\nProject Planning - Templates for project management\nMeeting Templates - Templates for team meetings\n\n\n\nAnalysis Documentation\n\nFindings Templates - Templates for documenting analysis results\nData Quality Checks - Templates for data validation\n\n\n\nTechnical Resources\n\nIssues and Bugs - Templates for tracking and managing issues\nReference Materials - Technical standards and best practices\nAnalysis Notebooks - Templates for data analysis"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Lean Analytics Journey",
    "section": "Getting Started",
    "text": "Getting Started\n\nStart with Book 1\nExplore Documentation in Book 2\nGet Practical in Book 3\nDefine Excellence in Book 4"
  },
  {
    "objectID": "index.html#key-topics-3",
    "href": "index.html#key-topics-3",
    "title": "Lean Analytics Journey",
    "section": "Key Topics",
    "text": "Key Topics\n\nData Quality → Book 1\nDocumentation → Book 2\nImplementation → Book 3\nExcellence → Book 4\n\nRemember: The journey to excellence is continuous. Each book represents a step forward in mastering data analytics and documentation practices."
  },
  {
    "objectID": "AndiStory.html",
    "href": "AndiStory.html",
    "title": "Andi’s Data Journey",
    "section": "",
    "text": "“Learn as if you will live forever, earn as if you will die tomorrow, return as if this is your legacy.”\n— Stoic Philosophy\n\nThis is the story of Andi, a data analyst who embarks on a journey to transform raw data into meaningful insights. Through her experiences, we’ll explore the principles of data quality, documentation, implementation, and excellence in data analytics.\n\n\n\n\nTo support efficient data analysis and documentation, we’ve set up a Docker-based development environment that can be run locally on your machine. This environment includes:\n\nVS Code in browser for easy access and collaboration\nPostgreSQL for data storage\nRedis for caching\npgAdmin for database management\n\nNote: This development environment must be run locally on your machine. GitHub Pages cannot run Docker containers or host development environments.\nTo set up the development environment locally: 1. Clone the repository 2. Navigate to the _DevelopmentEnvironment directory 3. Run docker-compose up -d\nThe development environment can then be accessed at: - VS Code: http://localhost:8080 - pgAdmin: http://localhost:5050 - PostgreSQL: localhost:5432 - Redis: localhost:6379\n\n\n\nTo maintain consistency and quality in our data analysis work, we’ve created a set of templates that cover various aspects of the data analysis process:\n\nAnalysis Notebooks\n\nStandardized format for documenting data analysis workflows\nIncludes sections for data exploration, cleaning, and visualization\nEnsures reproducibility and transparency\n\nData Quality Checks\n\nTemplates for assessing data quality\nCovers completeness, accuracy, consistency, and timeliness\nHelps identify and document data issues\n\nFindings Templates\n\nStructured format for presenting analysis results\nIncludes executive summary, methodology, and recommendations\nEnsures clear communication of insights\n\nIssues and Bugs Tracking\n\nTemplates for documenting data-related issues\nTracks resolution progress and impact\nMaintains a history of data quality improvements\n\nMeeting Templates\n\nStandardized format for data team meetings\nCovers agenda, action items, and follow-ups\nEnsures effective communication and accountability\n\nProject Planning\n\nTemplates for planning data analysis projects\nIncludes scope, timeline, and resource allocation\nHelps manage project execution and delivery\n\nReference Materials\n\nTemplates for documenting data sources and methodologies\nMaintains a knowledge base of analysis approaches\nSupports team learning and development\n\n\nThese templates are available in the Templates section of our documentation and can be accessed through the development environment."
  },
  {
    "objectID": "AndiStory.html#development-environment-and-templates",
    "href": "AndiStory.html#development-environment-and-templates",
    "title": "Andi’s Data Journey",
    "section": "",
    "text": "To support efficient data analysis and documentation, we’ve set up a Docker-based development environment that can be run locally on your machine. This environment includes:\n\nVS Code in browser for easy access and collaboration\nPostgreSQL for data storage\nRedis for caching\npgAdmin for database management\n\nNote: This development environment must be run locally on your machine. GitHub Pages cannot run Docker containers or host development environments.\nTo set up the development environment locally: 1. Clone the repository 2. Navigate to the _DevelopmentEnvironment directory 3. Run docker-compose up -d\nThe development environment can then be accessed at: - VS Code: http://localhost:8080 - pgAdmin: http://localhost:5050 - PostgreSQL: localhost:5432 - Redis: localhost:6379\n\n\n\nTo maintain consistency and quality in our data analysis work, we’ve created a set of templates that cover various aspects of the data analysis process:\n\nAnalysis Notebooks\n\nStandardized format for documenting data analysis workflows\nIncludes sections for data exploration, cleaning, and visualization\nEnsures reproducibility and transparency\n\nData Quality Checks\n\nTemplates for assessing data quality\nCovers completeness, accuracy, consistency, and timeliness\nHelps identify and document data issues\n\nFindings Templates\n\nStructured format for presenting analysis results\nIncludes executive summary, methodology, and recommendations\nEnsures clear communication of insights\n\nIssues and Bugs Tracking\n\nTemplates for documenting data-related issues\nTracks resolution progress and impact\nMaintains a history of data quality improvements\n\nMeeting Templates\n\nStandardized format for data team meetings\nCovers agenda, action items, and follow-ups\nEnsures effective communication and accountability\n\nProject Planning\n\nTemplates for planning data analysis projects\nIncludes scope, timeline, and resource allocation\nHelps manage project execution and delivery\n\nReference Materials\n\nTemplates for documenting data sources and methodologies\nMaintains a knowledge base of analysis approaches\nSupports team learning and development\n\n\nThese templates are available in the Templates section of our documentation and can be accessed through the development environment."
  },
  {
    "objectID": "AndiStory.html#the-art-and-science-of-data-analysis",
    "href": "AndiStory.html#the-art-and-science-of-data-analysis",
    "title": "Andi’s Data Journey",
    "section": "The Art and Science of Data Analysis",
    "text": "The Art and Science of Data Analysis\n\nA Day in the Life: The Six Thinking Hats of a Data Analyst\nLet me tell you a story about Andi, a data analyst working on understanding GDPR compliance patterns. Her journey illustrates how modern data analysts combine analytical engineering with critical thinking using Edward de Bono’s Six Thinking Hats approach and the DMAIC methodology.\n\n\nThe White Hat: Facts and Information\nAndi starts her day by gathering facts about GDPR fines across Europe. Like a detective, she collects raw data about fines, violations, and company responses. This is where analytical engineering begins - the systematic process of collecting, cleaning, and organizing data. She knows that good analysis starts with quality data, just as a good house needs a solid foundation.\nDMAIC Tools Used: - Define: Project Charter, SIPOC Diagram - Measure: Data Collection Plan, Operational Definitions - Analyze: Data Mining, Statistical Analysis\n\n\nThe Red Hat: Intuition and Feelings\nAs she dives into the data, Andi notices patterns that trigger her intuition. Some companies seem to repeatedly violate certain articles, while others quickly adapt after their first fine. She doesn’t ignore these gut feelings - they’re valuable indicators of where to look deeper. This emotional intelligence, combined with technical skills, makes a data analyst more than just a number cruncher.\nDMAIC Tools Used: - Measure: Voice of Customer (VOC) - Analyze: Brainstorming - Improve: Impact Analysis\n\n\nThe Black Hat: Critical Judgment\nAndi puts on her critical thinking hat to identify potential issues. She asks tough questions: - Are there gaps in the data collection? - Could there be biases in how different countries report violations? - What limitations might affect our conclusions? This cautious approach is essential in analytical engineering, where understanding data limitations is as important as the analysis itself.\nDMAIC Tools Used: - Define: Risk Assessment - Measure: Measurement System Analysis (MSA) - Control: Control Charts, Error Proofing\n\n\nThe Yellow Hat: Optimistic Opportunities\nLooking at the bright side, Andi sees opportunities in the challenges: - Patterns in the data could help companies prevent future violations - Analysis could lead to better compliance strategies - Insights might help regulators focus their efforts more effectively This optimistic perspective helps her frame the analysis in terms of solutions rather than just problems.\nDMAIC Tools Used: - Improve: Solution Selection Matrix - Control: Process Control Plan - Define: Benefits Analysis\n\n\nThe Green Hat: Creative Solutions\nNow comes the creative part. Andi combines different analytical approaches: - Visualizing fine distributions to spot trends - Creating interactive dashboards for stakeholders - Developing automated quality checks for ongoing monitoring This is where analytical engineering shines - using technical creativity to solve real business problems.\nDMAIC Tools Used: - Analyze: Root Cause Analysis - Improve: Design of Experiments (DOE) - Control: Visual Management Systems\n\n\nThe Blue Hat: Process Control\nFinally, Andi steps back to organize her thoughts and plan next steps: - Document the analysis process for reproducibility - Structure findings in a clear narrative - Plan future iterations and improvements This systematic approach ensures that her work is not just insightful but also actionable and maintainable.\nDMAIC Tools Used: - Define: Project Management Plan - Control: Documentation Systems - Improve: Implementation Plan"
  },
  {
    "objectID": "AndiStory.html#the-modern-data-analyst",
    "href": "AndiStory.html#the-modern-data-analyst",
    "title": "Andi’s Data Journey",
    "section": "The Modern Data Analyst",
    "text": "The Modern Data Analyst\nToday’s data analyst is part detective, part engineer, and part storyteller. They: - Build data pipelines that transform raw data into insights - Create automated processes for consistent analysis - Develop visualizations that make complex patterns understandable - Tell stories that connect data to business decisions"
  },
  {
    "objectID": "AndiStory.html#analytical-engineering-in-practice",
    "href": "AndiStory.html#analytical-engineering-in-practice",
    "title": "Andi’s Data Journey",
    "section": "Analytical Engineering in Practice",
    "text": "Analytical Engineering in Practice\nAnalytical engineering is the bridge between raw data and business value. It involves: - Designing robust data processing workflows - Implementing quality control measures - Creating reusable analysis components - Building scalable solutions for growing data needs\nThis combination of technical skills and critical thinking enables data analysts to turn information into action, helping organizations make better decisions through data."
  },
  {
    "objectID": "AndiStory.html#the-art-of-data-documentation",
    "href": "AndiStory.html#the-art-of-data-documentation",
    "title": "Andi’s Data Journey",
    "section": "The Art of Data Documentation",
    "text": "The Art of Data Documentation\n\nAndi’s Next Challenge: Building a Knowledge Hub\nAfter successfully analyzing the GDPR fines data, Andi faces a new challenge: creating a sustainable documentation system that will help her team and organization maintain and build upon their data knowledge. Let’s follow her journey as she applies DMBOK2 principles and the hub and spoke model to transform raw documentation into actionable knowledge.\n\n\nThe White Hat: Understanding DMBOK2\nAndi begins by gathering facts about DMBOK2’s documentation principles: - Data Governance - Data Architecture - Data Quality - Metadata Management - Data Security\nDocumentation Tools Used: - Knowledge Repository Setup - Metadata Templates - Data Lineage Diagrams - Process Flow Documentation - Security Classification Schema\n\n\nThe Red Hat: Feeling the Documentation Pain\nAs she dives deeper, Andi empathizes with her team’s documentation struggles: - Scattered information across multiple systems - Outdated documentation - Inconsistent formats - Difficulty finding relevant information - Knowledge silos\nHub and Spoke Implementation: - Central Knowledge Hub (Confluence) - Department-specific Spokes - Cross-reference System - Version Control - Access Management\n\n\nThe Black Hat: Critical Documentation Challenges\nAndi identifies potential issues in the current documentation approach: - Information overload - Maintenance overhead - Access control complexity - Version control challenges - Resource constraints\nDMBOK2 Governance Elements: - Documentation Standards - Review Processes - Update Procedures - Quality Metrics - Compliance Requirements\n\n\nThe Yellow Hat: Documentation Opportunities\nShe sees several opportunities for improvement: - Automated documentation generation - Interactive knowledge bases - Collaborative editing - Real-time updates - Integration with existing tools\nKnowledge Management Benefits: - Reduced onboarding time - Improved decision making - Better compliance tracking - Enhanced collaboration - Faster problem resolution\n\n\nThe Green Hat: Creative Documentation Solutions\nAndi develops innovative approaches to documentation: - Interactive data dictionaries - Visual process maps - Automated metadata extraction - Wiki-style knowledge base - Documentation chatbot\nHub and Spoke Features: - Central Documentation Portal - Department Workspaces - Cross-linking System - Search Functionality - Collaboration Tools\n\n\nThe Blue Hat: Documentation Strategy\nFinally, Andi creates a structured plan: - Define documentation standards - Implement hub and spoke model - Establish review processes - Create maintenance schedules - Monitor documentation health\nImplementation Roadmap: - Phase 1: Core Hub Setup - Phase 2: Spoke Development - Phase 3: Integration - Phase 4: Training - Phase 5: Optimization"
  },
  {
    "objectID": "AndiStory.html#implementation-deep-dive",
    "href": "AndiStory.html#implementation-deep-dive",
    "title": "Andi’s Data Journey",
    "section": "Implementation Deep Dive",
    "text": "Implementation Deep Dive\n\nAndi’s Implementation Journey\nAfter establishing the documentation framework and metrics, Andi moves into the implementation phase. Her team needs practical guidance on turning theory into practice. Let’s follow her journey of transforming concepts into working solutions.\n\n\nSetting Up the Development Environment\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#01579b', 'primaryTextColor': '#ffffff', 'primaryBorderColor': '#01579b', 'secondaryColor': '#fff3e0', 'secondaryTextColor': '#000000', 'secondaryBorderColor': '#ff6f00', 'tertiaryColor': '#f5f5f5', 'tertiaryTextColor': '#000000', 'tertiaryBorderColor': '#666' }}}%%\nflowchart LR\n    %% Main Development Environment Node\n    A[Development Environment] --&gt; B[Project Structure]\n    A --&gt; H[Configuration]\n    A --&gt; K[Technology Stack]\n    A --&gt; R[Development Tools]\n    \n    %% Project Structure\n    subgraph Project[Project Structure]\n        direction TB\n        B --&gt; C[data/]\n        C --&gt; C1[raw/]\n        C --&gt; C2[processed/]\n        C --&gt; C3[external/]\n        \n        B --&gt; D[src/]\n        D --&gt; D1[etl/]\n        D --&gt; D2[analysis/]\n        D --&gt; D3[visualization/]\n        D --&gt; D4[utils/]\n        \n        B --&gt; E[tests/]\n        E --&gt; E1[unit/]\n        E --&gt; E2[integration/]\n        E --&gt; E3[e2e/]\n        \n        B --&gt; F[docs/]\n        F --&gt; F1[api/]\n        F --&gt; F2[user_guides/]\n        F --&gt; F3[technical_docs/]\n        \n        B --&gt; G[notebooks/]\n        G --&gt; G1[exploratory/]\n        G --&gt; G2[reporting/]\n    end\n    \n    %% Configuration\n    subgraph Config[Configuration]\n        direction TB\n        H --&gt; I[requirements.txt]\n        H --&gt; J[.env.template]\n        H --&gt; H1[setup.py]\n        H --&gt; H2[config/]\n        H2 --&gt; H2A[dev.yaml]\n        H2 --&gt; H2B[prod.yaml]\n        H2 --&gt; H2C[test.yaml]\n    end\n    \n    %% Technology Stack\n    subgraph Tech[Technology Stack]\n        direction TB\n        K --&gt; L[Backend]\n        L --&gt; L1[\"Python 3.9+&lt;br/&gt;(Core Language)\"]\n        L --&gt; L2[\"FastAPI&lt;br/&gt;(Web Framework)\"]\n        L --&gt; L3[\"SQLAlchemy&lt;br/&gt;(ORM)\"]\n        L --&gt; L4[\"Alembic&lt;br/&gt;(Migrations)\"]\n        \n        K --&gt; M[Data Processing]\n        M --&gt; M1[\"Pandas&lt;br/&gt;(Data Analysis)\"]\n        M --&gt; M2[\"NumPy&lt;br/&gt;(Numerical Ops)\"]\n        M --&gt; M3[\"Scikit-learn&lt;br/&gt;(ML)\"]\n        M --&gt; M4[\"PySpark&lt;br/&gt;(Big Data)\"]\n        \n        K --&gt; N[Visualization]\n        N --&gt; N1[\"Plotly&lt;br/&gt;(Interactive Viz)\"]\n        N --&gt; N2[\"Dash&lt;br/&gt;(Dashboards)\"]\n        N --&gt; N3[\"Streamlit&lt;br/&gt;(Data Apps)\"]\n        N --&gt; N4[\"D3.js&lt;br/&gt;(Custom Viz)\"]\n        \n        K --&gt; O[Database]\n        O --&gt; O1[\"PostgreSQL&lt;br/&gt;(Primary DB)\"]\n        O --&gt; O2[\"Redis&lt;br/&gt;(Caching)\"]\n        O --&gt; O3[\"MongoDB&lt;br/&gt;(Document DB)\"]\n        O --&gt; O4[\"DuckDB&lt;br/&gt;(Analytics)\"]\n        \n        K --&gt; P[Infrastructure]\n        P --&gt; P1[\"Docker&lt;br/&gt;(Containers)\"]\n        P --&gt; P2[\"Kubernetes&lt;br/&gt;(Orchestration)\"]\n        P --&gt; P3[\"AWS&lt;br/&gt;(Cloud)\"]\n        P --&gt; P4[\"GitHub Actions&lt;br/&gt;(CI/CD)\"]\n    end\n    \n    %% Development Tools\n    subgraph Tools[Development Tools]\n        direction TB\n        R --&gt; S[IDEs]\n        S --&gt; S1[\"VS Code&lt;br/&gt;(Browser-based)\"]\n        S --&gt; S2[\"PyCharm&lt;br/&gt;(Python IDE)\"]\n        S --&gt; S3[\"Jupyter Lab&lt;br/&gt;(Notebooks)\"]\n        \n        R --&gt; T[Version Control]\n        T --&gt; T1[\"Git&lt;br/&gt;(Code Version)\"]\n        T --&gt; T2[\"DVC&lt;br/&gt;(Data Version)\"]\n        T --&gt; T3[\"GitHub&lt;br/&gt;(Repository)\"]\n        \n        R --&gt; U[Documentation]\n        U --&gt; U1[\"Quarto&lt;br/&gt;(Tech Writing)\"]\n        U --&gt; U2[\"Sphinx&lt;br/&gt;(API Docs)\"]\n        U --&gt; U3[\"MkDocs&lt;br/&gt;(Project Docs)\"]\n        \n        R --&gt; V[Quality & Testing]\n        V --&gt; V1[\"pytest&lt;br/&gt;(Testing)\"]\n        V --&gt; V2[\"Black&lt;br/&gt;(Formatting)\"]\n        V --&gt; V3[\"isort&lt;br/&gt;(Import Sort)\"]\n        V --&gt; V4[\"mypy&lt;br/&gt;(Type Check)\"]\n    end\n    \n    %% Styling\n    style A fill:#01579b,stroke:#01579b,stroke-width:2px,color:#ffffff\n    style K fill:#01579b,stroke:#01579b,stroke-width:2px,color:#ffffff\n    style L1,M1,N1,O1,P1,S1,T1,U1,V1 fill:#fff3e0,stroke:#ff6f00,stroke-width:2px,color:#000000\n    \n    %% Subgraph styling\n    style Project fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Config fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Tech fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Tools fill:#f5f5f5,stroke:#666,stroke-width:2px\n\nclick A \"javascript:void(0);\" \"Click to download as JPEG\"\n\n\n\n\n\n\n\n\nKey Components Legend\nThe highlighted components (in orange) represent the primary tools in each category:\n\n\n\nComponent\nDescription\nRole\n\n\n\n\nL1\nPython 3.9+\nCore programming language for all development\n\n\nM1\nPandas\nPrimary library for data analysis and manipulation\n\n\nN1\nPlotly\nMain tool for creating interactive visualizations\n\n\nO1\nPostgreSQL\nPrimary database for data storage and management\n\n\nP1\nDocker\nContainer platform for consistent environments\n\n\nS1\nVS Code\nBrowser-based integrated development environment\n\n\nT1\nGit\nVersion control system for code management\n\n\nU1\nQuarto\nTechnical documentation and report generation\n\n\nV1\npytest\nTesting framework for code quality assurance\n\n\n\nThese tools form the foundation of our development stack, each chosen for its reliability, community support, and integration capabilities."
  },
  {
    "objectID": "AndiStory.html#setting-up-the-development-environment-1",
    "href": "AndiStory.html#setting-up-the-development-environment-1",
    "title": "Andi’s Data Journey",
    "section": "Setting Up the Development Environment",
    "text": "Setting Up the Development Environment\nThe development environment is structured to support efficient data analysis and robust software development practices. Here’s a detailed breakdown of each component:\n\nProject Structure\nThe project follows a modular organization: - data/: Manages different data stages (raw, processed, external) - src/: Contains all source code with specialized subdirectories - tests/: Houses different types of tests - docs/: Stores various documentation types - notebooks/: Contains Jupyter notebooks for analysis and reporting\n\n\nConfiguration Management\nEssential configuration files and templates: - requirements.txt: Lists all Python dependencies - .env.template: Environment variable templates - setup.py: Package installation configuration - config/: Environment-specific configurations\n\n\nTechnology Stack\nOur comprehensive stack includes: 1. Backend: Python 3.9+ with FastAPI and SQLAlchemy 2. Data Processing: Pandas, NumPy, Scikit-learn, PySpark 3. Visualization: Plotly, Dash, Streamlit, D3.js 4. Database: PostgreSQL, Redis, MongoDB, DuckDB 5. Infrastructure: Docker, Kubernetes, AWS, GitHub Actions\n\n\nDevelopment Tools\nEssential tools for efficient development: 1. IDEs: - VS Code (browser-based via code-server) - PyCharm - Jupyter Lab 2. Version Control: Git, DVC, GitHub 3. Documentation: Quarto, Sphinx, MkDocs 4. Quality & Testing: pytest, Black, isort, mypy\n\n\nGetting Started\nTo set up the development environment:\n\nClone the repository:\n\ngit clone &lt;repository-url&gt;\ncd &lt;project-directory&gt;\n\nStart the Docker containers:\n\ncd _DevelopmentEnvironment\ndocker-compose up -d\nThis will launch: - PostgreSQL database (port 5432) - Redis cache service (port 6379) - PgAdmin interface (port 5050) - VS Code in browser via code-server (port 8080)\n\nCreate and activate a virtual environment:\n\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\nInstall dependencies:\n\npip install -r requirements.txt\n\nConfigure environment variables:\n\ncp .env.template .env\n# Edit .env with your specific configurations\n\nInitialize the database:\n\npython src/db/init_db.py\n\nRun tests to verify setup:\n\npytest tests/\n\n\nBrowser-Based Development\nOne of the key features of our development environment is the integration of code-server, which provides:\n\nFull VS Code experience in a browser\nAccess to the development environment from any device\nConsistent development environment for all team members\nPre-configured extensions and settings\n\nTo access the browser-based VS Code environment: - URL: http://localhost:8080 - Password: andi_password (customizable)\n\n\nBest Practices\n\nAlways work in a virtual environment\nKeep dependencies updated\nFollow the coding style guide\nWrite tests for new features\nDocument your code and processes\nUse version control for both code and data"
  },
  {
    "objectID": "AndiStory.html#data-processing-and-analysis",
    "href": "AndiStory.html#data-processing-and-analysis",
    "title": "Andi’s Data Journey",
    "section": "Data Processing and Analysis",
    "text": "Data Processing and Analysis\n\nOverview\n\n\nDataset Shape: (209, 5)\n\nData Types:\ndate           datetime64[ns]\nfine_amount           float64\ncountry                object\narticle                object\ntype                   object\ndtype: object\n\nSample Data:\n\n\n\n\n\n\n\n\n\ndate\nfine_amount\ncountry\narticle\ntype\n\n\n\n\n0\n2020-01-05\n46400.765203\nNetherlands\nArt. 32\nHealthcare\n\n\n1\n2020-01-12\n17900.851280\nSpain\nArt. 6\nTechnology\n\n\n2\n2020-01-19\n58193.724832\nGermany\nArt. 5\nTechnology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\ndate\n209\n2022-01-02 00:00:00\n2020-01-05 00:00:00\n2021-01-03 00:00:00\n2022-01-02 00:00:00\n2023-01-01 00:00:00\n2023-12-31 00:00:00\nNaN\n\n\nfine_amount\n209.0\n57120.991619\n432.846146\n7684.190837\n24036.122397\n47694.507784\n1303096.287149\n128799.183992\n\n\n\n\n\n\n\n\n\nData Cleaning\n\n\nMissing Values:\n\n\ndate           0\nfine_amount    0\ncountry        0\narticle        0\ntype           0\ndtype: int64\n\n\n\nDuplicate Rows: 0\n\nAfter Cleaning - Missing Values:\n\n\ndate           0\nfine_amount    0\ncountry        0\narticle        0\ntype           0\ndtype: int64\n\n\n\n\nExploratory Analysis\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nType\nCount\nMean Fine\nMedian Fine\nTotal Fines\n\n\n\n\n0\nFinance\n61\n52344.463191\n16505.658704\n3.193012e+06\n\n\n1\nHealthcare\n58\n59444.883400\n28110.756784\n3.447803e+06\n\n\n2\nRetail\n43\n60397.078421\n21585.005502\n2.597074e+06\n\n\n3\nTechnology\n47\n57455.263498\n25577.722778\n2.700397e+06\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\ndate\nyear\nmonth\nquarter\nfine_amount\nseverity\narticle\nviolation_category\n\n\n\n\n0\n2020-01-05\n2020\n1\n1\n46400.765203\nMedium\nArt. 32\nData Security\n\n\n1\n2020-01-12\n2020\n1\n1\n17900.851280\nMedium\nArt. 6\nLawful Basis\n\n\n2\n2020-01-19\n2020\n1\n1\n58193.724832\nMedium\nArt. 5\nOther\n\n\n3\n2020-01-26\n2020\n1\n1\n216326.643890\nHigh\nArt. 6\nLawful Basis\n\n\n4\n2020-02-02\n2020\n2\n1\n15502.707079\nMedium\nArt. 13\nOther\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nData Validation\n\n\n\n\n\n\n\n\n\nrule_description\npass_count\nfail_count\npass_percentage\n\n\n\n\nfine_amount\nCheck valid fine_amount\n209\n0\n100.0\n\n\ndate\nCheck valid date\n209\n0\n100.0\n\n\ncountry\nCheck valid country\n209\n0\n100.0\n\n\ntype\nCheck valid type\n209\n0\n100.0\n\n\n\n\n\n\n\nIdentified 21 outliers in fine amounts\n\n\n\n\n\n\n\n\n\ncountry\ntype\nfine_amount\ndate\n\n\n\n\n3\nSpain\nRetail\n2.163266e+05\n2020-01-26\n\n\n6\nFrance\nTechnology\n2.353477e+05\n2020-02-16\n\n\n20\nGermany\nHealthcare\n1.984859e+05\n2020-05-24\n\n\n31\nNetherlands\nTechnology\n3.544836e+05\n2020-08-09\n\n\n65\nNetherlands\nFinance\n1.684442e+05\n2021-04-04\n\n\n71\nSpain\nTechnology\n2.212514e+05\n2021-05-16\n\n\n73\nFrance\nFinance\n2.302603e+05\n2021-05-30\n\n\n82\nFrance\nFinance\n2.021653e+05\n2021-08-01\n\n\n106\nNetherlands\nRetail\n3.729795e+05\n2022-01-16\n\n\n113\nSpain\nRetail\n8.863464e+05\n2022-03-06\n\n\n118\nNetherlands\nHealthcare\n1.223002e+05\n2022-04-10\n\n\n122\nGermany\nRetail\n1.806274e+05\n2022-05-08\n\n\n125\nFrance\nFinance\n5.887067e+05\n2022-05-29\n\n\n135\nNetherlands\nTechnology\n2.252355e+05\n2022-08-07\n\n\n141\nNetherlands\nFinance\n1.564848e+05\n2022-09-18\n\n\n156\nNetherlands\nTechnology\n3.617330e+05\n2023-01-01\n\n\n162\nSpain\nHealthcare\n1.252283e+05\n2023-02-12\n\n\n167\nGermany\nFinance\n3.789613e+05\n2023-03-19\n\n\n177\nItaly\nHealthcare\n1.949115e+05\n2023-05-28\n\n\n179\nNetherlands\nHealthcare\n1.303096e+06\n2023-06-11\n\n\n202\nGermany\nTechnology\n1.118123e+05\n2023-11-19"
  },
  {
    "objectID": "AndiStory.html#building-the-data-pipeline",
    "href": "AndiStory.html#building-the-data-pipeline",
    "title": "Andi’s Data Journey",
    "section": "Building the Data Pipeline",
    "text": "Building the Data Pipeline\n\nETL Process for GDPR Fines\n\n\n\nETL Workflow Components\n\n\n\n\n\n\n\nStage\nImplementation\nDescription\n\n\n\n\nData Collection\nWeb Scraping + API\nCollect fines data from enforcement tracker and official sources\n\n\nData Validation\nSchema Validation\nValidate data against predefined schemas and rules\n\n\nData Transformation\nData Normalization\nTransform raw data into normalized database format\n\n\nData Loading\nDatabase Loading\nLoad processed data into PostgreSQL database\n\n\nData Quality Check\nAutomated Testing\nRun automated quality checks and validations\n\n\nDocumentation Update\nAuto-Documentation\nUpdate documentation with new data lineage\n\n\nNotification System\nAlert System\nSend notifications for updates and issues"
  },
  {
    "objectID": "AndiStory.html#the-argh-framework",
    "href": "AndiStory.html#the-argh-framework",
    "title": "Andi’s Data Journey",
    "section": "The ARGH Framework",
    "text": "The ARGH Framework\n\nUnderstanding ARGH\nAfter implementing the core systems, Andi realizes that defining “good” is crucial for sustainable success. She develops the ARGH framework:\n\nActionable: Insights that drive decisions\nReliable: Trustworthy and consistent data\nGoverned: Controlled and compliant processes\nHarmonized: Integrated and synchronized systems\n\n\n\n\nARGH Framework Components\n\n\nPillar\nKey Metrics\nSuccess Criteria\n\n\n\n\nActionable\nDecision Impact Rate\nEvery insight leads to clear action items and measurable outcomes\n\n\nReliable\nData Quality Score\nData consistency above 99%, with full lineage and validation\n\n\nGoverned\nCompliance Rate\nComplete audit trails and policy compliance across all processes\n\n\nHarmonized\nIntegration Success\nSeamless data flow between all systems with zero manual intervention"
  },
  {
    "objectID": "AndiStory.html#the-never-ending-journey",
    "href": "AndiStory.html#the-never-ending-journey",
    "title": "Andi’s Data Journey",
    "section": "The Never-Ending Journey",
    "text": "The Never-Ending Journey\nAndi’s journey teaches us that “good” is not a destination but a continuous journey of improvement. The ARGH framework provides a compass for this journey:\n\nActionable: Every insight should drive meaningful change\nReliable: Trust is built on consistent quality\nGoverned: Control enables freedom\nHarmonized: Integration creates value\n\nRemember: &gt; “The goal is not to be perfect at everything, but to be excellent at what matters most to your organization.”\nThe future of data analytics is not just about technology—it’s about creating value through actionable insights, reliable systems, governed processes, and harmonized operations. As Andi would say, “ARGH!” might sound like frustration, but it’s actually the sound of excellence in the making."
  },
  {
    "objectID": "AndiStory.html#methodologies-and-frameworks",
    "href": "AndiStory.html#methodologies-and-frameworks",
    "title": "Andi’s Data Journey",
    "section": "Methodologies and Frameworks",
    "text": "Methodologies and Frameworks\n\nDMAIC\nDefinition: A data-driven improvement cycle used to improve, optimize and stabilize business processes and designs. - Define: Identify the problem and project goals - Measure: Collect data to understand the current state - Analyze: Identify root causes of problems - Improve: Implement and verify solutions - Control: Maintain the improvements\n\n\nSix Thinking Hats\nDefinition: A thinking tool for group discussion and individual thinking involving six colored hats: - White Hat: Facts and information - Red Hat: Feelings and intuition - Black Hat: Critical judgment - Yellow Hat: Positive aspects - Green Hat: Creativity - Blue Hat: Process control\n\n\nARGH Framework\nDefinition: A framework for achieving excellence in data analytics: - Actionable: Insights that drive decisions - Reliable: Trustworthy and consistent data - Governed: Controlled and compliant processes - Harmonized: Integrated and synchronized systems\n\n\nDMBOK2\nDefinition: Data Management Body of Knowledge, a comprehensive framework for data management: - Data Governance - Data Architecture - Data Quality - Metadata Management - Data Security"
  },
  {
    "objectID": "AndiStory.html#technical-terms",
    "href": "AndiStory.html#technical-terms",
    "title": "Andi’s Data Journey",
    "section": "Technical Terms",
    "text": "Technical Terms\n\nData Quality Dimensions\n\nCompleteness: The degree to which all required data is present\nAccuracy: The degree to which data correctly describes the real-world object or event\nConsistency: The degree to which data maintains consistency across the data set\nTimeliness: The degree to which data represents reality from the required point in time\nRelevance: The degree to which data is applicable and helpful for the task at hand\n\n\n\nAnalytical Engineering\nDefinition: The practice of designing, implementing, and maintaining systems that transform raw data into actionable insights: - Data Pipeline Development - Quality Control Implementation - Process Automation - Scalable Solutions Design"
  },
  {
    "objectID": "AndiStory.html#books-and-publications",
    "href": "AndiStory.html#books-and-publications",
    "title": "Andi’s Data Journey",
    "section": "Books and Publications",
    "text": "Books and Publications\n\nDe Bono, E. (1985). Six Thinking Hats: An Essential Approach to Business Management. Little, Brown and Company.\nDAMA International. (2017). DAMA-DMBOK: Data Management Body of Knowledge (2nd Edition). Technics Publications.\nPyzdek, T., & Keller, P. (2018). The Six Sigma Handbook (5th Edition). McGraw-Hill Education."
  },
  {
    "objectID": "AndiStory.html#online-resources",
    "href": "AndiStory.html#online-resources",
    "title": "Andi’s Data Journey",
    "section": "Online Resources",
    "text": "Online Resources\n\nInternational Organization for Standardization. (2015). ISO 9001:2015 Quality Management Systems. https://www.iso.org/standard/62085.html\nGDPR.eu. (2018). Complete guide to GDPR compliance. https://gdpr.eu/\nData Management Association (DAMA). https://www.dama.org/"
  },
  {
    "objectID": "AndiStory.html#industry-standards-and-frameworks",
    "href": "AndiStory.html#industry-standards-and-frameworks",
    "title": "Andi’s Data Journey",
    "section": "Industry Standards and Frameworks",
    "text": "Industry Standards and Frameworks\n\nCOBIT (Control Objectives for Information and Related Technologies)\n\nFramework for IT management and IT governance\n\nITIL (Information Technology Infrastructure Library)\n\nSet of detailed practices for IT service management\n\nCRISP-DM (Cross-Industry Standard Process for Data Mining)\n\nIndustry-standard process model for data mining projects"
  },
  {
    "objectID": "AndiStory.html#additional-reading",
    "href": "AndiStory.html#additional-reading",
    "title": "Andi’s Data Journey",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nAgile Data Warehouse Design\n\nLawrence Corr & Jim Stagnitto\nCollaborative dimensional modeling\n\nData Quality Assessment Framework\n\nWorld Bank Group\nStatistical capacity building\n\nThe Data Warehouse Toolkit\n\nRalph Kimball & Margy Ross\nDimensional modeling fundamentals"
  },
  {
    "objectID": "AndiStory.html#interactive-gdpr-fines-dashboard",
    "href": "AndiStory.html#interactive-gdpr-fines-dashboard",
    "title": "Andi’s Data Journey",
    "section": "Interactive GDPR Fines Dashboard",
    "text": "Interactive GDPR Fines Dashboard\n\nOverviewTemporal AnalysisGeographic DistributionViolation TypesIndustry ImpactOverview\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nTotal Fines\n€121,140,485,786.38\n\n\nTotal Cases\n200\n\n\nAverage Fine\n€605,702,428.93\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\nGDPR Fines Analysis Summary\n\n\n\nThe analysis of GDPR fines reveals several key patterns and insights:\n\nTemporal Trends\n\nIncreasing trend in enforcement actions since GDPR implementation\nSeasonal variations in fine amounts and frequency\nNotable increase in large fines over time\n\nGeographic Distribution\n\nHigher enforcement activity in certain jurisdictions\nVarying fine amounts across different countries\nRegional patterns in types of violations\n\nFine Amount Patterns\n\nWide range of fine amounts, from minor to significant penalties\nCorrelation between violation severity and fine amounts\nIndustry-specific patterns in fine amounts\n\nCommon Violations\n\nMost frequently violated GDPR articles\nPatterns in violation types across industries\nRecurring compliance challenges\n\nIndustry Impact\n\nSector-specific compliance challenges\nVariation in fine severity across industries\nIndustry-specific risk patterns\n\nSeverity Analysis\n\nCorrelation between violation types and fine amounts\nIndustry-specific severity patterns\nImpact of multiple violations on fine amounts\n\n\n\nDisclaimer: The data used in these visualizations is generated dummy data for illustrative purposes only. For access to real GDPR enforcement data and up-to-date information about fines, please visit GDPR Enforcement Tracker."
  },
  {
    "objectID": "AndiStory.html#gdpr-fines-analysis-summary",
    "href": "AndiStory.html#gdpr-fines-analysis-summary",
    "title": "Andi’s Data Journey",
    "section": "GDPR Fines Analysis Summary",
    "text": "GDPR Fines Analysis Summary"
  },
  {
    "objectID": "project_planning.html",
    "href": "project_planning.html",
    "title": "Project Planning",
    "section": "",
    "text": "This document provides templates and guidelines for project management in the GDPR Fines Analysis project.\n\n\n# Project Charter: [Project Name]\n\n## Project Overview\n- **Project Name**: [Full project name]\n- **Project ID**: [Unique identifier]\n- **Start Date**: [YYYY-MM-DD]\n- **End Date**: [YYYY-MM-DD]\n- **Project Sponsor**: [Name and contact details]\n- **Project Manager**: [Name and contact details]\n\n## Project Description\n[Detailed description of the project, including context and background]\n\n## Project Objectives\n- [Specific objective 1]\n- [Specific objective 2]\n- [Specific objective 3]\n\n## Success Criteria\n- [Measurable success criterion 1]\n- [Measurable success criterion 2]\n- [Measurable success criterion 3]\n\n## Project Scope\n### In Scope\n- [Item or activity within project scope]\n- [Item or activity within project scope]\n\n### Out of Scope\n- [Item or activity outside project scope]\n- [Item or activity outside project scope]\n\n## Project Timeline\n- **Phase 1**: [Description] - [Start date] to [End date]\n- **Phase 2**: [Description] - [Start date] to [End date]\n- **Phase 3**: [Description] - [Start date] to [End date]\n\n## Project Resources\n- **Team Members**:\n  - [Name, Role, Allocation %]\n  - [Name, Role, Allocation %]\n- **Budget**: [Budget amount]\n- **Tools and Technologies**: [List of tools and technologies]\n\n## Stakeholders\n- [Stakeholder name, role, and interest]\n- [Stakeholder name, role, and interest]\n\n## Risks and Mitigation Strategies\n- **Risk 1**: [Description]\n  - **Probability**: [High/Medium/Low]\n  - **Impact**: [High/Medium/Low]\n  - **Mitigation**: [Strategy to mitigate]\n- **Risk 2**: [Description]\n  - **Probability**: [High/Medium/Low]\n  - **Impact**: [High/Medium/Low]\n  - **Mitigation**: [Strategy to mitigate]\n\n## Approvals\n- **Project Sponsor**: [Name, Date]\n- **Project Manager**: [Name, Date]\n- **Key Stakeholder**: [Name, Date]\n\n\n\n# Work Breakdown Structure: [Project Name]\n\n## 1. Project Initiation\n  ### 1.1 Project Charter\n    #### 1.1.1 Draft charter\n    #### 1.1.2 Review charter\n    #### 1.1.3 Finalize charter\n  ### 1.2 Stakeholder Identification\n    #### 1.2.1 Identify stakeholders\n    #### 1.2.2 Analyze stakeholder requirements\n    #### 1.2.3 Document stakeholder register\n\n## 2. Data Collection and Preparation\n  ### 2.1 Data Source Identification\n    #### 2.1.1 Research available sources\n    #### 2.1.2 Evaluate source quality\n    #### 2.1.3 Document source metadata\n  ### 2.2 Data Extraction\n    #### 2.2.1 Develop extraction scripts\n    #### 2.2.2 Schedule extraction jobs\n    #### 2.2.3 Monitor extraction process\n  ### 2.3 Data Cleaning\n    #### 2.3.1 Identify data quality issues\n    #### 2.3.2 Implement cleaning routines\n    #### 2.3.3 Validate cleaned data\n\n## 3. Data Analysis\n  ### 3.1 Exploratory Analysis\n    #### 3.1.1 Perform statistical analysis\n    #### 3.1.2 Identify patterns and trends\n    #### 3.1.3 Document initial findings\n  ### 3.2 In-depth Analysis\n    #### 3.2.1 Apply analytical techniques\n    #### 3.2.2 Test hypotheses\n    #### 3.2.3 Document analysis results\n\n## 4. Visualization and Reporting\n  ### 4.1 Dashboard Development\n    #### 4.1.1 Design dashboard layout\n    #### 4.1.2 Implement visualizations\n    #### 4.1.3 Test dashboard functionality\n  ### 4.2 Report Creation\n    #### 4.2.1 Draft report content\n    #### 4.2.2 Review and revise\n    #### 4.2.3 Finalize report\n\n## 5. Project Closure\n  ### 5.1 Documentation\n    #### 5.1.1 Compile project documentation\n    #### 5.1.2 Archive project files\n    #### 5.1.3 Update knowledge base\n  ### 5.2 Project Review\n    #### 5.2.1 Conduct lessons learned session\n    #### 5.2.2 Document best practices\n    #### 5.2.3 Present final project summary\n\n\n\n# Project Timeline: [Project Name]\n\n## Project Duration\n- **Start Date**: [YYYY-MM-DD]\n- **End Date**: [YYYY-MM-DD]\n- **Total Duration**: [X] weeks/months\n\n## Phase 1: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Phase 2: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Phase 3: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Dependencies and Critical Path\n- [Task 1] must be completed before [Task 2]\n- [Task 3] and [Task 4] can be done in parallel\n- Critical path: [Task 1] → [Task 5] → [Task 7] → [Task 9]\n\n## Resource Allocation Timeline\n- **[Team Member/Resource 1]**:\n  - [Phase 1]: [Allocation %]\n  - [Phase 2]: [Allocation %]\n  - [Phase 3]: [Allocation %]\n- **[Team Member/Resource 2]**:\n  - [Phase 1]: [Allocation %]\n  - [Phase 2]: [Allocation %]\n  - [Phase 3]: [Allocation %]\n\n\n\n# RACI Matrix: [Project Name]\n\n| Task/Deliverable | [Stakeholder 1] | [Stakeholder 2] | [Stakeholder 3] | [Stakeholder 4] |\n|------------------|-----------------|-----------------|-----------------|-----------------|\n| [Task 1]         | R               | A               | C               | I               |\n| [Task 2]         | C               | R               | A               | I               |\n| [Task 3]         | I               | C               | R               | A               |\n| [Task 4]         | A               | R               | I               | C               |\n\n**Legend**:\n- **R (Responsible)**: Person who performs the work\n- **A (Accountable)**: Person ultimately answerable for the work\n- **C (Consulted)**: Person who provides input before work is done\n- **I (Informed)**: Person who is kept up-to-date on progress\n\n\n\n# Risk Register: [Project Name]\n\n| Risk ID | Description | Category | Probability | Impact | Risk Score | Mitigation Strategy | Contingency Plan | Owner | Status |\n|---------|-------------|----------|------------|--------|------------|---------------------|------------------|-------|--------|\n| R001    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n| R002    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n| R003    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n\n**Risk Categories**:\n- Technical\n- Schedule\n- Resource\n- Data Quality\n- Stakeholder\n- External\n\n**Risk Score Calculation**:\n- High Probability + High Impact = High Risk (9)\n- High Probability + Medium Impact = High Risk (6)\n- Medium Probability + High Impact = High Risk (6)\n- Medium Probability + Medium Impact = Medium Risk (4)\n- Low Probability + High Impact = Medium Risk (3)\n- High Probability + Low Impact = Medium Risk (3)\n- Medium Probability + Low Impact = Low Risk (2)\n- Low Probability + Medium Impact = Low Risk (2)\n- Low Probability + Low Impact = Low Risk (1)\n\n\n\n# Project Status Report: [Project Name]\n\n## Report Information\n- **Reporting Period**: [Start date] to [End date]\n- **Report Date**: [YYYY-MM-DD]\n- **Report Prepared By**: [Name]\n\n## Executive Summary\n[Brief summary of project status, key achievements, and major issues]\n\n## Overall Status\n- **Schedule**: [On Track / At Risk / Delayed]\n- **Budget**: [On Track / At Risk / Over Budget]\n- **Scope**: [On Track / At Risk / Changed]\n- **Resources**: [On Track / At Risk / Insufficient]\n- **Risks**: [Low / Medium / High]\n\n## Accomplishments This Period\n- [Accomplishment 1]\n- [Accomplishment 2]\n- [Accomplishment 3]\n\n## Planned vs. Actual Progress\n| Milestone/Deliverable | Planned Date | Actual/Forecasted Date | Status | Variance (days) |\n|-----------------------|--------------|------------------------|--------|-----------------|\n| [Milestone 1]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n| [Milestone 2]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n| [Milestone 3]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n\n## Key Issues/Risks and Mitigation Strategies\n| Issue/Risk | Description | Impact | Mitigation/Resolution | Owner | Due Date |\n|------------|-------------|--------|----------------------|-------|----------|\n| [Issue 1]  | [Description] | [Impact] | [Strategy] | [Name] | [Date] |\n| [Issue 2]  | [Description] | [Impact] | [Strategy] | [Name] | [Date] |\n\n## Budget Status\n- **Total Budget**: [Amount]\n- **Spent to Date**: [Amount]\n- **Remaining**: [Amount]\n- **Forecasted at Completion**: [Amount]\n- **Variance**: [Amount] ([Percentage])\n\n## Next Period Activities\n- [Activity 1]\n- [Activity 2]\n- [Activity 3]\n\n## Decisions Required\n- [Decision 1]\n- [Decision 2]\n\n## Additional Notes\n[Any additional information or comments]\n\n\n\n\nCopy the template markdown.\nReplace all placeholders (text within [brackets]) with project-specific information.\nSave the completed template in the project documentation repository.\nUpdate as needed throughout the project lifecycle.\n\nThese templates are designed to be flexible and can be adapted to fit the specific needs of your project."
  },
  {
    "objectID": "project_planning.html#project-charter-template",
    "href": "project_planning.html#project-charter-template",
    "title": "Project Planning",
    "section": "",
    "text": "# Project Charter: [Project Name]\n\n## Project Overview\n- **Project Name**: [Full project name]\n- **Project ID**: [Unique identifier]\n- **Start Date**: [YYYY-MM-DD]\n- **End Date**: [YYYY-MM-DD]\n- **Project Sponsor**: [Name and contact details]\n- **Project Manager**: [Name and contact details]\n\n## Project Description\n[Detailed description of the project, including context and background]\n\n## Project Objectives\n- [Specific objective 1]\n- [Specific objective 2]\n- [Specific objective 3]\n\n## Success Criteria\n- [Measurable success criterion 1]\n- [Measurable success criterion 2]\n- [Measurable success criterion 3]\n\n## Project Scope\n### In Scope\n- [Item or activity within project scope]\n- [Item or activity within project scope]\n\n### Out of Scope\n- [Item or activity outside project scope]\n- [Item or activity outside project scope]\n\n## Project Timeline\n- **Phase 1**: [Description] - [Start date] to [End date]\n- **Phase 2**: [Description] - [Start date] to [End date]\n- **Phase 3**: [Description] - [Start date] to [End date]\n\n## Project Resources\n- **Team Members**:\n  - [Name, Role, Allocation %]\n  - [Name, Role, Allocation %]\n- **Budget**: [Budget amount]\n- **Tools and Technologies**: [List of tools and technologies]\n\n## Stakeholders\n- [Stakeholder name, role, and interest]\n- [Stakeholder name, role, and interest]\n\n## Risks and Mitigation Strategies\n- **Risk 1**: [Description]\n  - **Probability**: [High/Medium/Low]\n  - **Impact**: [High/Medium/Low]\n  - **Mitigation**: [Strategy to mitigate]\n- **Risk 2**: [Description]\n  - **Probability**: [High/Medium/Low]\n  - **Impact**: [High/Medium/Low]\n  - **Mitigation**: [Strategy to mitigate]\n\n## Approvals\n- **Project Sponsor**: [Name, Date]\n- **Project Manager**: [Name, Date]\n- **Key Stakeholder**: [Name, Date]"
  },
  {
    "objectID": "project_planning.html#work-breakdown-structure-wbs-template",
    "href": "project_planning.html#work-breakdown-structure-wbs-template",
    "title": "Project Planning",
    "section": "",
    "text": "# Work Breakdown Structure: [Project Name]\n\n## 1. Project Initiation\n  ### 1.1 Project Charter\n    #### 1.1.1 Draft charter\n    #### 1.1.2 Review charter\n    #### 1.1.3 Finalize charter\n  ### 1.2 Stakeholder Identification\n    #### 1.2.1 Identify stakeholders\n    #### 1.2.2 Analyze stakeholder requirements\n    #### 1.2.3 Document stakeholder register\n\n## 2. Data Collection and Preparation\n  ### 2.1 Data Source Identification\n    #### 2.1.1 Research available sources\n    #### 2.1.2 Evaluate source quality\n    #### 2.1.3 Document source metadata\n  ### 2.2 Data Extraction\n    #### 2.2.1 Develop extraction scripts\n    #### 2.2.2 Schedule extraction jobs\n    #### 2.2.3 Monitor extraction process\n  ### 2.3 Data Cleaning\n    #### 2.3.1 Identify data quality issues\n    #### 2.3.2 Implement cleaning routines\n    #### 2.3.3 Validate cleaned data\n\n## 3. Data Analysis\n  ### 3.1 Exploratory Analysis\n    #### 3.1.1 Perform statistical analysis\n    #### 3.1.2 Identify patterns and trends\n    #### 3.1.3 Document initial findings\n  ### 3.2 In-depth Analysis\n    #### 3.2.1 Apply analytical techniques\n    #### 3.2.2 Test hypotheses\n    #### 3.2.3 Document analysis results\n\n## 4. Visualization and Reporting\n  ### 4.1 Dashboard Development\n    #### 4.1.1 Design dashboard layout\n    #### 4.1.2 Implement visualizations\n    #### 4.1.3 Test dashboard functionality\n  ### 4.2 Report Creation\n    #### 4.2.1 Draft report content\n    #### 4.2.2 Review and revise\n    #### 4.2.3 Finalize report\n\n## 5. Project Closure\n  ### 5.1 Documentation\n    #### 5.1.1 Compile project documentation\n    #### 5.1.2 Archive project files\n    #### 5.1.3 Update knowledge base\n  ### 5.2 Project Review\n    #### 5.2.1 Conduct lessons learned session\n    #### 5.2.2 Document best practices\n    #### 5.2.3 Present final project summary"
  },
  {
    "objectID": "project_planning.html#project-timeline-template",
    "href": "project_planning.html#project-timeline-template",
    "title": "Project Planning",
    "section": "",
    "text": "# Project Timeline: [Project Name]\n\n## Project Duration\n- **Start Date**: [YYYY-MM-DD]\n- **End Date**: [YYYY-MM-DD]\n- **Total Duration**: [X] weeks/months\n\n## Phase 1: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Phase 2: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Phase 3: [Phase Name]\n- **Duration**: [Start date] to [End date]\n- **Key Milestones**:\n  - **[Milestone 1]**: [Due date]\n  - **[Milestone 2]**: [Due date]\n- **Deliverables**:\n  - [Deliverable 1]\n  - [Deliverable 2]\n\n## Dependencies and Critical Path\n- [Task 1] must be completed before [Task 2]\n- [Task 3] and [Task 4] can be done in parallel\n- Critical path: [Task 1] → [Task 5] → [Task 7] → [Task 9]\n\n## Resource Allocation Timeline\n- **[Team Member/Resource 1]**:\n  - [Phase 1]: [Allocation %]\n  - [Phase 2]: [Allocation %]\n  - [Phase 3]: [Allocation %]\n- **[Team Member/Resource 2]**:\n  - [Phase 1]: [Allocation %]\n  - [Phase 2]: [Allocation %]\n  - [Phase 3]: [Allocation %]"
  },
  {
    "objectID": "project_planning.html#raci-matrix-template",
    "href": "project_planning.html#raci-matrix-template",
    "title": "Project Planning",
    "section": "",
    "text": "# RACI Matrix: [Project Name]\n\n| Task/Deliverable | [Stakeholder 1] | [Stakeholder 2] | [Stakeholder 3] | [Stakeholder 4] |\n|------------------|-----------------|-----------------|-----------------|-----------------|\n| [Task 1]         | R               | A               | C               | I               |\n| [Task 2]         | C               | R               | A               | I               |\n| [Task 3]         | I               | C               | R               | A               |\n| [Task 4]         | A               | R               | I               | C               |\n\n**Legend**:\n- **R (Responsible)**: Person who performs the work\n- **A (Accountable)**: Person ultimately answerable for the work\n- **C (Consulted)**: Person who provides input before work is done\n- **I (Informed)**: Person who is kept up-to-date on progress"
  },
  {
    "objectID": "project_planning.html#risk-register-template",
    "href": "project_planning.html#risk-register-template",
    "title": "Project Planning",
    "section": "",
    "text": "# Risk Register: [Project Name]\n\n| Risk ID | Description | Category | Probability | Impact | Risk Score | Mitigation Strategy | Contingency Plan | Owner | Status |\n|---------|-------------|----------|------------|--------|------------|---------------------|------------------|-------|--------|\n| R001    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n| R002    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n| R003    | [Description] | [Category] | [H/M/L] | [H/M/L] | [Score] | [Strategy] | [Plan] | [Name] | [Status] |\n\n**Risk Categories**:\n- Technical\n- Schedule\n- Resource\n- Data Quality\n- Stakeholder\n- External\n\n**Risk Score Calculation**:\n- High Probability + High Impact = High Risk (9)\n- High Probability + Medium Impact = High Risk (6)\n- Medium Probability + High Impact = High Risk (6)\n- Medium Probability + Medium Impact = Medium Risk (4)\n- Low Probability + High Impact = Medium Risk (3)\n- High Probability + Low Impact = Medium Risk (3)\n- Medium Probability + Low Impact = Low Risk (2)\n- Low Probability + Medium Impact = Low Risk (2)\n- Low Probability + Low Impact = Low Risk (1)"
  },
  {
    "objectID": "project_planning.html#status-report-template",
    "href": "project_planning.html#status-report-template",
    "title": "Project Planning",
    "section": "",
    "text": "# Project Status Report: [Project Name]\n\n## Report Information\n- **Reporting Period**: [Start date] to [End date]\n- **Report Date**: [YYYY-MM-DD]\n- **Report Prepared By**: [Name]\n\n## Executive Summary\n[Brief summary of project status, key achievements, and major issues]\n\n## Overall Status\n- **Schedule**: [On Track / At Risk / Delayed]\n- **Budget**: [On Track / At Risk / Over Budget]\n- **Scope**: [On Track / At Risk / Changed]\n- **Resources**: [On Track / At Risk / Insufficient]\n- **Risks**: [Low / Medium / High]\n\n## Accomplishments This Period\n- [Accomplishment 1]\n- [Accomplishment 2]\n- [Accomplishment 3]\n\n## Planned vs. Actual Progress\n| Milestone/Deliverable | Planned Date | Actual/Forecasted Date | Status | Variance (days) |\n|-----------------------|--------------|------------------------|--------|-----------------|\n| [Milestone 1]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n| [Milestone 2]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n| [Milestone 3]         | [Date]       | [Date]                 | [Status] | [+/- Days]    |\n\n## Key Issues/Risks and Mitigation Strategies\n| Issue/Risk | Description | Impact | Mitigation/Resolution | Owner | Due Date |\n|------------|-------------|--------|----------------------|-------|----------|\n| [Issue 1]  | [Description] | [Impact] | [Strategy] | [Name] | [Date] |\n| [Issue 2]  | [Description] | [Impact] | [Strategy] | [Name] | [Date] |\n\n## Budget Status\n- **Total Budget**: [Amount]\n- **Spent to Date**: [Amount]\n- **Remaining**: [Amount]\n- **Forecasted at Completion**: [Amount]\n- **Variance**: [Amount] ([Percentage])\n\n## Next Period Activities\n- [Activity 1]\n- [Activity 2]\n- [Activity 3]\n\n## Decisions Required\n- [Decision 1]\n- [Decision 2]\n\n## Additional Notes\n[Any additional information or comments]"
  },
  {
    "objectID": "project_planning.html#how-to-use-these-templates",
    "href": "project_planning.html#how-to-use-these-templates",
    "title": "Project Planning",
    "section": "",
    "text": "Copy the template markdown.\nReplace all placeholders (text within [brackets]) with project-specific information.\nSave the completed template in the project documentation repository.\nUpdate as needed throughout the project lifecycle.\n\nThese templates are designed to be flexible and can be adapted to fit the specific needs of your project."
  },
  {
    "objectID": "findings_templates.html",
    "href": "findings_templates.html",
    "title": "Findings Templates",
    "section": "",
    "text": "[Brief overview of key findings and recommendations]\n\n\n\n\nData Sources:\nAnalysis Approach:\nTools Used:\nLimitations:\n\n\n\n\n\n[Finding 1]\n\nSupporting Evidence:\nImpact:\nRecommendations:\n\n[Finding 2]\n\nSupporting Evidence:\nImpact:\nRecommendations:\n\n\n\n\n\n[Include relevant charts/graphs with captions]\n\n\n\n\n[Action Item 1]\n[Action Item 2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nScore (1-5)\nIssues Found\nRecommendations\n\n\n\n\nCompleteness\n\n\n\n\n\nAccuracy\n\n\n\n\n\nConsistency\n\n\n\n\n\nTimeliness\n\n\n\n\n\nValidity\n\n\n\n\n\n\n\n\n\n\n[Issue 1]\n\nImpact:\nRoot Cause:\nMitigation Plan:\n\n[Issue 2]\n\nImpact:\nRoot Cause:\nMitigation Plan:\n\n\n\n\n\n\nShort-term Actions:\nLong-term Solutions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirement\nStatus\nEvidence\nNotes\n\n\n\n\n[Requirement 1]\n\n\n\n\n\n[Requirement 2]\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\nLikelihood\nImpact\nMitigation\n\n\n\n\n[Risk 1]\n\n\n\n\n\n[Risk 2]\n\n\n\n\n\n\n\n\n\n\n[Recommendation 1]\n\nPriority:\nTimeline:\nResources Needed:\n\n[Recommendation 2]\n\nPriority:\nTimeline:\nResources Needed:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Design:\nProposed Changes:\nTechnical Debt:\n\n\n\n\n\n\n\nMetric\nCurrent\nTarget\nStatus\n\n\n\n\n[Metric 1]\n\n\n\n\n\n[Metric 2]\n\n\n\n\n\n\n\n\n\n\nVulnerabilities Found:\nMitigation Steps:\nCompliance Requirements:\n\n\n\n\n\nPhase 1:\nPhase 2:\nPhase 3:"
  },
  {
    "objectID": "findings_templates.html#data-analysis-findings-template",
    "href": "findings_templates.html#data-analysis-findings-template",
    "title": "Findings Templates",
    "section": "",
    "text": "[Brief overview of key findings and recommendations]\n\n\n\n\nData Sources:\nAnalysis Approach:\nTools Used:\nLimitations:\n\n\n\n\n\n[Finding 1]\n\nSupporting Evidence:\nImpact:\nRecommendations:\n\n[Finding 2]\n\nSupporting Evidence:\nImpact:\nRecommendations:\n\n\n\n\n\n[Include relevant charts/graphs with captions]\n\n\n\n\n[Action Item 1]\n[Action Item 2]"
  },
  {
    "objectID": "findings_templates.html#data-quality-assessment-template",
    "href": "findings_templates.html#data-quality-assessment-template",
    "title": "Findings Templates",
    "section": "",
    "text": "Dimension\nScore (1-5)\nIssues Found\nRecommendations\n\n\n\n\nCompleteness\n\n\n\n\n\nAccuracy\n\n\n\n\n\nConsistency\n\n\n\n\n\nTimeliness\n\n\n\n\n\nValidity\n\n\n\n\n\n\n\n\n\n\n[Issue 1]\n\nImpact:\nRoot Cause:\nMitigation Plan:\n\n[Issue 2]\n\nImpact:\nRoot Cause:\nMitigation Plan:\n\n\n\n\n\n\nShort-term Actions:\nLong-term Solutions:"
  },
  {
    "objectID": "findings_templates.html#gdpr-compliance-findings-template",
    "href": "findings_templates.html#gdpr-compliance-findings-template",
    "title": "Findings Templates",
    "section": "",
    "text": "Requirement\nStatus\nEvidence\nNotes\n\n\n\n\n[Requirement 1]\n\n\n\n\n\n[Requirement 2]\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\nLikelihood\nImpact\nMitigation\n\n\n\n\n[Risk 1]\n\n\n\n\n\n[Risk 2]\n\n\n\n\n\n\n\n\n\n\n[Recommendation 1]\n\nPriority:\nTimeline:\nResources Needed:\n\n[Recommendation 2]\n\nPriority:\nTimeline:\nResources Needed:"
  },
  {
    "objectID": "findings_templates.html#technical-implementation-findings",
    "href": "findings_templates.html#technical-implementation-findings",
    "title": "Findings Templates",
    "section": "",
    "text": "Current Design:\nProposed Changes:\nTechnical Debt:\n\n\n\n\n\n\n\nMetric\nCurrent\nTarget\nStatus\n\n\n\n\n[Metric 1]\n\n\n\n\n\n[Metric 2]\n\n\n\n\n\n\n\n\n\n\nVulnerabilities Found:\nMitigation Steps:\nCompliance Requirements:\n\n\n\n\n\nPhase 1:\nPhase 2:\nPhase 3:"
  },
  {
    "objectID": "reference_materials.html",
    "href": "reference_materials.html",
    "title": "Reference Materials",
    "section": "",
    "text": "This document provides technical standards and best practices for the GDPR Fines Analysis project.\n\n\n\n\n\n\nWe follow PEP 8 with the following specifics:\n\nIndentation: Use 4 spaces per indentation level.\nMaximum Line Length: 88 characters (compatible with Black formatter).\nImports:\n\nGroup imports in the following order:\n\nStandard library imports\nRelated third-party imports\nLocal application/library-specific imports\n\nWithin each group, imports should be in alphabetical order.\nUse absolute imports rather than relative imports.\n\n\n# Standard library imports\nimport os\nimport sys\nfrom datetime import datetime\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Local imports\nfrom src.data import load_data\nfrom src.models import train_model\n\nNaming Conventions:\n\nsnake_case for functions, variables, and methods\nPascalCase for classes\nUPPER_CASE for constants\nPrefix private attributes with underscore: _private_variable\n\nDocumentation:\n\nAll modules, classes, methods, and functions must include docstrings following NumPy docstring style.\nInclude type hints for function parameters and return values.\n\n\ndef process_data(data_frame: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Process the data by normalizing values in the specified column.\n    \n    Parameters\n    ----------\n    data_frame : pd.DataFrame\n        Input dataframe containing the data to process\n    column_name : str\n        Name of the column to normalize\n        \n    Returns\n    -------\n    pd.DataFrame\n        Processed dataframe with normalized values\n        \n    Examples\n    --------\n    &gt;&gt;&gt; df = pd.DataFrame({'value': [1, 2, 3]})\n    &gt;&gt;&gt; process_data(df, 'value')\n       value  normalized_value\n    0      1              0.0\n    1      2              0.5\n    2      3              1.0\n    \"\"\"\n    # Implementation here\n    return processed_dataframe\n\n\n\n\nLinting: Use flake8 for code linting.\nFormatting: Use black for code formatting.\nType Checking: Use mypy for static type checking.\nImport Sorting: Use isort for sorting imports.\nDocstring Checking: Use pydocstyle for docstring formatting checks.\n\nConfigure these tools in a setup.cfg or pyproject.toml file at the project root.\n\n\n\n\nUse pytest for unit testing.\nOrganize tests in a tests directory with the same structure as the source code.\nName test files as test_*.py and test functions as test_*.\nAim for at least 80% code coverage.\n\n# In tests/data/test_preprocessing.py\nimport pytest\nimport pandas as pd\nfrom src.data.preprocessing import normalize_column\n\ndef test_normalize_column():\n    \"\"\"Test that the normalize_column function correctly scales values between 0 and 1.\"\"\"\n    # Arrange\n    df = pd.DataFrame({'value': [1, 2, 3]})\n    \n    # Act\n    result = normalize_column(df, 'value')\n    \n    # Assert\n    expected = pd.DataFrame({\n        'value': [1, 2, 3],\n        'value_normalized': [0.0, 0.5, 1.0]\n    })\n    pd.testing.assert_frame_equal(result, expected)\n\n\n\n\n\nCapitalization:\n\nKeywords and function names in UPPERCASE\nTable names, column names, and aliases in snake_case\n\nFormatting:\n\nIndent subqueries and CTEs\nPlace each column on a separate line\nPlace each JOIN on a separate line\nAlign keywords for readability\n\nNaming Conventions:\n\nTable names should be plural (e.g., customers, not customer)\nColumn names should be singular (e.g., first_name, not first_names)\nPrimary keys should be named id\nForeign keys should be named table_name_id (e.g., customer_id)\n\nExample Query:\n\nWITH monthly_fines AS (\n    SELECT\n        DATE_TRUNC('month', fine_date) AS month,\n        country,\n        article,\n        SUM(fine_amount) AS total_fines,\n        COUNT(*) AS fine_count\n    FROM\n        gdpr_fines\n    WHERE\n        fine_date &gt;= '2018-05-25'\n        AND fine_amount &gt; 0\n    GROUP BY\n        DATE_TRUNC('month', fine_date),\n        country,\n        article\n)\n\nSELECT\n    mf.month,\n    mf.country,\n    c.region,\n    mf.article,\n    a.description AS article_description,\n    mf.total_fines,\n    mf.fine_count,\n    ROUND(mf.total_fines / mf.fine_count, 2) AS average_fine\nFROM\n    monthly_fines mf\nJOIN\n    countries c ON mf.country = c.country_name\nJOIN\n    gdpr_articles a ON mf.article = a.article_id\nWHERE\n    mf.total_fines &gt; 10000\nORDER BY\n    mf.month DESC,\n    mf.total_fines DESC\n\n\n\n\n\n\nDocument all datasets using this template:\n# Data Dictionary: [Dataset Name]\n\n## Dataset Overview\n- **Name**: [Dataset name]\n- **Description**: [Brief description of the dataset]\n- **Source**: [Where the data comes from]\n- **Owner**: [Team or person responsible for the dataset]\n- **Update Frequency**: [How often the data is updated]\n- **Last Updated**: [Date of last update]\n- **Version**: [Version number]\n\n## Table: [Table Name]\n\n| Column Name | Data Type | Description | Example Values | Constraints | Notes |\n|-------------|-----------|-------------|----------------|-------------|-------|\n| id | INTEGER | Primary key | 1, 2, 3 | NOT NULL, UNIQUE | Auto-incremented |\n| first_name | VARCHAR(50) | Person's first name | \"John\", \"Jane\" | NOT NULL | |\n| email | VARCHAR(100) | Email address | \"example@example.com\" | UNIQUE | Lowercase storage |\n| created_at | TIMESTAMP | Record creation timestamp | \"2023-01-01 12:00:00\" | NOT NULL | UTC timezone |\n| status | ENUM | Current status | \"active\", \"inactive\", \"pending\" | NOT NULL | Default: \"pending\" |\n\n## Relationships\n- **[Relationship 1]**: Table A.column_x references Table B.column_y\n- **[Relationship 2]**: Table C.column_z references Table A.column_w\n\n## Usage Guidelines\n- [Guideline 1]\n- [Guideline 2]\n- [Guideline 3]\n\n## Known Issues\n- [Issue 1]\n- [Issue 2]\n\n## Changelog\n- **[Date]**: [Changes made]\n- **[Date]**: [Changes made]\n\n\n\nDefine data quality rules for each dataset using this template:\n# Data Quality Rules: [Dataset Name]\n\n## Quality Dimensions\n\n### Completeness\n- **Rule C1**: [Column X] must not be NULL\n- **Rule C2**: At least one of [Column Y] or [Column Z] must have a value\n\n### Accuracy\n- **Rule A1**: [Column X] must be within range [min-max]\n- **Rule A2**: [Column Y] must match pattern [regex pattern]\n\n### Consistency\n- **Rule CS1**: If [Column X] = [Value], then [Column Y] must be [Value]\n- **Rule CS2**: [Column Z] must be consistent with [Related Table.Column]\n\n### Timeliness\n- **Rule T1**: [Timestamp Column] must not be in the future\n- **Rule T2**: Records must be updated at least every [time period]\n\n### Uniqueness\n- **Rule U1**: [Column X] must be unique\n- **Rule U2**: The combination of [Column Y] and [Column Z] must be unique\n\n## Validation Procedures\n- **Daily Validation**: [Description of daily checks]\n- **Weekly Validation**: [Description of weekly checks]\n- **Monthly Validation**: [Description of monthly checks]\n\n## Error Handling\n- **Error Severity Levels**:\n  - **Critical**: [Description and actions]\n  - **High**: [Description and actions]\n  - **Medium**: [Description and actions]\n  - **Low**: [Description and actions]\n  \n- **Notification Procedures**: [Who gets notified and how]\n- **Remediation Steps**: [Standard steps for fixing issues]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArticle\nTitle\nSummary\nImpact on Data Analysis\n\n\n\n\nArt. 5\nPrinciples relating to processing of personal data\nDefines core principles: lawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, storage limitation, integrity, and confidentiality\nAll data processing must adhere to these principles; documentation must demonstrate compliance\n\n\nArt. 6\nLawfulness of processing\nDefines legal bases for processing personal data\nMust establish and document a valid legal basis for all data analysis activities\n\n\nArt. 9\nProcessing of special categories of personal data\nRestricts processing of sensitive data categories\nSpecial protections needed for analysis of health, biometric, or other sensitive data\n\n\nArt. 17\nRight to erasure (‘right to be forgotten’)\nData subjects can request deletion of their data\nMust be able to identify and remove specific individuals’ data from analyses\n\n\nArt. 22\nAutomated individual decision-making, including profiling\nRestricts solely automated decisions with legal effects\nImpacts predictive models and automated decision systems\n\n\nArt. 25\nData protection by design and by default\nPrivacy must be built into systems from the start\nAnalysis systems must incorporate privacy controls from design phase\n\n\nArt. 32\nSecurity of processing\nRequires appropriate security measures\nData analysis environments must implement appropriate security controls\n\n\nArt. 35\nData protection impact assessment\nRequired for high-risk processing\nComplex analyses may require formal impact assessment\n\n\n\n\n\n\n# GDPR Data Processing Checklist\n\n## Before Starting Analysis\n\n- [ ] Identify the purpose of the data processing\n- [ ] Determine the legal basis for processing\n- [ ] Verify that data is necessary for the stated purpose (data minimization)\n- [ ] Check if a Data Protection Impact Assessment (DPIA) is needed\n- [ ] Ensure appropriate security measures are in place\n- [ ] Verify that data subject rights can be fulfilled\n- [ ] Document the data processing activities\n\n## During Analysis\n\n- [ ] Only process data for the documented purpose\n- [ ] Implement privacy-enhancing techniques where possible:\n  - [ ] Data aggregation\n  - [ ] Pseudonymization\n  - [ ] Anonymization\n- [ ] Restrict access to personal data\n- [ ] Maintain processing records\n- [ ] Monitor for data breaches\n\n## After Analysis\n\n- [ ] Securely store or delete the data according to retention policy\n- [ ] Document results in compliance with GDPR\n- [ ] Ensure outputs don't inadvertently allow re-identification\n- [ ] Update processing records\n\n\n\n\n\n\n\n\n\n\n\nLibrary\nVersion\nPurpose\nDocumentation\n\n\n\n\npandas\n2.0.0+\nData manipulation and analysis\nPandas Documentation\n\n\nnumpy\n1.23.0+\nNumerical operations\nNumPy Documentation\n\n\nscikit-learn\n1.2.0+\nMachine learning algorithms\nScikit-learn Documentation\n\n\nmatplotlib\n3.7.0+\nStatic visualizations\nMatplotlib Documentation\n\n\nplotly\n5.13.0+\nInteractive visualizations\nPlotly Documentation\n\n\njupyter\n1.0.0+\nInteractive notebooks\nJupyter Documentation\n\n\ngreat_expectations\n0.15.0+\nData validation\nGreat Expectations Documentation\n\n\n\n\n\n\n\n\n\nTool\nVersion\nPurpose\nDocumentation\n\n\n\n\nPostgreSQL\n14.0+\nPrimary database\nPostgreSQL Documentation\n\n\nDuckDB\n0.7.0+\nAnalytical SQL engine\nDuckDB Documentation\n\n\nSQLAlchemy\n2.0.0+\nORM for Python\nSQLAlchemy Documentation\n\n\npgAdmin\n6.0+\nPostgreSQL administration\npgAdmin Documentation\n\n\n\n\n\n\n\n\n\nUse the following color schemes for consistency:\n\nCategorical Data (up to 10 categories):\ncategorical_colors = [\n    '#4e79a7', '#f28e2c', '#e15759', '#76b7b2', '#59a14f',\n    '#edc949', '#af7aa1', '#ff9da7', '#9c755f', '#bab0ab'\n]\nSequential Data (Light to Dark):\nsequential_colors = [\n    '#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6',\n    '#4292c6', '#2171b5', '#08519c', '#08306b'\n]\nDiverging Data (Negative to Positive):\ndiverging_colors = [\n    '#d73027', '#f46d43', '#fdae61', '#fee090', '#ffffbf',\n    '#e0f3f8', '#abd9e9', '#74add1', '#4575b4'\n]\n\n\n\n\n\n\n\n\n\n\n\n\nData Relationship\nRecommended Chart Types\nWhen to Use\n\n\n\n\nComparison\nBar charts, Column charts\nComparing values across categories\n\n\nDistribution\nHistograms, Box plots, Violin plots\nShowing data distribution and outliers\n\n\nComposition\nPie charts, Stacked bars, Treemaps\nShowing parts of a whole\n\n\nTrend\nLine charts, Area charts\nShowing changes over time\n\n\nRelationship\nScatter plots, Bubble charts, Heatmaps\nShowing correlation between variables\n\n\nGeospatial\nChoropleth maps, Point maps\nShowing data with geographic component\n\n\n\n\n\n\n\n\nThese reference materials should be consulted throughout the project lifecycle:\n\nPlanning Phase:\n\nReview data governance requirements\nAlign with GDPR compliance needs\nSelect appropriate technical tools\n\nDevelopment Phase:\n\nFollow coding standards\nImplement data quality rules\nApply visualization guidelines\n\nMaintenance Phase:\n\nUpdate documentation as needed\nEnsure ongoing GDPR compliance\nMonitor data quality\n\nKnowledge Transfer:\n\nUse these materials for onboarding new team members\nReference during knowledge sharing sessions\nCite in project documentation"
  },
  {
    "objectID": "reference_materials.html#coding-standards",
    "href": "reference_materials.html#coding-standards",
    "title": "Reference Materials",
    "section": "",
    "text": "We follow PEP 8 with the following specifics:\n\nIndentation: Use 4 spaces per indentation level.\nMaximum Line Length: 88 characters (compatible with Black formatter).\nImports:\n\nGroup imports in the following order:\n\nStandard library imports\nRelated third-party imports\nLocal application/library-specific imports\n\nWithin each group, imports should be in alphabetical order.\nUse absolute imports rather than relative imports.\n\n\n# Standard library imports\nimport os\nimport sys\nfrom datetime import datetime\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Local imports\nfrom src.data import load_data\nfrom src.models import train_model\n\nNaming Conventions:\n\nsnake_case for functions, variables, and methods\nPascalCase for classes\nUPPER_CASE for constants\nPrefix private attributes with underscore: _private_variable\n\nDocumentation:\n\nAll modules, classes, methods, and functions must include docstrings following NumPy docstring style.\nInclude type hints for function parameters and return values.\n\n\ndef process_data(data_frame: pd.DataFrame, column_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Process the data by normalizing values in the specified column.\n    \n    Parameters\n    ----------\n    data_frame : pd.DataFrame\n        Input dataframe containing the data to process\n    column_name : str\n        Name of the column to normalize\n        \n    Returns\n    -------\n    pd.DataFrame\n        Processed dataframe with normalized values\n        \n    Examples\n    --------\n    &gt;&gt;&gt; df = pd.DataFrame({'value': [1, 2, 3]})\n    &gt;&gt;&gt; process_data(df, 'value')\n       value  normalized_value\n    0      1              0.0\n    1      2              0.5\n    2      3              1.0\n    \"\"\"\n    # Implementation here\n    return processed_dataframe\n\n\n\n\nLinting: Use flake8 for code linting.\nFormatting: Use black for code formatting.\nType Checking: Use mypy for static type checking.\nImport Sorting: Use isort for sorting imports.\nDocstring Checking: Use pydocstyle for docstring formatting checks.\n\nConfigure these tools in a setup.cfg or pyproject.toml file at the project root.\n\n\n\n\nUse pytest for unit testing.\nOrganize tests in a tests directory with the same structure as the source code.\nName test files as test_*.py and test functions as test_*.\nAim for at least 80% code coverage.\n\n# In tests/data/test_preprocessing.py\nimport pytest\nimport pandas as pd\nfrom src.data.preprocessing import normalize_column\n\ndef test_normalize_column():\n    \"\"\"Test that the normalize_column function correctly scales values between 0 and 1.\"\"\"\n    # Arrange\n    df = pd.DataFrame({'value': [1, 2, 3]})\n    \n    # Act\n    result = normalize_column(df, 'value')\n    \n    # Assert\n    expected = pd.DataFrame({\n        'value': [1, 2, 3],\n        'value_normalized': [0.0, 0.5, 1.0]\n    })\n    pd.testing.assert_frame_equal(result, expected)\n\n\n\n\n\nCapitalization:\n\nKeywords and function names in UPPERCASE\nTable names, column names, and aliases in snake_case\n\nFormatting:\n\nIndent subqueries and CTEs\nPlace each column on a separate line\nPlace each JOIN on a separate line\nAlign keywords for readability\n\nNaming Conventions:\n\nTable names should be plural (e.g., customers, not customer)\nColumn names should be singular (e.g., first_name, not first_names)\nPrimary keys should be named id\nForeign keys should be named table_name_id (e.g., customer_id)\n\nExample Query:\n\nWITH monthly_fines AS (\n    SELECT\n        DATE_TRUNC('month', fine_date) AS month,\n        country,\n        article,\n        SUM(fine_amount) AS total_fines,\n        COUNT(*) AS fine_count\n    FROM\n        gdpr_fines\n    WHERE\n        fine_date &gt;= '2018-05-25'\n        AND fine_amount &gt; 0\n    GROUP BY\n        DATE_TRUNC('month', fine_date),\n        country,\n        article\n)\n\nSELECT\n    mf.month,\n    mf.country,\n    c.region,\n    mf.article,\n    a.description AS article_description,\n    mf.total_fines,\n    mf.fine_count,\n    ROUND(mf.total_fines / mf.fine_count, 2) AS average_fine\nFROM\n    monthly_fines mf\nJOIN\n    countries c ON mf.country = c.country_name\nJOIN\n    gdpr_articles a ON mf.article = a.article_id\nWHERE\n    mf.total_fines &gt; 10000\nORDER BY\n    mf.month DESC,\n    mf.total_fines DESC"
  },
  {
    "objectID": "reference_materials.html#data-governance",
    "href": "reference_materials.html#data-governance",
    "title": "Reference Materials",
    "section": "",
    "text": "Document all datasets using this template:\n# Data Dictionary: [Dataset Name]\n\n## Dataset Overview\n- **Name**: [Dataset name]\n- **Description**: [Brief description of the dataset]\n- **Source**: [Where the data comes from]\n- **Owner**: [Team or person responsible for the dataset]\n- **Update Frequency**: [How often the data is updated]\n- **Last Updated**: [Date of last update]\n- **Version**: [Version number]\n\n## Table: [Table Name]\n\n| Column Name | Data Type | Description | Example Values | Constraints | Notes |\n|-------------|-----------|-------------|----------------|-------------|-------|\n| id | INTEGER | Primary key | 1, 2, 3 | NOT NULL, UNIQUE | Auto-incremented |\n| first_name | VARCHAR(50) | Person's first name | \"John\", \"Jane\" | NOT NULL | |\n| email | VARCHAR(100) | Email address | \"example@example.com\" | UNIQUE | Lowercase storage |\n| created_at | TIMESTAMP | Record creation timestamp | \"2023-01-01 12:00:00\" | NOT NULL | UTC timezone |\n| status | ENUM | Current status | \"active\", \"inactive\", \"pending\" | NOT NULL | Default: \"pending\" |\n\n## Relationships\n- **[Relationship 1]**: Table A.column_x references Table B.column_y\n- **[Relationship 2]**: Table C.column_z references Table A.column_w\n\n## Usage Guidelines\n- [Guideline 1]\n- [Guideline 2]\n- [Guideline 3]\n\n## Known Issues\n- [Issue 1]\n- [Issue 2]\n\n## Changelog\n- **[Date]**: [Changes made]\n- **[Date]**: [Changes made]\n\n\n\nDefine data quality rules for each dataset using this template:\n# Data Quality Rules: [Dataset Name]\n\n## Quality Dimensions\n\n### Completeness\n- **Rule C1**: [Column X] must not be NULL\n- **Rule C2**: At least one of [Column Y] or [Column Z] must have a value\n\n### Accuracy\n- **Rule A1**: [Column X] must be within range [min-max]\n- **Rule A2**: [Column Y] must match pattern [regex pattern]\n\n### Consistency\n- **Rule CS1**: If [Column X] = [Value], then [Column Y] must be [Value]\n- **Rule CS2**: [Column Z] must be consistent with [Related Table.Column]\n\n### Timeliness\n- **Rule T1**: [Timestamp Column] must not be in the future\n- **Rule T2**: Records must be updated at least every [time period]\n\n### Uniqueness\n- **Rule U1**: [Column X] must be unique\n- **Rule U2**: The combination of [Column Y] and [Column Z] must be unique\n\n## Validation Procedures\n- **Daily Validation**: [Description of daily checks]\n- **Weekly Validation**: [Description of weekly checks]\n- **Monthly Validation**: [Description of monthly checks]\n\n## Error Handling\n- **Error Severity Levels**:\n  - **Critical**: [Description and actions]\n  - **High**: [Description and actions]\n  - **Medium**: [Description and actions]\n  - **Low**: [Description and actions]\n  \n- **Notification Procedures**: [Who gets notified and how]\n- **Remediation Steps**: [Standard steps for fixing issues]"
  },
  {
    "objectID": "reference_materials.html#gdpr-compliance-reference",
    "href": "reference_materials.html#gdpr-compliance-reference",
    "title": "Reference Materials",
    "section": "",
    "text": "Article\nTitle\nSummary\nImpact on Data Analysis\n\n\n\n\nArt. 5\nPrinciples relating to processing of personal data\nDefines core principles: lawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, storage limitation, integrity, and confidentiality\nAll data processing must adhere to these principles; documentation must demonstrate compliance\n\n\nArt. 6\nLawfulness of processing\nDefines legal bases for processing personal data\nMust establish and document a valid legal basis for all data analysis activities\n\n\nArt. 9\nProcessing of special categories of personal data\nRestricts processing of sensitive data categories\nSpecial protections needed for analysis of health, biometric, or other sensitive data\n\n\nArt. 17\nRight to erasure (‘right to be forgotten’)\nData subjects can request deletion of their data\nMust be able to identify and remove specific individuals’ data from analyses\n\n\nArt. 22\nAutomated individual decision-making, including profiling\nRestricts solely automated decisions with legal effects\nImpacts predictive models and automated decision systems\n\n\nArt. 25\nData protection by design and by default\nPrivacy must be built into systems from the start\nAnalysis systems must incorporate privacy controls from design phase\n\n\nArt. 32\nSecurity of processing\nRequires appropriate security measures\nData analysis environments must implement appropriate security controls\n\n\nArt. 35\nData protection impact assessment\nRequired for high-risk processing\nComplex analyses may require formal impact assessment\n\n\n\n\n\n\n# GDPR Data Processing Checklist\n\n## Before Starting Analysis\n\n- [ ] Identify the purpose of the data processing\n- [ ] Determine the legal basis for processing\n- [ ] Verify that data is necessary for the stated purpose (data minimization)\n- [ ] Check if a Data Protection Impact Assessment (DPIA) is needed\n- [ ] Ensure appropriate security measures are in place\n- [ ] Verify that data subject rights can be fulfilled\n- [ ] Document the data processing activities\n\n## During Analysis\n\n- [ ] Only process data for the documented purpose\n- [ ] Implement privacy-enhancing techniques where possible:\n  - [ ] Data aggregation\n  - [ ] Pseudonymization\n  - [ ] Anonymization\n- [ ] Restrict access to personal data\n- [ ] Maintain processing records\n- [ ] Monitor for data breaches\n\n## After Analysis\n\n- [ ] Securely store or delete the data according to retention policy\n- [ ] Document results in compliance with GDPR\n- [ ] Ensure outputs don't inadvertently allow re-identification\n- [ ] Update processing records"
  },
  {
    "objectID": "reference_materials.html#technical-stack-reference",
    "href": "reference_materials.html#technical-stack-reference",
    "title": "Reference Materials",
    "section": "",
    "text": "Library\nVersion\nPurpose\nDocumentation\n\n\n\n\npandas\n2.0.0+\nData manipulation and analysis\nPandas Documentation\n\n\nnumpy\n1.23.0+\nNumerical operations\nNumPy Documentation\n\n\nscikit-learn\n1.2.0+\nMachine learning algorithms\nScikit-learn Documentation\n\n\nmatplotlib\n3.7.0+\nStatic visualizations\nMatplotlib Documentation\n\n\nplotly\n5.13.0+\nInteractive visualizations\nPlotly Documentation\n\n\njupyter\n1.0.0+\nInteractive notebooks\nJupyter Documentation\n\n\ngreat_expectations\n0.15.0+\nData validation\nGreat Expectations Documentation\n\n\n\n\n\n\n\n\n\nTool\nVersion\nPurpose\nDocumentation\n\n\n\n\nPostgreSQL\n14.0+\nPrimary database\nPostgreSQL Documentation\n\n\nDuckDB\n0.7.0+\nAnalytical SQL engine\nDuckDB Documentation\n\n\nSQLAlchemy\n2.0.0+\nORM for Python\nSQLAlchemy Documentation\n\n\npgAdmin\n6.0+\nPostgreSQL administration\npgAdmin Documentation\n\n\n\n\n\n\n\n\n\nUse the following color schemes for consistency:\n\nCategorical Data (up to 10 categories):\ncategorical_colors = [\n    '#4e79a7', '#f28e2c', '#e15759', '#76b7b2', '#59a14f',\n    '#edc949', '#af7aa1', '#ff9da7', '#9c755f', '#bab0ab'\n]\nSequential Data (Light to Dark):\nsequential_colors = [\n    '#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6',\n    '#4292c6', '#2171b5', '#08519c', '#08306b'\n]\nDiverging Data (Negative to Positive):\ndiverging_colors = [\n    '#d73027', '#f46d43', '#fdae61', '#fee090', '#ffffbf',\n    '#e0f3f8', '#abd9e9', '#74add1', '#4575b4'\n]\n\n\n\n\n\n\n\n\n\n\n\n\nData Relationship\nRecommended Chart Types\nWhen to Use\n\n\n\n\nComparison\nBar charts, Column charts\nComparing values across categories\n\n\nDistribution\nHistograms, Box plots, Violin plots\nShowing data distribution and outliers\n\n\nComposition\nPie charts, Stacked bars, Treemaps\nShowing parts of a whole\n\n\nTrend\nLine charts, Area charts\nShowing changes over time\n\n\nRelationship\nScatter plots, Bubble charts, Heatmaps\nShowing correlation between variables\n\n\nGeospatial\nChoropleth maps, Point maps\nShowing data with geographic component"
  },
  {
    "objectID": "reference_materials.html#how-to-use-these-reference-materials",
    "href": "reference_materials.html#how-to-use-these-reference-materials",
    "title": "Reference Materials",
    "section": "",
    "text": "These reference materials should be consulted throughout the project lifecycle:\n\nPlanning Phase:\n\nReview data governance requirements\nAlign with GDPR compliance needs\nSelect appropriate technical tools\n\nDevelopment Phase:\n\nFollow coding standards\nImplement data quality rules\nApply visualization guidelines\n\nMaintenance Phase:\n\nUpdate documentation as needed\nEnsure ongoing GDPR compliance\nMonitor data quality\n\nKnowledge Transfer:\n\nUse these materials for onboarding new team members\nReference during knowledge sharing sessions\nCite in project documentation"
  },
  {
    "objectID": "data_quality_checks.html",
    "href": "data_quality_checks.html",
    "title": "Data Quality Checks",
    "section": "",
    "text": "This document provides templates and guidelines for performing data quality checks in the GDPR Fines Analysis project.\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nDescription\nKey Questions\n\n\n\n\nCompleteness\nThe degree to which all required data is present\nAre there missing values? What percentage of data is complete?\n\n\nAccuracy\nThe degree to which data correctly reflects the real-world entity\nDoes the data match known correct values? Are there validation rules in place?\n\n\nConsistency\nThe degree to which data is consistent across datasets\nAre values consistent across related tables? Are formats standardized?\n\n\nTimeliness\nThe degree to which data is available within the expected time frame\nHow recent is the data? Is it updated frequently enough?\n\n\nValidity\nThe degree to which data conforms to defined formats and ranges\nDoes the data conform to specified formats, types, and ranges?\n\n\nUniqueness\nThe degree to which data is free from duplication\nAre there duplicate records? How are they identified and managed?\n\n\nIntegrity\nThe degree to which relationships between data elements are maintained\nAre referential integrity constraints enforced? Are relationships preserved?\n\n\n\n\n\n\n\n# Data Quality Assessment Report\n\n## Overview\n- **Dataset Name**: [Dataset Name]\n- **Assessment Date**: [YYYY-MM-DD]\n- **Assessed By**: [Name]\n- **Dataset Version/Date**: [Version/Date]\n- **Total Records**: [Number]\n\n## Data Quality Score Summary\n\n| Dimension | Score (1-5) | Weight | Weighted Score | Threshold | Pass/Fail |\n|-----------|-------------|--------|----------------|-----------|-----------|\n| Completeness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Accuracy | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Consistency | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Timeliness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Validity | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Uniqueness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Integrity | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| **Overall Score** | | | [Total Weighted Score] | [Overall Threshold] | [Pass/Fail] |\n\n## Detailed Assessment\n\n### Completeness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Missing Value Rate: [Percentage]\n  - Fields with Missing Values: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Accuracy\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Error Rate: [Percentage]\n  - Fields with Errors: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Consistency\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Inconsistency Rate: [Percentage]\n  - Inconsistent Elements: [Element List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Timeliness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Data Freshness: [Time Since Last Update]\n  - Update Frequency: [Frequency]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Validity\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Invalid Data Rate: [Percentage]\n  - Fields with Invalid Data: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Uniqueness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Duplication Rate: [Percentage]\n  - Duplicate Patterns: [Pattern Description]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Integrity\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Referential Integrity Violations: [Count]\n  - Affected Relations: [Relation List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n## Critical Issues Summary\n| Issue ID | Description | Dimension | Severity | Impact | Recommended Action | Owner | Due Date |\n|----------|-------------|-----------|----------|--------|---------------------|-------|----------|\n| DQ001 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n| DQ002 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n| DQ003 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n\n## Data Quality Improvement Plan\n- **Short-term Actions** (Next 2 weeks):\n  - [Action 1]\n  - [Action 2]\n- **Medium-term Actions** (Next 1-2 months):\n  - [Action 1]\n  - [Action 2]\n- **Long-term Actions** (Next 3-6 months):\n  - [Action 1]\n  - [Action 2]\n\n## Appendix\n- **Data Quality Checks Performed**: [List of specific checks]\n- **Tools Used**: [List of tools]\n- **Reference Documentation**: [List of references]\n\n\n\n\n\n\n\n-- Example: Check for Completeness (Missing Values)\nSELECT \n  column_name, \n  COUNT(*) AS total_records,\n  SUM(CASE WHEN column_name IS NULL THEN 1 ELSE 0 END) AS null_count,\n  ROUND(100.0 * SUM(CASE WHEN column_name IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) AS null_percentage\nFROM table_name\nGROUP BY column_name\nORDER BY null_percentage DESC;\n\n-- Example: Check for Duplicates\nSELECT \n  id_column, \n  COUNT(*) AS occurrence_count\nFROM table_name\nGROUP BY id_column\nHAVING COUNT(*) &gt; 1;\n\n-- Example: Data Range Validation\nSELECT \n  COUNT(*) AS invalid_records\nFROM table_name\nWHERE numeric_column &lt; min_value OR numeric_column &gt; max_value;\n\n-- Example: Referential Integrity Check\nSELECT \n  a.foreign_key_column\nFROM table_a a\nLEFT JOIN table_b b ON a.foreign_key_column = b.primary_key_column\nWHERE b.primary_key_column IS NULL;\n\n\n\nimport pandas as pd\nimport numpy as np\nimport great_expectations as ge\n\n# Example: Load data and create a Great Expectations DataFrame\ndf = pd.read_csv(\"your_data.csv\")\nge_df = ge.from_pandas(df)\n\n# Example: Check column completeness\ncompleteness_results = ge_df.expect_column_values_to_not_be_null(\"column_name\")\n\n# Example: Check for valid values in a category\ncategory_results = ge_df.expect_column_values_to_be_in_set(\n    \"category_column\", [\"value1\", \"value2\", \"value3\"]\n)\n\n# Example: Check for value ranges\nrange_results = ge_df.expect_column_values_to_be_between(\n    \"numeric_column\", min_value=0, max_value=100\n)\n\n# Example: Check for unique values\nuniqueness_results = ge_df.expect_column_values_to_be_unique(\"id_column\")\n\n# Example: Get all validation results\nvalidation_results = ge_df.validate()\n\n\n\n\n\n\n\nMissing value counts\nNew record counts\nBasic validity checks\n\n\n\n\n\nComprehensive schema validation\nCross-table consistency checks\nTrend analysis\n\n\n\n\n\nFull data quality assessment\nHistorical comparison\nQuality improvement tracking\n\n\n\n\n\n\n\nIssue Detection\n\nAutomated monitoring alerts\nManual review findings\nUser-reported issues\n\nIssue Triage\n\nAssess severity and impact\nDetermine root cause\nAssign priority\n\nResolution Planning\n\nDevelop fix strategy\nEstimate resources needed\nEstablish timeline\n\nImplementation\n\nApply fixes\nDocument changes\nUpdate relevant data\n\nVerification\n\nConfirm issue resolution\nRun validation tests\nApprove changes\n\nDocumentation and Learning\n\nUpdate documentation\nShare lessons learned\nImprove prevention measures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirement\nDescription\nQuality Dimensions\nExample Checks\n\n\n\n\nAccuracy\nPersonal data must be accurate and kept up to date\nAccuracy, Timeliness\nDate of last update checks, Verification against trusted sources\n\n\nCompleteness\nRequired personal data fields must be complete\nCompleteness\nMandatory field validation, Conditional completeness checks\n\n\nConsistency\nPersonal data must be consistent across systems\nConsistency\nCross-system validation, Format standardization\n\n\nRetention\nData should not be kept longer than necessary\nTimeliness\nAge of data checks, Automated retention policy enforcement\n\n\nRelevance\nOnly relevant personal data should be processed\nValidity\nPurpose specification checks, Minimization validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRight\nData Quality Requirements\nImplementation Approaches\n\n\n\n\nRight to Access\nData must be complete and available\nComprehensive data inventory, Access request procedures\n\n\nRight to Rectification\nData must be correctable and changes tracked\nChange management process, Data lineage tracking\n\n\nRight to Erasure\nData must be identifiable and removable\nData mapping, Deletion verification\n\n\nRight to Restrict Processing\nProcessing status must be trackable\nProcessing flags, Access controls\n\n\nRight to Data Portability\nData must be in machine-readable format\nStandard format validation, Export functionality\n\n\n\n\n\n\n\n\nAdapt the templates to your specific dataset requirements.\nImplement the automated checks as part of your data processing pipeline.\nEstablish regular data quality assessment schedules.\nDocument all findings and actions in a centralized repository.\nReview and update data quality procedures as needs evolve.\n\nThe templates and procedures in this document provide a foundation for maintaining high data quality standards. They should be customized based on the specific requirements of your data and project goals."
  },
  {
    "objectID": "data_quality_checks.html#data-quality-framework",
    "href": "data_quality_checks.html#data-quality-framework",
    "title": "Data Quality Checks",
    "section": "",
    "text": "Dimension\nDescription\nKey Questions\n\n\n\n\nCompleteness\nThe degree to which all required data is present\nAre there missing values? What percentage of data is complete?\n\n\nAccuracy\nThe degree to which data correctly reflects the real-world entity\nDoes the data match known correct values? Are there validation rules in place?\n\n\nConsistency\nThe degree to which data is consistent across datasets\nAre values consistent across related tables? Are formats standardized?\n\n\nTimeliness\nThe degree to which data is available within the expected time frame\nHow recent is the data? Is it updated frequently enough?\n\n\nValidity\nThe degree to which data conforms to defined formats and ranges\nDoes the data conform to specified formats, types, and ranges?\n\n\nUniqueness\nThe degree to which data is free from duplication\nAre there duplicate records? How are they identified and managed?\n\n\nIntegrity\nThe degree to which relationships between data elements are maintained\nAre referential integrity constraints enforced? Are relationships preserved?"
  },
  {
    "objectID": "data_quality_checks.html#data-quality-assessment-template",
    "href": "data_quality_checks.html#data-quality-assessment-template",
    "title": "Data Quality Checks",
    "section": "",
    "text": "# Data Quality Assessment Report\n\n## Overview\n- **Dataset Name**: [Dataset Name]\n- **Assessment Date**: [YYYY-MM-DD]\n- **Assessed By**: [Name]\n- **Dataset Version/Date**: [Version/Date]\n- **Total Records**: [Number]\n\n## Data Quality Score Summary\n\n| Dimension | Score (1-5) | Weight | Weighted Score | Threshold | Pass/Fail |\n|-----------|-------------|--------|----------------|-----------|-----------|\n| Completeness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Accuracy | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Consistency | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Timeliness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Validity | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Uniqueness | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| Integrity | [Score] | [Weight] | [Weighted Score] | [Threshold] | [Pass/Fail] |\n| **Overall Score** | | | [Total Weighted Score] | [Overall Threshold] | [Pass/Fail] |\n\n## Detailed Assessment\n\n### Completeness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Missing Value Rate: [Percentage]\n  - Fields with Missing Values: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Accuracy\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Error Rate: [Percentage]\n  - Fields with Errors: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Consistency\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Inconsistency Rate: [Percentage]\n  - Inconsistent Elements: [Element List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Timeliness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Data Freshness: [Time Since Last Update]\n  - Update Frequency: [Frequency]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Validity\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Invalid Data Rate: [Percentage]\n  - Fields with Invalid Data: [Field List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Uniqueness\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Duplication Rate: [Percentage]\n  - Duplicate Patterns: [Pattern Description]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n### Integrity\n- **Score**: [1-5]\n- **Issues Detected**: \n  - [Issue 1]: [Details]\n  - [Issue 2]: [Details]\n- **Metrics**:\n  - Referential Integrity Violations: [Count]\n  - Affected Relations: [Relation List]\n- **Recommendations**:\n  - [Recommendation 1]\n  - [Recommendation 2]\n\n## Critical Issues Summary\n| Issue ID | Description | Dimension | Severity | Impact | Recommended Action | Owner | Due Date |\n|----------|-------------|-----------|----------|--------|---------------------|-------|----------|\n| DQ001 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n| DQ002 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n| DQ003 | [Description] | [Dimension] | [High/Medium/Low] | [Impact] | [Action] | [Name] | [Date] |\n\n## Data Quality Improvement Plan\n- **Short-term Actions** (Next 2 weeks):\n  - [Action 1]\n  - [Action 2]\n- **Medium-term Actions** (Next 1-2 months):\n  - [Action 1]\n  - [Action 2]\n- **Long-term Actions** (Next 3-6 months):\n  - [Action 1]\n  - [Action 2]\n\n## Appendix\n- **Data Quality Checks Performed**: [List of specific checks]\n- **Tools Used**: [List of tools]\n- **Reference Documentation**: [List of references]"
  },
  {
    "objectID": "data_quality_checks.html#data-quality-control-procedures",
    "href": "data_quality_checks.html#data-quality-control-procedures",
    "title": "Data Quality Checks",
    "section": "",
    "text": "-- Example: Check for Completeness (Missing Values)\nSELECT \n  column_name, \n  COUNT(*) AS total_records,\n  SUM(CASE WHEN column_name IS NULL THEN 1 ELSE 0 END) AS null_count,\n  ROUND(100.0 * SUM(CASE WHEN column_name IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) AS null_percentage\nFROM table_name\nGROUP BY column_name\nORDER BY null_percentage DESC;\n\n-- Example: Check for Duplicates\nSELECT \n  id_column, \n  COUNT(*) AS occurrence_count\nFROM table_name\nGROUP BY id_column\nHAVING COUNT(*) &gt; 1;\n\n-- Example: Data Range Validation\nSELECT \n  COUNT(*) AS invalid_records\nFROM table_name\nWHERE numeric_column &lt; min_value OR numeric_column &gt; max_value;\n\n-- Example: Referential Integrity Check\nSELECT \n  a.foreign_key_column\nFROM table_a a\nLEFT JOIN table_b b ON a.foreign_key_column = b.primary_key_column\nWHERE b.primary_key_column IS NULL;\n\n\n\nimport pandas as pd\nimport numpy as np\nimport great_expectations as ge\n\n# Example: Load data and create a Great Expectations DataFrame\ndf = pd.read_csv(\"your_data.csv\")\nge_df = ge.from_pandas(df)\n\n# Example: Check column completeness\ncompleteness_results = ge_df.expect_column_values_to_not_be_null(\"column_name\")\n\n# Example: Check for valid values in a category\ncategory_results = ge_df.expect_column_values_to_be_in_set(\n    \"category_column\", [\"value1\", \"value2\", \"value3\"]\n)\n\n# Example: Check for value ranges\nrange_results = ge_df.expect_column_values_to_be_between(\n    \"numeric_column\", min_value=0, max_value=100\n)\n\n# Example: Check for unique values\nuniqueness_results = ge_df.expect_column_values_to_be_unique(\"id_column\")\n\n# Example: Get all validation results\nvalidation_results = ge_df.validate()\n\n\n\n\n\n\n\nMissing value counts\nNew record counts\nBasic validity checks\n\n\n\n\n\nComprehensive schema validation\nCross-table consistency checks\nTrend analysis\n\n\n\n\n\nFull data quality assessment\nHistorical comparison\nQuality improvement tracking"
  },
  {
    "objectID": "data_quality_checks.html#data-quality-issue-resolution-process",
    "href": "data_quality_checks.html#data-quality-issue-resolution-process",
    "title": "Data Quality Checks",
    "section": "",
    "text": "Issue Detection\n\nAutomated monitoring alerts\nManual review findings\nUser-reported issues\n\nIssue Triage\n\nAssess severity and impact\nDetermine root cause\nAssign priority\n\nResolution Planning\n\nDevelop fix strategy\nEstimate resources needed\nEstablish timeline\n\nImplementation\n\nApply fixes\nDocument changes\nUpdate relevant data\n\nVerification\n\nConfirm issue resolution\nRun validation tests\nApprove changes\n\nDocumentation and Learning\n\nUpdate documentation\nShare lessons learned\nImprove prevention measures"
  },
  {
    "objectID": "data_quality_checks.html#gdpr-specific-data-quality-considerations",
    "href": "data_quality_checks.html#gdpr-specific-data-quality-considerations",
    "title": "Data Quality Checks",
    "section": "",
    "text": "Requirement\nDescription\nQuality Dimensions\nExample Checks\n\n\n\n\nAccuracy\nPersonal data must be accurate and kept up to date\nAccuracy, Timeliness\nDate of last update checks, Verification against trusted sources\n\n\nCompleteness\nRequired personal data fields must be complete\nCompleteness\nMandatory field validation, Conditional completeness checks\n\n\nConsistency\nPersonal data must be consistent across systems\nConsistency\nCross-system validation, Format standardization\n\n\nRetention\nData should not be kept longer than necessary\nTimeliness\nAge of data checks, Automated retention policy enforcement\n\n\nRelevance\nOnly relevant personal data should be processed\nValidity\nPurpose specification checks, Minimization validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRight\nData Quality Requirements\nImplementation Approaches\n\n\n\n\nRight to Access\nData must be complete and available\nComprehensive data inventory, Access request procedures\n\n\nRight to Rectification\nData must be correctable and changes tracked\nChange management process, Data lineage tracking\n\n\nRight to Erasure\nData must be identifiable and removable\nData mapping, Deletion verification\n\n\nRight to Restrict Processing\nProcessing status must be trackable\nProcessing flags, Access controls\n\n\nRight to Data Portability\nData must be in machine-readable format\nStandard format validation, Export functionality"
  },
  {
    "objectID": "data_quality_checks.html#how-to-use-these-templates",
    "href": "data_quality_checks.html#how-to-use-these-templates",
    "title": "Data Quality Checks",
    "section": "",
    "text": "Adapt the templates to your specific dataset requirements.\nImplement the automated checks as part of your data processing pipeline.\nEstablish regular data quality assessment schedules.\nDocument all findings and actions in a centralized repository.\nReview and update data quality procedures as needs evolve.\n\nThe templates and procedures in this document provide a foundation for maintaining high data quality standards. They should be customized based on the specific requirements of your data and project goals."
  }
]