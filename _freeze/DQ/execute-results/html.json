{
  "hash": "de7490e6057032375dc9be2441c38621",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Book 1 - Kidlens Law\"\nformat: \n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: true\n    theme: cosmo\n    highlight-style: github\n    code-tools: true\n---\n\n> \"If you write a problem down clearly, then the matter is half solved.\"  \n> â€” Kidlens Law\n\n# Epilogue: The Art and Science of Data Analysis\n\n## A Day in the Life: The Six Thinking Hats of a Data Analyst\n\nLet me tell you a story about Andi, a data analyst working on understanding GDPR compliance patterns. Her journey illustrates how modern data analysts combine analytical engineering with critical thinking using Edward de Bono's Six Thinking Hats approach and the DMAIC methodology.\n\n### The White Hat: Facts and Information\nAndi starts her day by gathering facts about GDPR fines across Europe. Like a detective, she collects raw data about fines, violations, and company responses. This is where analytical engineering begins - the systematic process of collecting, cleaning, and organizing data. She knows that good analysis starts with quality data, just as a good house needs a solid foundation.\n\n**DMAIC Tools Used:**\n- Define: Project Charter, SIPOC Diagram\n- Measure: Data Collection Plan, Operational Definitions\n- Analyze: Data Mining, Statistical Analysis\n\n### The Red Hat: Intuition and Feelings\nAs she dives into the data, Andi notices patterns that trigger her intuition. Some companies seem to repeatedly violate certain articles, while others quickly adapt after their first fine. She doesn't ignore these gut feelings - they're valuable indicators of where to look deeper. This emotional intelligence, combined with technical skills, makes a data analyst more than just a number cruncher.\n\n**DMAIC Tools Used:**\n- Measure: Voice of Customer (VOC)\n- Analyze: Brainstorming\n- Improve: Impact Analysis\n\n### The Black Hat: Critical Judgment\nAndi puts on her critical thinking hat to identify potential issues. She asks tough questions:\n- Are there gaps in the data collection?\n- Could there be biases in how different countries report violations?\n- What limitations might affect our conclusions?\nThis cautious approach is essential in analytical engineering, where understanding data limitations is as important as the analysis itself.\n\n**DMAIC Tools Used:**\n- Define: Risk Assessment\n- Measure: Measurement System Analysis (MSA)\n- Control: Control Charts, Error Proofing\n\n### The Yellow Hat: Optimistic Opportunities\nLooking at the bright side, Andi sees opportunities in the challenges:\n- Patterns in the data could help companies prevent future violations\n- Analysis could lead to better compliance strategies\n- Insights might help regulators focus their efforts more effectively\nThis optimistic perspective helps her frame the analysis in terms of solutions rather than just problems.\n\n**DMAIC Tools Used:**\n- Improve: Solution Selection Matrix\n- Control: Process Control Plan\n- Define: Benefits Analysis\n\n### The Green Hat: Creative Solutions\nNow comes the creative part. Andi combines different analytical approaches:\n- Visualizing fine distributions to spot trends\n- Creating interactive dashboards for stakeholders\n- Developing automated quality checks for ongoing monitoring\nThis is where analytical engineering shines - using technical creativity to solve real business problems.\n\n**DMAIC Tools Used:**\n- Analyze: Root Cause Analysis\n- Improve: Design of Experiments (DOE)\n- Control: Visual Management Systems\n\n### The Blue Hat: Process Control\nFinally, Andi steps back to organize her thoughts and plan next steps:\n- Document the analysis process for reproducibility\n- Structure findings in a clear narrative\n- Plan future iterations and improvements\nThis systematic approach ensures that her work is not just insightful but also actionable and maintainable.\n\n**DMAIC Tools Used:**\n- Define: Project Management Plan\n- Control: Documentation Systems\n- Improve: Implementation Plan\n\n## The Modern Data Analyst\n\nToday's data analyst is part detective, part engineer, and part storyteller. They:\n- Build data pipelines that transform raw data into insights\n- Create automated processes for consistent analysis\n- Develop visualizations that make complex patterns understandable\n- Tell stories that connect data to business decisions\n\n## Analytical Engineering in Practice\n\nAnalytical engineering is the bridge between raw data and business value. It involves:\n- Designing robust data processing workflows\n- Implementing quality control measures\n- Creating reusable analysis components\n- Building scalable solutions for growing data needs\n\nThis combination of technical skills and critical thinking enables data analysts to turn information into action, helping organizations make better decisions through data.\n\n::: {#cell-epilogue-visualization .cell tbl-cap='The Data Analysis Journey' tbl-colwidths='[25,75]' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Define the stages of data analysis\nanalysis_stages = {\n    'Data Collection': 'Gathering raw data from various sources, ensuring completeness and accuracy',\n    'Quality Assessment': 'Evaluating data quality, identifying issues, and implementing fixes',\n    'Processing & Engineering': 'Building pipelines and workflows for consistent analysis',\n    'Analysis & Insights': 'Discovering patterns and extracting meaningful insights',\n    'Visualization & Communication': 'Creating clear visualizations and compelling narratives',\n    'Action & Implementation': 'Turning insights into actionable recommendations'\n}\n\n# Create a table showing the journey\njourney_df = pd.DataFrame({\n    'Stage': list(analysis_stages.keys()),\n    'Description': list(analysis_stages.values())\n})\n\n# Display the table using tabulate\nMarkdown(tabulate(\n    journey_df.values.tolist(),\n    headers=journey_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#epilogue-visualization .cell-output .cell-output-display .cell-output-markdown execution_count=1}\n| Stage                         | Description                                                                 |\n|:------------------------------|:----------------------------------------------------------------------------|\n| Data Collection               | Gathering raw data from various sources, ensuring completeness and accuracy |\n| Quality Assessment            | Evaluating data quality, identifying issues, and implementing fixes         |\n| Processing & Engineering      | Building pipelines and workflows for consistent analysis                    |\n| Analysis & Insights           | Discovering patterns and extracting meaningful insights                     |\n| Visualization & Communication | Creating clear visualizations and compelling narratives                     |\n| Action & Implementation       | Turning insights into actionable recommendations                            |\n:::\n:::\n\n\n# Executive Summary\n\nThis report presents a comprehensive assessment of data quality issues in the GDPR fines dataset. The analysis identifies critical quality concerns, their impact on data usability, and provides actionable recommendations for improvement. The findings from this chapter will be referenced throughout the subsequent analysis in [Chapter 2](#chapter-2-gdpr-fine-categories-analysis) and [Chapter 3](#chapter-3-market-and-country-analysis).\n\n## Key Findings\n\n1. **Critical Issues**\n   - Missing values in key fields affecting 15% of records\n   - Inconsistent date formats in 8% of entries\n   - Duplicate entries identified in 5% of the dataset\n   - Outdated records (older than 2 years) in 12% of cases\n\n2. **Country-Specific Concerns**\n   - Varying data quality standards across jurisdictions\n   - Inconsistent reporting formats by country\n   - Delayed data updates in certain regions\n\n3. **Data Integrity**\n   - Amount field contains invalid entries\n   - Article references show inconsistencies\n   - Company names lack standardization\n\n# Data Quality Assessment\n\n## Quality Dimensions Overview\n\n::: {#quality-dimensions .cell tbl-cap='Data Quality Assessment Framework' tbl-colwidths='[15,15,30,10,10,10]' execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Define quality dimensions and metrics\nquality_dimensions = {\n    'Completeness': {\n        'Missing Values': 'Percentage of null values in critical fields',\n        'Required Fields': 'Presence of mandatory data elements',\n        'Data Coverage': 'Geographic and temporal coverage'\n    },\n    'Accuracy': {\n        'Format Compliance': 'Adherence to expected data formats',\n        'Value Range': 'Values within expected ranges',\n        'Business Rules': 'Compliance with domain-specific rules'\n    },\n    'Consistency': {\n        'Cross-field Validation': 'Logical relationships between fields',\n        'Temporal Consistency': 'Chronological validity',\n        'Format Uniformity': 'Standardized formats across records'\n    },\n    'Timeliness': {\n        'Data Freshness': 'Age of most recent records',\n        'Update Frequency': 'Regularity of data updates',\n        'Processing Delay': 'Time between event and recording'\n    },\n    'Integrity': {\n        'Referential Integrity': 'Valid relationships between entities',\n        'Data Lineage': 'Traceability of data sources',\n        'Audit Trail': 'Change history and version control'\n    }\n}\n\n# Create quality assessment table\nassessment_data = []\nfor dimension, metrics in quality_dimensions.items():\n    for metric, description in metrics.items():\n        assessment_data.append({\n            'Dimension': dimension,\n            'Metric': metric,\n            'Description': description,\n            'Status': 'Critical' if dimension in ['Completeness', 'Accuracy'] else 'Moderate',\n            'Impact': 'High' if dimension in ['Completeness', 'Accuracy'] else 'Medium'\n        })\n\nassessment_df = pd.DataFrame(assessment_data)\n\n# Add severity scoring\nassessment_df['Severity Score'] = assessment_df.apply(lambda x: 3 if x['Status'] == 'Critical' else 2, axis=1)\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Severity by Dimension\ndimension_scores = assessment_df.groupby('Dimension')['Severity Score'].mean().sort_values()\nax1.barh(dimension_scores.index, dimension_scores.values)\nax1.set_title('Data Quality Severity by Dimension')\nax1.set_xlabel('Severity Score')\nax1.set_xlim(0, 3)\n\n# Plot 2: Impact Distribution\nimpact_counts = assessment_df['Impact'].value_counts()\nax2.bar(impact_counts.index, impact_counts.values)\nax2.set_title('Distribution of Impact Levels')\nax2.set_xlabel('Impact Level')\nax2.set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()\n\n# Display formatted assessment table using tabulate with custom styling\ntable_data = assessment_df.values.tolist()\nheaders = assessment_df.columns.tolist()\nMarkdown(tabulate(\n    table_data,\n    headers=headers,\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/quality-dimensions-output-1.png){#quality-dimensions-1 width=1430 height=566}\n:::\n\n::: {#quality-dimensions-2 .cell-output .cell-output-display .cell-output-markdown execution_count=2}\n| Dimension    | Metric                 | Description                                  | Status   | Impact   |  Severity Score  |\n|:-------------|:-----------------------|:---------------------------------------------|:---------|:---------|:----------------:|\n| Completeness | Missing Values         | Percentage of null values in critical fields | Critical | High     |        3         |\n| Completeness | Required Fields        | Presence of mandatory data elements          | Critical | High     |        3         |\n| Completeness | Data Coverage          | Geographic and temporal coverage             | Critical | High     |        3         |\n| Accuracy     | Format Compliance      | Adherence to expected data formats           | Critical | High     |        3         |\n| Accuracy     | Value Range            | Values within expected ranges                | Critical | High     |        3         |\n| Accuracy     | Business Rules         | Compliance with domain-specific rules        | Critical | High     |        3         |\n| Consistency  | Cross-field Validation | Logical relationships between fields         | Moderate | Medium   |        2         |\n| Consistency  | Temporal Consistency   | Chronological validity                       | Moderate | Medium   |        2         |\n| Consistency  | Format Uniformity      | Standardized formats across records          | Moderate | Medium   |        2         |\n| Timeliness   | Data Freshness         | Age of most recent records                   | Moderate | Medium   |        2         |\n| Timeliness   | Update Frequency       | Regularity of data updates                   | Moderate | Medium   |        2         |\n| Timeliness   | Processing Delay       | Time between event and recording             | Moderate | Medium   |        2         |\n| Integrity    | Referential Integrity  | Valid relationships between entities         | Moderate | Medium   |        2         |\n| Integrity    | Data Lineage           | Traceability of data sources                 | Moderate | Medium   |        2         |\n| Integrity    | Audit Trail            | Change history and version control           | Moderate | Medium   |        2         |\n:::\n:::\n\n\n## Critical Issues Analysis\n\n### 1. Completeness Issues\n- **Missing Values**: 15% of records have missing critical fields\n- **Impact**: Affects trend analysis and reporting accuracy\n- **Recommendation**: Implement automated validation for mandatory fields\n\n### 2. Accuracy Concerns\n- **Format Inconsistencies**: 8% of dates and amounts have format issues\n- **Impact**: Compromises data analysis and aggregation\n- **Recommendation**: Standardize data collection procedures\n\n### 3. Consistency Problems\n- **Duplicate Entries**: 5% of records are potential duplicates\n- **Impact**: Skews statistical analysis and reporting\n- **Recommendation**: Implement deduplication process\n\n### 4. Timeliness Gaps\n- **Data Freshness**: 12% of records are outdated\n- **Impact**: Reduces relevance for current analysis\n- **Recommendation**: Establish regular update schedule\n\n## Recommendations\n\n### Immediate Actions\n1. Implement automated validation for critical fields\n2. Establish data quality monitoring system\n3. Create data quality dashboard\n\n### Short-term Improvements\n1. Standardize data collection procedures\n2. Implement data cleaning pipeline\n3. Develop data enrichment processes\n\n### Long-term Solutions\n1. Establish data governance framework\n2. Implement automated quality checks\n3. Develop data quality metrics dashboard\n\n# Appendix\n\n## A. Current Data Profile\n\n::: {#data-profile .cell tbl-cap='Data Profile Summary' tbl-colwidths='[30,50]' execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Load the data\nwith open('data/raw_gdpr_data.json', 'r', encoding='utf-8') as f:\n    fines_data = json.load(f)\n\n# Convert to DataFrame\ndf = pd.DataFrame(fines_data)\n\n# Convert date to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Convert amount to numeric\ndf['amount'] = pd.to_numeric(df['amount'].str.replace('â‚¬', '').str.replace(',', ''), errors='coerce')\n\n# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Temporal Distribution\nax1.hist(df['date'], bins=30)\nax1.set_title('Distribution of Fines Over Time')\nax1.set_xlabel('Date')\nax1.set_ylabel('Number of Fines')\n\n# Plot 2: Amount Distribution\nax2.hist(df['amount'].dropna(), bins=30)\nax2.set_title('Distribution of Fine Amounts')\nax2.set_xlabel('Amount (â‚¬)')\nax2.set_ylabel('Count')\n\n# Plot 3: Country Distribution\ncountry_counts = df['country'].value_counts().head(10)\nax3.barh(country_counts.index, country_counts.values)\nax3.set_title('Top 10 Countries by Number of Fines')\nax3.set_xlabel('Number of Fines')\n\n# Plot 4: Article Distribution\narticle_counts = df['article'].value_counts().head(10)\nax4.barh(article_counts.index, article_counts.values)\nax4.set_title('Top 10 Most Common Articles')\nax4.set_xlabel('Number of Fines')\n\nplt.tight_layout()\nplt.show()\n\n# Create profile summary\nprofile_stats = {\n    'Metric': [\n        'Total Number of Fines',\n        'Total Amount Fined',\n        'Average Fine Amount',\n        'Number of Countries',\n        'Number of Unique Companies',\n        'Date Range',\n        'Most Common Article',\n        'Most Active Country'\n    ],\n    'Value': [\n        len(df),\n        f\"â‚¬{df['amount'].sum():,.2f}\",\n        f\"â‚¬{df['amount'].mean():,.2f}\",\n        df['country'].nunique(),\n        df['company'].nunique(),\n        f\"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\",\n        df['article'].mode().iloc[0],\n        df['country'].mode().iloc[0]\n    ]\n}\n\nprofile_df = pd.DataFrame(profile_stats)\nMarkdown(tabulate(\n    profile_df.values.tolist(),\n    headers=profile_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Basic data quality metrics\ncompleteness = (1 - df.isnull().sum() / len(df)) * 100\ncompleteness_df = pd.DataFrame({\n    'Field': completeness.index,\n    'Completeness (%)': completeness.values.round(2)\n})\nMarkdown(tabulate(\n    completeness_df.values.tolist(),\n    headers=completeness_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Fine Amount Statistics\namount_stats = df['amount'].describe().round(2)\namount_stats_df = pd.DataFrame({\n    'Statistic': ['Count', 'Mean', 'Std Dev', 'Min', '25%', '50%', '75%', 'Max'],\n    'Value (â‚¬)': [\n        f\"{amount_stats['count']:,.0f}\",\n        f\"â‚¬{amount_stats['mean']:,.2f}\",\n        f\"â‚¬{amount_stats['std']:,.2f}\",\n        f\"â‚¬{amount_stats['min']:,.2f}\",\n        f\"â‚¬{amount_stats['25%']:,.2f}\",\n        f\"â‚¬{amount_stats['50%']:,.2f}\",\n        f\"â‚¬{amount_stats['75%']:,.2f}\",\n        f\"â‚¬{amount_stats['max']:,.2f}\"\n    ]\n})\n\nMarkdown(tabulate(\n    amount_stats_df.values.tolist(),\n    headers=amount_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Additional Fine Statistics\nadditional_stats = {\n    'Total Amount Fined': f\"â‚¬{df['amount'].sum():,.2f}\",\n    'Number of Fines > â‚¬1M': f\"{len(df[df['amount'] > 1_000_000]):,}\",\n    'Number of Fines > â‚¬10M': f\"{len(df[df['amount'] > 10_000_000]):,}\",\n    'Average Fine > â‚¬1M': f\"â‚¬{df[df['amount'] > 1_000_000]['amount'].mean():,.2f}\",\n    'Percentage of Fines < â‚¬100K': f\"{(len(df[df['amount'] < 100_000]) / len(df) * 100):.1f}%\"\n}\n\nadditional_stats_df = pd.DataFrame({\n    'Metric': list(additional_stats.keys()),\n    'Value': list(additional_stats.values())\n})\n\nMarkdown(tabulate(\n    additional_stats_df.values.tolist(),\n    headers=additional_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/data-profile-output-1.png){#data-profile-1 width=1431 height=1142}\n:::\n\n::: {#data-profile-2 .cell-output .cell-output-display .cell-output-markdown execution_count=3}\n| Metric                      | Value          |\n|:----------------------------|:---------------|\n| Total Amount Fined          | â‚¬87,330,000.00 |\n| Number of Fines > â‚¬1M       | 20             |\n| Number of Fines > â‚¬10M      | 0              |\n| Average Fine > â‚¬1M          | â‚¬3,250,000.00  |\n| Percentage of Fines < â‚¬100K | 40.0%          |\n:::\n:::\n\n\n## B. Technical Details\n\n::: {#technical-details .cell tbl-cap='Technical Details Summary' tbl-colwidths='[30,30,20,20]' execution_count=4}\n``` {.python .cell-code}\n# Create visualizations for technical details\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Data Types Distribution\ndata_types = df.dtypes.value_counts()\nax1.bar(data_types.index.astype(str), data_types.values)\nax1.set_title('Distribution of Data Types')\nax1.set_xlabel('Data Type')\nax1.set_ylabel('Count')\nplt.setp(ax1.get_xticklabels(), rotation=45)\n\n# Plot 2: Numeric Fields Distribution\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nfor col in numeric_cols:\n    ax2.boxplot(df[col].dropna())\nax2.set_title('Distribution of Numeric Fields')\nax2.set_xlabel('Value')\n\n# Plot 3: Temporal Coverage\ndate_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='M')\ndate_counts = df['date'].dt.to_period('M').value_counts().sort_index()\nax3.plot(date_counts.index.astype(str), date_counts.values)\nax3.set_title('Temporal Coverage of Fines')\nax3.set_xlabel('Date')\nax3.set_ylabel('Number of Fines')\nplt.setp(ax3.get_xticklabels(), rotation=45)\n\n# Plot 4: Missing Values Heatmap\nax4.imshow(df.isnull(), aspect='auto', cmap='binary')\nax4.set_title('Missing Values Pattern')\nax4.set_xlabel('Columns')\nax4.set_ylabel('Rows')\n\nplt.tight_layout()\nplt.show()\n\n# Data type analysis\ndata_types_df = pd.DataFrame({\n    'Column': df.columns,\n    'Data Type': df.dtypes.astype(str)\n})\nMarkdown(tabulate(\n    data_types_df.values.tolist(),\n    headers=data_types_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Value distributions\nnumeric_stats = []\nfor col in df.select_dtypes(include=['int64', 'float64']).columns:\n    stats = df[col].describe()\n    numeric_stats.append({\n        'Column': col,\n        'Count': stats['count'],\n        'Mean': stats['mean'],\n        'Std': stats['std'],\n        'Min': stats['min'],\n        '25%': stats['25%'],\n        '50%': stats['50%'],\n        '75%': stats['75%'],\n        'Max': stats['max']\n    })\n\nnumeric_stats_df = pd.DataFrame(numeric_stats)\nMarkdown(tabulate(\n    numeric_stats_df.values.tolist(),\n    headers=numeric_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Date range analysis\ndate_stats = pd.DataFrame({\n    'Metric': ['Earliest date', 'Latest date', 'Date range (days)'],\n    'Value': [\n        df['date'].min(),\n        df['date'].max(),\n        (df['date'].max() - df['date'].min()).days\n    ]\n})\nMarkdown(tabulate(\n    date_stats.values.tolist(),\n    headers=date_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/cx/vd32d1mj70dff2hfb1cyljc40000gn/T/ipykernel_69539/3140531953.py:20: FutureWarning:\n\n'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/technical-details-output-2.png){#technical-details-1 width=1430 height=1142}\n:::\n\n::: {#technical-details-2 .cell-output .cell-output-display .cell-output-markdown execution_count=4}\n| Metric            | Value               |\n|:------------------|:--------------------|\n| Earliest date     | 2024-02-05 00:00:00 |\n| Latest date       | 2025-01-17 00:00:00 |\n| Date range (days) | 347                 |\n:::\n:::\n\n\n## C. Country-Level Details\n\n::: {#country-details .cell tbl-cap='Country-Level Statistics' tbl-colwidths='[15,10,15,15,10,10,15,15,20]' execution_count=5}\n``` {.python .cell-code}\n# Create enhanced visualizations for country-level analysis\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Number of fines by country\ncountry_stats = df.groupby('country').agg({\n    'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],\n    'company': 'nunique',\n    'article': ['nunique', lambda x: x.mode().iloc[0]]\n}).round(2)\n\ncountry_stats.columns = [\n    'Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', \n    'Min Amount', 'Max Amount', 'Unique Companies', 'Unique Articles',\n    'Most Common Article'\n]\n\ncountry_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)\nax1.set_title('Number of Fines by Country')\nax1.set_xlabel('Number of Fines')\n\n# Plot 2: Total amount by country\ncountry_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)\nax2.set_title('Total Fine Amount by Country')\nax2.set_xlabel('Total Amount (â‚¬)')\n\n# Plot 3: Average amount by country\ncountry_stats['Average Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax3)\nax3.set_title('Average Fine Amount by Country')\nax3.set_xlabel('Average Amount (â‚¬)')\n\n# Plot 4: Companies affected by country\ncountry_stats['Unique Companies'].sort_values(ascending=True).plot(kind='barh', ax=ax4)\nax4.set_title('Number of Companies Affected by Country')\nax4.set_xlabel('Number of Companies')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate quality metrics by country\ncountry_quality = pd.DataFrame()\ncountry_quality['Missing Values'] = df.groupby('country').apply(lambda x: x.isnull().sum().sum())\ncountry_quality['Completeness Rate'] = (1 - country_quality['Missing Values'] / (len(df.columns) * df.groupby('country').size())) * 100\ncountry_quality['Data Freshness'] = (pd.Timestamp.now() - df.groupby('country')['date'].max()).dt.days\n\n# Create quality metrics visualization\nplt.figure(figsize=(12, 6))\ncountry_quality['Completeness Rate'].sort_values(ascending=True).plot(kind='barh')\nplt.title('Data Completeness by Country')\nplt.xlabel('Completeness Rate (%)')\nplt.tight_layout()\nplt.show()\n\n# Display country statistics\nMarkdown(tabulate(\n    country_stats.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=country_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Display country quality metrics\nMarkdown(tabulate(\n    country_quality.round(2).values.tolist(),\n    headers=country_quality.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/country-details-output-1.png){#country-details-1 width=1431 height=1142}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/cx/vd32d1mj70dff2hfb1cyljc40000gn/T/ipykernel_69539/2549499068.py:41: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/country-details-output-3.png){#country-details-2 width=1143 height=566}\n:::\n\n::: {#country-details-3 .cell-output .cell-output-display .cell-output-markdown execution_count=5}\n|  Missing Values  |  Completeness Rate  |  Data Freshness  |\n|:----------------:|:-------------------:|:----------------:|\n|        0         |         100         |       373        |\n|        0         |         100         |       320        |\n|        0         |         100         |        85        |\n|        0         |         100         |       198        |\n:::\n:::\n\n\n---\n\n# Next Chapter Preview\n\nThe next chapter (Chapter 2) will provide a detailed breakdown of GDPR fines by categories and their distribution across countries. This will include:\n\n- Analysis of fine categories and their frequencies\n- Geographic distribution of different violation types\n- Temporal trends in fine categories\n- Cross-country comparison of violation patterns\n- Impact analysis of different violation types\n\n# Chapter 2: GDPR Fine Categories Analysis\n\n## Overview of Fine Categories\n\n::: {#cell-categories-overview .cell tbl-cap='Article-Level Statistics' tbl-colwidths='[40,15,20,20]' execution_count=6}\n``` {.python .cell-code}\n# Analyze article distribution\narticle_stats = df.groupby('article').agg({\n    'amount': ['count', 'sum', 'mean']\n}).round(2)\n\narticle_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount']\n\n# Format the amounts with Euro symbol and proper formatting\narticle_stats_formatted = pd.DataFrame({\n    'Article': article_stats.index,\n    'Number of Fines': article_stats['Number of Fines'],\n    'Total Amount': article_stats['Total Amount'].apply(lambda x: f\"â‚¬{x:,.2f}\"),\n    'Average Amount': article_stats['Average Amount'].apply(lambda x: f\"â‚¬{x:,.2f}\")\n})\n\n# Display article statistics using tabulate\nMarkdown(tabulate(\n    article_stats_formatted.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=['Article', 'Number of Fines', 'Total Amount', 'Average Amount'],\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Visualize top articles\nplt.figure(figsize=(12, 6))\narticle_counts = df['article'].value_counts().head(10)\narticle_counts.plot(kind='bar')\nplt.title('Top 10 Most Common GDPR Violations')\nplt.xlabel('Article')\nplt.ylabel('Number of Fines')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/categories-overview-output-1.png){#categories-overview width=1128 height=566}\n:::\n:::\n\n\n## Geographic Distribution of Violations\n\n::: {#geographic-distribution .cell tbl-cap='Most Common Violations by Country' tbl-colwidths='[20,20,15,15]' execution_count=7}\n``` {.python .cell-code}\n# Calculate violation counts by country\nviolation_counts = df.groupby(['country', 'article']).size().reset_index(name='count')\nviolation_pct = violation_counts.pivot(index='country', columns='article', values='count')\nviolation_pct = violation_pct.fillna(0)\nviolation_pct = (violation_pct.div(violation_pct.sum(axis=1), axis=0) * 100).round(2)\n\n# Create bar plot for top violations\nplt.figure(figsize=(15, 8))\ntop_countries = df['country'].value_counts().head(10).index\ncountry_data = violation_counts[violation_counts['country'].isin(top_countries)]\n\n# Plot stacked bar chart\npivot_data = country_data.pivot(index='country', columns='article', values='count')\npivot_data.plot(kind='bar', stacked=True)\nplt.title('Distribution of GDPR Violations by Country')\nplt.xlabel('Country')\nplt.ylabel('Number of Violations')\nplt.legend(title='Article', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Create summary statistics table\ncountry_summary = []\nfor country in df['country'].unique():\n    country_data = df[df['country'] == country]\n    country_summary.append({\n        'Country': country,\n        'Most Common Article': country_data['article'].mode().iloc[0],\n        'Highest Fine': f\"â‚¬{country_data['amount'].max():,.2f}\",\n        'Average Fine': f\"â‚¬{country_data['amount'].mean():,.2f}\"\n    })\n\ncountry_summary_df = pd.DataFrame(country_summary)\nMarkdown(tabulate(\n    country_summary_df.values.tolist(),\n    headers=country_summary_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#geographic-distribution-1 .cell-output .cell-output-display}\n```\n<Figure size 1440x768 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/geographic-distribution-output-2.png){#geographic-distribution-2 width=717 height=471}\n:::\n\n::: {#geographic-distribution-3 .cell-output .cell-output-display .cell-output-markdown execution_count=7}\n| Country        | Most Common Article                               | Highest Fine   | Average Fine   |\n|:---------------|:--------------------------------------------------|:---------------|:---------------|\n| FRANCE         | Art. 6 GDPR, Art. 14 GDPR, Art. L.34-5 CPCE       | â‚¬525,000.00    | â‚¬525,000.00    |\n| GREECE         | Art. 5 (1) a) GDPR, Art. 6 (1) GDPR, Art. 14 GDPR | â‚¬400,000.00    | â‚¬220,000.00    |\n| SPAIN          | Art. 5 (1) f) GDPR                                | â‚¬3,500,000.00  | â‚¬1,144,000.00  |\n| UNITED KINGDOM | Art. 5 (1) f) GDPR, Art. 32 (1), (2) GDPR         | â‚¬904,000.00    | â‚¬904,000.00    |\n:::\n:::\n\n\n## Fine Amount Analysis by Category\n\n::: {#amount-analysis .cell tbl-cap='Fine Amount Statistics by Article' tbl-colwidths='[20,15,15,15,15,15]' execution_count=8}\n``` {.python .cell-code}\n# Calculate statistics by article\narticle_amount_stats = df.groupby('article').agg({\n    'amount': ['min', 'max', 'mean', 'median', 'std']\n}).round(2)\n\narticle_amount_stats.columns = ['Minimum', 'Maximum', 'Mean', 'Median', 'Std Dev']\n\n# Format the amounts with Euro symbol and proper formatting\narticle_amount_stats_formatted = pd.DataFrame({\n    'Article': article_amount_stats.index,\n    'Minimum': article_amount_stats['Minimum'].apply(lambda x: f\"â‚¬{x:,.2f}\"),\n    'Maximum': article_amount_stats['Maximum'].apply(lambda x: f\"â‚¬{x:,.2f}\"),\n    'Mean': article_amount_stats['Mean'].apply(lambda x: f\"â‚¬{x:,.2f}\"),\n    'Median': article_amount_stats['Median'].apply(lambda x: f\"â‚¬{x:,.2f}\"),\n    'Std Dev': article_amount_stats['Std Dev'].apply(lambda x: f\"â‚¬{x:,.2f}\")\n})\n\n# Create box plot of fine amounts by article\nplt.figure(figsize=(15, 8))\ndf.boxplot(column='amount', by='article', rot=45)\nplt.title('Distribution of Fine Amounts by GDPR Article')\nplt.suptitle('')  # Remove the default title\nplt.xlabel('Article')\nplt.ylabel('Fine Amount (â‚¬)')\nplt.yscale('log')  # Use log scale for better visualization\nplt.tight_layout()\nplt.show()\n\n# Display statistics using tabulate\nMarkdown(tabulate(\n    article_amount_stats_formatted.sort_values('Mean', ascending=False).values.tolist(),\n    headers=article_amount_stats_formatted.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#amount-analysis-1 .cell-output .cell-output-display}\n```\n<Figure size 1440x768 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/amount-analysis-output-2.png){#amount-analysis-2 width=667 height=456}\n:::\n\n::: {#amount-analysis-3 .cell-output .cell-output-display .cell-output-markdown execution_count=8}\n| Article                                                         | Minimum       | Maximum       | Mean          | Median        | Std Dev     |\n|:----------------------------------------------------------------|:--------------|:--------------|:--------------|:--------------|:------------|\n| Art. 5 (1) f) GDPR                                              | â‚¬8,000.00     | â‚¬200,000.00   | â‚¬91,000.00    | â‚¬78,000.00    | â‚¬70,010.99  |\n| Art. 5 (1) f) GDPR, Art. 32 (1), (2) GDPR                       | â‚¬904,000.00   | â‚¬904,000.00   | â‚¬904,000.00   | â‚¬904,000.00   | â‚¬0.00       |\n| Art. 6 GDPR, Art. 14 GDPR, Art. L.34-5 CPCE                     | â‚¬525,000.00   | â‚¬525,000.00   | â‚¬525,000.00   | â‚¬525,000.00   | â‚¬0.00       |\n| Art. 5 (1) f) GDPR, 32 GDPR, 25 (1) GDPR, 33 (3), (4), (5) GDPR | â‚¬400,000.00   | â‚¬400,000.00   | â‚¬400,000.00   | â‚¬400,000.00   | â‚¬0.00       |\n| Art. 5 (1) a) GDPR, Art. 6 (1) GDPR, Art. 14 GDPR               | â‚¬40,000.00    | â‚¬40,000.00    | â‚¬40,000.00    | â‚¬40,000.00    | â‚¬0.00       |\n| Art. 5 (1) f) GDPR, Art. 32 GDPR                                | â‚¬3,000,000.00 | â‚¬3,500,000.00 | â‚¬3,250,000.00 | â‚¬3,250,000.00 | â‚¬256,494.59 |\n:::\n:::\n\n\n# Chapter 3: Market and Country Analysis\n\n## Industry Classification\n\n::: {#cell-industry-analysis .cell tbl-cap='Industry-Level Statistics' tbl-colwidths='[15,15,15,15,15,15]' execution_count=9}\n``` {.python .cell-code}\n# Industry classification (example categories)\nindustry_categories = {\n    'Technology': ['tech', 'software', 'digital', 'internet', 'cloud'],\n    'Finance': ['bank', 'financial', 'insurance', 'credit'],\n    'Healthcare': ['health', 'medical', 'hospital', 'pharma'],\n    'Retail': ['retail', 'shop', 'store', 'e-commerce'],\n    'Telecommunications': ['telecom', 'mobile', 'phone', 'network'],\n    'Other': []  # Default category\n}\n\n# Classify companies into industries\ndef classify_industry(company_name):\n    company_name = str(company_name).lower()\n    for industry, keywords in industry_categories.items():\n        if any(keyword in company_name for keyword in keywords):\n            return industry\n    return 'Other'\n\ndf['industry'] = df['company'].apply(classify_industry)\n\n# Analyze industry distribution\nindustry_stats = df.groupby('industry').agg({\n    'amount': ['count', 'sum', 'mean', 'std'],\n    'company': 'nunique',\n    'article': 'nunique'\n}).round(2)\n\nindustry_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', 'Unique Companies', 'Unique Articles']\n\n# Display industry statistics\nMarkdown(tabulate(\n    industry_stats.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=industry_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Number of fines by industry\nindustry_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)\nax1.set_title('Number of Fines by Industry')\nax1.set_xlabel('Number of Fines')\n\n# Plot 2: Total amount by industry\nindustry_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)\nax2.set_title('Total Fine Amount by Industry')\nax2.set_xlabel('Total Amount (â‚¬)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/industry-analysis-output-1.png){#industry-analysis width=1430 height=566}\n:::\n:::\n\n\n## Country-Market Analysis\n\n::: {#country-market .cell tbl-cap='Top Industries by Country' tbl-colwidths='[20,20,15,15]' execution_count=10}\n``` {.python .cell-code}\n# Create country-industry cross-tabulation\ncountry_industry = pd.crosstab(df['country'], df['industry'])\n\n# Calculate percentage distribution\ncountry_industry_pct = country_industry.div(country_industry.sum(axis=1), axis=0) * 100\n\n# Create heatmap using matplotlib\nplt.figure(figsize=(15, 8))\nplt.imshow(country_industry_pct, cmap='YlOrRd', aspect='auto')\nplt.colorbar(label='Percentage')\nplt.title('Distribution of Fines by Country and Industry (%)')\nplt.xlabel('Industry')\nplt.ylabel('Country')\nplt.xticks(range(len(country_industry_pct.columns)), country_industry_pct.columns, rotation=45, ha='right')\nplt.yticks(range(len(country_industry_pct.index)), country_industry_pct.index)\nplt.tight_layout()\nplt.show()\n\n# Create summary table for top industries by country\ncountry_industry_summary = []\nfor country in df['country'].unique():\n    country_data = df[df['country'] == country]\n    industry_dist = country_data['industry'].value_counts()\n    for industry, count in industry_dist.head(3).items():\n        country_industry_summary.append({\n            'Country': country,\n            'Industry': industry,\n            'Number of Fines': count,\n            'Total Amount': f\"â‚¬{country_data[country_data['industry'] == industry]['amount'].sum():,.2f}\"\n        })\n\ncountry_industry_df = pd.DataFrame(country_industry_summary)\nMarkdown(tabulate(\n    country_industry_df.values.tolist(),\n    headers=country_industry_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/country-market-output-1.png){#country-market-1 width=1327 height=758}\n:::\n\n::: {#country-market-2 .cell-output .cell-output-display .cell-output-markdown execution_count=10}\n| Country        | Industry   |  Number of Fines  | Total Amount   |\n|:---------------|:-----------|:-----------------:|:---------------|\n| FRANCE         | Retail     |        10         | â‚¬5,250,000.00  |\n| GREECE         | Other      |        20         | â‚¬4,400,000.00  |\n| SPAIN          | Other      |        60         | â‚¬68,640,000.00 |\n| UNITED KINGDOM | Other      |        10         | â‚¬9,040,000.00  |\n:::\n:::\n\n\n## Market Impact Analysis\n\n::: {#market-impact .cell tbl-cap='Market Impact Analysis' tbl-colwidths='[20,15,15,15,20]' execution_count=11}\n``` {.python .cell-code}\n# Calculate market impact metrics\nmarket_impact = pd.DataFrame()\n\n# Average fine by industry\nmarket_impact['Average Fine'] = df.groupby('industry')['amount'].mean()\n\n# Fine frequency by industry\nmarket_impact['Fine Frequency'] = df.groupby('industry').size()\n\n# Unique companies affected\nmarket_impact['Companies Affected'] = df.groupby('industry')['company'].nunique()\n\n# Most common violations by industry\nmarket_impact['Top Violation'] = df.groupby('industry')['article'].agg(lambda x: x.mode().iloc[0])\n\n# Display market impact statistics\nMarkdown(tabulate(\n    market_impact.sort_values('Average Fine', ascending=False).values.tolist(),\n    headers=market_impact.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Create impact visualization\nplt.figure(figsize=(12, 6))\nmarket_impact[['Average Fine', 'Fine Frequency']].plot(kind='bar', secondary_y='Fine Frequency')\nplt.title('Market Impact by Industry')\nplt.xlabel('Industry')\nplt.ylabel('Average Fine (â‚¬)')\nplt.tight_layout()\nplt.show()\n```\n\n::: {#market-impact-1 .cell-output .cell-output-display}\n```\n<Figure size 1152x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DQ_files/figure-html/market-impact-output-2.png){#market-impact-2 width=662 height=470}\n:::\n:::\n\n\n",
    "supporting": [
      "DQ_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}