{
  "hash": "2fa42f0b35e9dec5b79b06376411d953",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Book 3 - From Theory to Practice: Getting Your Hands Dirty\"\nformat: \n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: true\n    theme: cosmo\n    highlight-style: github\n    code-tools: true\n---\n\n> \"The best way to learn is to do; the worst way to teach is to talk.\"  \n> â€” Paul Halmos\n\n# Chapter 1: Implementation Deep Dive\n\n## Andi's Implementation Journey\n\nAfter establishing the documentation framework and metrics, Andi moves into the implementation phase. Her team needs practical guidance on turning theory into practice. Let's follow her journey of transforming concepts into working solutions.\n\n### Setting Up the Development Environment\n\n::: {#cell-setup-requirements .cell tbl-cap='Project Dependencies' tbl-colwidths='[20,15,65]' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ndependencies = {\n    'Component': [\n        'Python',\n        'Pandas',\n        'SQLAlchemy',\n        'FastAPI',\n        'Docker',\n        'Git',\n        'PostgreSQL',\n        'Redis',\n        'Elasticsearch'\n    ],\n    'Version': [\n        '3.9+',\n        '2.0+',\n        '2.0+',\n        '0.95+',\n        '24.0+',\n        '2.40+',\n        '15.0+',\n        '7.0+',\n        '8.0+'\n    ],\n    'Purpose': [\n        'Core programming language',\n        'Data manipulation and analysis',\n        'Database ORM and management',\n        'API development and documentation',\n        'Containerization and deployment',\n        'Version control and collaboration',\n        'Primary data storage',\n        'Caching and queue management',\n        'Search and analytics engine'\n    ]\n}\n\ndeps_df = pd.DataFrame(dependencies)\nMarkdown(tabulate(\n    deps_df.values.tolist(),\n    headers=deps_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#setup-requirements .cell-output .cell-output-display .cell-output-markdown execution_count=1}\n| Component     | Version   | Purpose                           |\n|:--------------|:----------|:----------------------------------|\n| Python        | 3.9+      | Core programming language         |\n| Pandas        | 2.0+      | Data manipulation and analysis    |\n| SQLAlchemy    | 2.0+      | Database ORM and management       |\n| FastAPI       | 0.95+     | API development and documentation |\n| Docker        | 24.0+     | Containerization and deployment   |\n| Git           | 2.40+     | Version control and collaboration |\n| PostgreSQL    | 15.0+     | Primary data storage              |\n| Redis         | 7.0+      | Caching and queue management      |\n| Elasticsearch | 8.0+      | Search and analytics engine       |\n:::\n:::\n\n\n## Chapter 2: Building the Data Pipeline\n\n### ETL Process for GDPR Fines\n\n::: {#cell-etl-workflow .cell tbl-cap='ETL Pipeline Components' tbl-colwidths='[20,20,60]' execution_count=2}\n``` {.python .cell-code}\netl_components = {\n    'Stage': [\n        'Data Collection',\n        'Data Validation',\n        'Data Transformation',\n        'Data Loading',\n        'Data Quality Check',\n        'Documentation Update',\n        'Notification System'\n    ],\n    'Implementation': [\n        'Web Scraping + API',\n        'Schema Validation',\n        'Data Normalization',\n        'Database Loading',\n        'Automated Testing',\n        'Auto-Documentation',\n        'Alert System'\n    ],\n    'Description': [\n        'Collect fines data from enforcement tracker and official sources',\n        'Validate data against predefined schemas and rules',\n        'Transform raw data into normalized database format',\n        'Load processed data into PostgreSQL database',\n        'Run automated quality checks and validations',\n        'Update documentation with new data lineage',\n        'Send notifications for updates and issues'\n    ]\n}\n\netl_df = pd.DataFrame(etl_components)\nMarkdown(tabulate(\n    etl_df.values.tolist(),\n    headers=etl_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#etl-workflow .cell-output .cell-output-display .cell-output-markdown execution_count=2}\n| Stage                | Implementation     | Description                                                      |\n|:---------------------|:-------------------|:-----------------------------------------------------------------|\n| Data Collection      | Web Scraping + API | Collect fines data from enforcement tracker and official sources |\n| Data Validation      | Schema Validation  | Validate data against predefined schemas and rules               |\n| Data Transformation  | Data Normalization | Transform raw data into normalized database format               |\n| Data Loading         | Database Loading   | Load processed data into PostgreSQL database                     |\n| Data Quality Check   | Automated Testing  | Run automated quality checks and validations                     |\n| Documentation Update | Auto-Documentation | Update documentation with new data lineage                       |\n| Notification System  | Alert System       | Send notifications for updates and issues                        |\n:::\n:::\n\n\n## Chapter 3: Code Implementation Examples\n\n### Data Collection Module\n\n```python\nfrom typing import Dict, List\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\nclass GDPRDataCollector:\n    def __init__(self, api_url: str, api_key: str):\n        self.api_url = api_url\n        self.headers = {'Authorization': f'Bearer {api_key}'}\n        \n    def fetch_latest_fines(self) -> List[Dict]:\n        \"\"\"Fetch latest GDPR fines from the enforcement tracker.\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/fines\",\n            headers=self.headers\n        )\n        return response.json()\n    \n    def process_fines(self, fines_data: List[Dict]) -> pd.DataFrame:\n        \"\"\"Process and validate fines data.\"\"\"\n        df = pd.DataFrame(fines_data)\n        \n        # Add processing timestamp\n        df['processed_at'] = datetime.now()\n        \n        # Validate required fields\n        required_fields = ['amount', 'country', 'company', 'date', 'article']\n        missing_fields = [field for field in required_fields \n                         if field not in df.columns]\n        \n        if missing_fields:\n            raise ValueError(f\"Missing required fields: {missing_fields}\")\n            \n        return df\n```\n\n### Data Quality Checks\n\n```python\nclass DataQualityChecker:\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n        self.quality_metrics = {}\n        \n    def check_completeness(self) -> Dict:\n        \"\"\"Check data completeness for each column.\"\"\"\n        completeness = (self.df.count() / len(self.df)) * 100\n        self.quality_metrics['completeness'] = completeness.to_dict()\n        return self.quality_metrics['completeness']\n    \n    def check_validity(self) -> Dict:\n        \"\"\"Check data validity based on business rules.\"\"\"\n        validity = {\n            'amount': (self.df['amount'] >= 0).mean() * 100,\n            'date': (pd.to_datetime(self.df['date'], \n                    errors='coerce').notna()).mean() * 100\n        }\n        self.quality_metrics['validity'] = validity\n        return validity\n```\n\n## Chapter 4: Documentation Generation\n\n### Automated Documentation Example\n\n```python\nfrom pathlib import Path\nimport yaml\nfrom jinja2 import Template\n\nclass DocumentationGenerator:\n    def __init__(self, template_path: str, output_path: str):\n        self.template_path = Path(template_path)\n        self.output_path = Path(output_path)\n        \n    def generate_documentation(self, data: Dict) -> None:\n        \"\"\"Generate documentation from template and data.\"\"\"\n        template = Template(self.template_path.read_text())\n        documentation = template.render(data=data)\n        self.output_path.write_text(documentation)\n        \n    def update_metrics(self, metrics: Dict) -> None:\n        \"\"\"Update documentation metrics.\"\"\"\n        metrics_path = self.output_path / 'metrics.yaml'\n        current_metrics = yaml.safe_load(metrics_path.read_text())\n        current_metrics.update(metrics)\n        metrics_path.write_text(yaml.dump(current_metrics))\n```\n\n## Chapter 5: Testing and Validation\n\n### Unit Test Examples\n\n```python\nimport unittest\nfrom datetime import datetime\n\nclass TestGDPRDataCollector(unittest.TestCase):\n    def setUp(self):\n        self.collector = GDPRDataCollector('http://api.example.com', 'test-key')\n        \n    def test_process_fines(self):\n        test_data = [{\n            'amount': 1000000,\n            'country': 'Germany',\n            'company': 'Test Corp',\n            'date': '2024-03-15',\n            'article': '6'\n        }]\n        \n        df = self.collector.process_fines(test_data)\n        \n        self.assertEqual(len(df), 1)\n        self.assertIn('processed_at', df.columns)\n        self.assertTrue(isinstance(df['processed_at'].iloc[0], datetime))\n```\n\n# Next Steps: Continuous Improvement\n\n## Monitoring and Optimization\n\n::: {#cell-improvement-metrics .cell tbl-cap='Continuous Improvement Metrics' tbl-colwidths='[25,15,60]' execution_count=3}\n``` {.python .cell-code}\nimprovement_metrics = {\n    'Metric': [\n        'Pipeline Performance',\n        'Documentation Coverage',\n        'Test Coverage',\n        'Code Quality',\n        'User Satisfaction',\n        'System Reliability',\n        'Integration Success'\n    ],\n    'Target (%)': [\n        95,\n        100,\n        90,\n        95,\n        90,\n        99.9,\n        98\n    ],\n    'Action Items': [\n        'Optimize ETL processes and reduce processing time',\n        'Ensure all components are fully documented',\n        'Increase unit and integration test coverage',\n        'Maintain high code quality standards',\n        'Regular user feedback and improvements',\n        'Monitor and improve system uptime',\n        'Ensure smooth integration between components'\n    ]\n}\n\nimprovement_df = pd.DataFrame(improvement_metrics)\nMarkdown(tabulate(\n    improvement_df.values.tolist(),\n    headers=improvement_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n```\n\n::: {#improvement-metrics .cell-output .cell-output-display .cell-output-markdown execution_count=3}\n| Metric                 |  Target (%)  | Action Items                                      |\n|:-----------------------|:------------:|:--------------------------------------------------|\n| Pipeline Performance   |      95      | Optimize ETL processes and reduce processing time |\n| Documentation Coverage |     100      | Ensure all components are fully documented        |\n| Test Coverage          |      90      | Increase unit and integration test coverage       |\n| Code Quality           |      95      | Maintain high code quality standards              |\n| User Satisfaction      |      90      | Regular user feedback and improvements            |\n| System Reliability     |     99.9     | Monitor and improve system uptime                 |\n| Integration Success    |      98      | Ensure smooth integration between components      |\n:::\n:::\n\n\n## Future Enhancements\n\n1. **Advanced Analytics Integration**\n   - Machine learning for pattern detection\n   - Predictive analytics for fine trends\n   - Automated report generation\n\n2. **Enhanced Automation**\n   - Automated data collection from multiple sources\n   - Intelligent data validation\n   - Automated documentation updates\n\n3. **User Experience Improvements**\n   - Interactive dashboards\n   - Custom reporting tools\n   - Mobile-friendly interfaces\n\n4. **System Scalability**\n   - Cloud infrastructure optimization\n   - Performance monitoring\n   - Load balancing implementation\n\nRemember: The journey from theory to practice is continuous. Keep iterating, improving, and adapting to new challenges and requirements. \n\n",
    "supporting": [
      "book3_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}