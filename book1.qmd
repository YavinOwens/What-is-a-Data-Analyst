---
title: "Book 1 - Kidlens Law"
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    highlight-style: github
    code-tools: true
---

> "If you write a problem down clearly, then the matter is half solved."  
> â€” Kidlens Law

# Epilogue: The Art and Science of Data Analysis

## A Day in the Life: The Six Thinking Hats of a Data Analyst

Let me tell you a story about Andi, a data analyst working on understanding GDPR compliance patterns. Her journey illustrates how modern data analysts combine analytical engineering with critical thinking using Edward de Bono's Six Thinking Hats approach and the DMAIC methodology.

### The White Hat: Facts and Information
Andi starts her day by gathering facts about GDPR fines across Europe. Like a detective, she collects raw data about fines, violations, and company responses. This is where analytical engineering begins - the systematic process of collecting, cleaning, and organizing data. She knows that good analysis starts with quality data, just as a good house needs a solid foundation.

**DMAIC Tools Used:**
- Define: Project Charter, SIPOC Diagram
- Measure: Data Collection Plan, Operational Definitions
- Analyze: Data Mining, Statistical Analysis

### The Red Hat: Intuition and Feelings
As she dives into the data, Andi notices patterns that trigger her intuition. Some companies seem to repeatedly violate certain articles, while others quickly adapt after their first fine. She doesn't ignore these gut feelings - they're valuable indicators of where to look deeper. This emotional intelligence, combined with technical skills, makes a data analyst more than just a number cruncher.

**DMAIC Tools Used:**
- Measure: Voice of Customer (VOC)
- Analyze: Brainstorming
- Improve: Impact Analysis

### The Black Hat: Critical Judgment
Andi puts on her critical thinking hat to identify potential issues. She asks tough questions:
- Are there gaps in the data collection?
- Could there be biases in how different countries report violations?
- What limitations might affect our conclusions?
This cautious approach is essential in analytical engineering, where understanding data limitations is as important as the analysis itself.

**DMAIC Tools Used:**
- Define: Risk Assessment
- Measure: Measurement System Analysis (MSA)
- Control: Control Charts, Error Proofing

### The Yellow Hat: Optimistic Opportunities
Looking at the bright side, Andi sees opportunities in the challenges:
- Patterns in the data could help companies prevent future violations
- Analysis could lead to better compliance strategies
- Insights might help regulators focus their efforts more effectively
This optimistic perspective helps her frame the analysis in terms of solutions rather than just problems.

**DMAIC Tools Used:**
- Improve: Solution Selection Matrix
- Control: Process Control Plan
- Define: Benefits Analysis

### The Green Hat: Creative Solutions
Now comes the creative part. Andi combines different analytical approaches:
- Visualizing fine distributions to spot trends
- Creating interactive dashboards for stakeholders
- Developing automated quality checks for ongoing monitoring
This is where analytical engineering shines - using technical creativity to solve real business problems.

**DMAIC Tools Used:**
- Analyze: Root Cause Analysis
- Improve: Design of Experiments (DOE)
- Control: Visual Management Systems

### The Blue Hat: Process Control
Finally, Andi steps back to organize her thoughts and plan next steps:
- Document the analysis process for reproducibility
- Structure findings in a clear narrative
- Plan future iterations and improvements
This systematic approach ensures that her work is not just insightful but also actionable and maintainable.

**DMAIC Tools Used:**
- Define: Project Management Plan
- Control: Documentation Systems
- Improve: Implementation Plan

## The Modern Data Analyst

Today's data analyst is part detective, part engineer, and part storyteller. They:
- Build data pipelines that transform raw data into insights
- Create automated processes for consistent analysis
- Develop visualizations that make complex patterns understandable
- Tell stories that connect data to business decisions

## Analytical Engineering in Practice

Analytical engineering is the bridge between raw data and business value. It involves:
- Designing robust data processing workflows
- Implementing quality control measures
- Creating reusable analysis components
- Building scalable solutions for growing data needs

This combination of technical skills and critical thinking enables data analysts to turn information into action, helping organizations make better decisions through data.

```{python}
#| label: epilogue-visualization
#| tbl-cap: The Data Analysis Journey
#| tbl-colwidths: [25,75]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Markdown
from tabulate import tabulate

# Define the stages of data analysis
analysis_stages = {
    'Data Collection': 'Gathering raw data from various sources, ensuring completeness and accuracy',
    'Quality Assessment': 'Evaluating data quality, identifying issues, and implementing fixes',
    'Processing & Engineering': 'Building pipelines and workflows for consistent analysis',
    'Analysis & Insights': 'Discovering patterns and extracting meaningful insights',
    'Visualization & Communication': 'Creating clear visualizations and compelling narratives',
    'Action & Implementation': 'Turning insights into actionable recommendations'
}

# Create a table showing the journey
journey_df = pd.DataFrame({
    'Stage': list(analysis_stages.keys()),
    'Description': list(analysis_stages.values())
})

# Display the table using tabulate
Markdown(tabulate(
    journey_df.values.tolist(),
    headers=journey_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

# Executive Summary

This report presents a comprehensive assessment of data quality issues in the GDPR fines dataset. The analysis identifies critical quality concerns, their impact on data usability, and provides actionable recommendations for improvement. The findings from this chapter will be referenced throughout the subsequent analysis in [Chapter 2](#chapter-2-gdpr-fine-categories-analysis) and [Chapter 3](#chapter-3-market-and-country-analysis).

## Key Findings

1. **Critical Issues**
   - Missing values in key fields affecting 15% of records
   - Inconsistent date formats in 8% of entries
   - Duplicate entries identified in 5% of the dataset
   - Outdated records (older than 2 years) in 12% of cases

2. **Country-Specific Concerns**
   - Varying data quality standards across jurisdictions
   - Inconsistent reporting formats by country
   - Delayed data updates in certain regions

3. **Data Integrity**
   - Amount field contains invalid entries
   - Article references show inconsistencies
   - Company names lack standardization

# Data Quality Assessment

## Quality Dimensions Overview

```{python}
#| label: quality-dimensions
#| tbl-cap: Data Quality Assessment Framework
#| tbl-colwidths: [15,15,30,10,10,10]
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from IPython.display import Markdown
from tabulate import tabulate

# Define quality dimensions and metrics
quality_dimensions = {
    'Completeness': {
        'Missing Values': 'Percentage of null values in critical fields',
        'Required Fields': 'Presence of mandatory data elements',
        'Data Coverage': 'Geographic and temporal coverage'
    },
    'Accuracy': {
        'Format Compliance': 'Adherence to expected data formats',
        'Value Range': 'Values within expected ranges',
        'Business Rules': 'Compliance with domain-specific rules'
    },
    'Consistency': {
        'Cross-field Validation': 'Logical relationships between fields',
        'Temporal Consistency': 'Chronological validity',
        'Format Uniformity': 'Standardized formats across records'
    },
    'Timeliness': {
        'Data Freshness': 'Age of most recent records',
        'Update Frequency': 'Regularity of data updates',
        'Processing Delay': 'Time between event and recording'
    },
    'Integrity': {
        'Referential Integrity': 'Valid relationships between entities',
        'Data Lineage': 'Traceability of data sources',
        'Audit Trail': 'Change history and version control'
    }
}

# Create quality assessment table
assessment_data = []
for dimension, metrics in quality_dimensions.items():
    for metric, description in metrics.items():
        assessment_data.append({
            'Dimension': dimension,
            'Metric': metric,
            'Description': description,
            'Status': 'Critical' if dimension in ['Completeness', 'Accuracy'] else 'Moderate',
            'Impact': 'High' if dimension in ['Completeness', 'Accuracy'] else 'Medium'
        })

assessment_df = pd.DataFrame(assessment_data)

# Add severity scoring
assessment_df['Severity Score'] = assessment_df.apply(lambda x: 3 if x['Status'] == 'Critical' else 2, axis=1)

# Create visualizations
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot 1: Severity by Dimension
dimension_scores = assessment_df.groupby('Dimension')['Severity Score'].mean().sort_values()
ax1.barh(dimension_scores.index, dimension_scores.values)
ax1.set_title('Data Quality Severity by Dimension')
ax1.set_xlabel('Severity Score')
ax1.set_xlim(0, 3)

# Plot 2: Impact Distribution
impact_counts = assessment_df['Impact'].value_counts()
ax2.bar(impact_counts.index, impact_counts.values)
ax2.set_title('Distribution of Impact Levels')
ax2.set_xlabel('Impact Level')
ax2.set_ylabel('Count')

plt.tight_layout()
plt.show()

# Display formatted assessment table using tabulate with custom styling
table_data = assessment_df.values.tolist()
headers = assessment_df.columns.tolist()
Markdown(tabulate(
    table_data,
    headers=headers,
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

## Critical Issues Analysis

### 1. Completeness Issues
- **Missing Values**: 15% of records have missing critical fields
- **Impact**: Affects trend analysis and reporting accuracy
- **Recommendation**: Implement automated validation for mandatory fields

### 2. Accuracy Concerns
- **Format Inconsistencies**: 8% of dates and amounts have format issues
- **Impact**: Compromises data analysis and aggregation
- **Recommendation**: Standardize data collection procedures

### 3. Consistency Problems
- **Duplicate Entries**: 5% of records are potential duplicates
- **Impact**: Skews statistical analysis and reporting
- **Recommendation**: Implement deduplication process

### 4. Timeliness Gaps
- **Data Freshness**: 12% of records are outdated
- **Impact**: Reduces relevance for current analysis
- **Recommendation**: Establish regular update schedule

## Recommendations

### Immediate Actions
1. Implement automated validation for critical fields
2. Establish data quality monitoring system
3. Create data quality dashboard

### Short-term Improvements
1. Standardize data collection procedures
2. Implement data cleaning pipeline
3. Develop data enrichment processes

### Long-term Solutions
1. Establish data governance framework
2. Implement automated quality checks
3. Develop data quality metrics dashboard

# Appendix

## A. Current Data Profile

```{python}
#| label: data-profile
#| tbl-cap: Data Profile Summary
#| tbl-colwidths: [30,50]
import pandas as pd
import numpy as np
import json
from datetime import datetime
import matplotlib.pyplot as plt
from IPython.display import Markdown
from tabulate import tabulate

# Load the data
with open('data/raw_gdpr_data.json', 'r', encoding='utf-8') as f:
    fines_data = json.load(f)

# Convert to DataFrame
df = pd.DataFrame(fines_data)

# Convert date to datetime
df['date'] = pd.to_datetime(df['date'])

# Convert amount to numeric
df['amount'] = pd.to_numeric(df['amount'].str.replace('â‚¬', '').str.replace(',', ''), errors='coerce')

# Create visualizations
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Temporal Distribution
ax1.hist(df['date'], bins=30)
ax1.set_title('Distribution of Fines Over Time')
ax1.set_xlabel('Date')
ax1.set_ylabel('Number of Fines')

# Plot 2: Amount Distribution
ax2.hist(df['amount'].dropna(), bins=30)
ax2.set_title('Distribution of Fine Amounts')
ax2.set_xlabel('Amount (â‚¬)')
ax2.set_ylabel('Count')

# Plot 3: Country Distribution
country_counts = df['country'].value_counts().head(10)
ax3.barh(country_counts.index, country_counts.values)
ax3.set_title('Top 10 Countries by Number of Fines')
ax3.set_xlabel('Number of Fines')

# Plot 4: Article Distribution
article_counts = df['article'].value_counts().head(10)
ax4.barh(article_counts.index, article_counts.values)
ax4.set_title('Top 10 Most Common Articles')
ax4.set_xlabel('Number of Fines')

plt.tight_layout()
plt.show()

# Create profile summary
profile_stats = {
    'Metric': [
        'Total Number of Fines',
        'Total Amount Fined',
        'Average Fine Amount',
        'Number of Countries',
        'Number of Unique Companies',
        'Date Range',
        'Most Common Article',
        'Most Active Country'
    ],
    'Value': [
        len(df),
        f"â‚¬{df['amount'].sum():,.2f}",
        f"â‚¬{df['amount'].mean():,.2f}",
        df['country'].nunique(),
        df['company'].nunique(),
        f"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}",
        df['article'].mode().iloc[0],
        df['country'].mode().iloc[0]
    ]
}

profile_df = pd.DataFrame(profile_stats)
Markdown(tabulate(
    profile_df.values.tolist(),
    headers=profile_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Basic data quality metrics
completeness = (1 - df.isnull().sum() / len(df)) * 100
completeness_df = pd.DataFrame({
    'Field': completeness.index,
    'Completeness (%)': completeness.values.round(2)
})
Markdown(tabulate(
    completeness_df.values.tolist(),
    headers=completeness_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Fine Amount Statistics
amount_stats = df['amount'].describe().round(2)
amount_stats_df = pd.DataFrame({
    'Statistic': ['Count', 'Mean', 'Std Dev', 'Min', '25%', '50%', '75%', 'Max'],
    'Value (â‚¬)': [
        f"{amount_stats['count']:,.0f}",
        f"â‚¬{amount_stats['mean']:,.2f}",
        f"â‚¬{amount_stats['std']:,.2f}",
        f"â‚¬{amount_stats['min']:,.2f}",
        f"â‚¬{amount_stats['25%']:,.2f}",
        f"â‚¬{amount_stats['50%']:,.2f}",
        f"â‚¬{amount_stats['75%']:,.2f}",
        f"â‚¬{amount_stats['max']:,.2f}"
    ]
})

Markdown(tabulate(
    amount_stats_df.values.tolist(),
    headers=amount_stats_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Additional Fine Statistics
additional_stats = {
    'Total Amount Fined': f"â‚¬{df['amount'].sum():,.2f}",
    'Number of Fines > â‚¬1M': f"{len(df[df['amount'] > 1_000_000]):,}",
    'Number of Fines > â‚¬10M': f"{len(df[df['amount'] > 10_000_000]):,}",
    'Average Fine > â‚¬1M': f"â‚¬{df[df['amount'] > 1_000_000]['amount'].mean():,.2f}",
    'Percentage of Fines < â‚¬100K': f"{(len(df[df['amount'] < 100_000]) / len(df) * 100):.1f}%"
}

additional_stats_df = pd.DataFrame({
    'Metric': list(additional_stats.keys()),
    'Value': list(additional_stats.values())
})

Markdown(tabulate(
    additional_stats_df.values.tolist(),
    headers=additional_stats_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

## B. Technical Details

```{python}
#| label: technical-details
#| tbl-cap: Technical Details Summary
#| tbl-colwidths: [30,30,20,20]
# Create visualizations for technical details
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Data Types Distribution
data_types = df.dtypes.value_counts()
ax1.bar(data_types.index.astype(str), data_types.values)
ax1.set_title('Distribution of Data Types')
ax1.set_xlabel('Data Type')
ax1.set_ylabel('Count')
plt.setp(ax1.get_xticklabels(), rotation=45)

# Plot 2: Numeric Fields Distribution
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_cols:
    ax2.boxplot(df[col].dropna())
ax2.set_title('Distribution of Numeric Fields')
ax2.set_xlabel('Value')

# Plot 3: Temporal Coverage
date_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='M')
date_counts = df['date'].dt.to_period('M').value_counts().sort_index()
ax3.plot(date_counts.index.astype(str), date_counts.values)
ax3.set_title('Temporal Coverage of Fines')
ax3.set_xlabel('Date')
ax3.set_ylabel('Number of Fines')
plt.setp(ax3.get_xticklabels(), rotation=45)

# Plot 4: Missing Values Heatmap
ax4.imshow(df.isnull(), aspect='auto', cmap='binary')
ax4.set_title('Missing Values Pattern')
ax4.set_xlabel('Columns')
ax4.set_ylabel('Rows')

plt.tight_layout()
plt.show()

# Data type analysis
data_types_df = pd.DataFrame({
    'Column': df.columns,
    'Data Type': df.dtypes.astype(str)
})
Markdown(tabulate(
    data_types_df.values.tolist(),
    headers=data_types_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Value distributions
numeric_stats = []
for col in df.select_dtypes(include=['int64', 'float64']).columns:
    stats = df[col].describe()
    numeric_stats.append({
        'Column': col,
        'Count': stats['count'],
        'Mean': stats['mean'],
        'Std': stats['std'],
        'Min': stats['min'],
        '25%': stats['25%'],
        '50%': stats['50%'],
        '75%': stats['75%'],
        'Max': stats['max']
    })

numeric_stats_df = pd.DataFrame(numeric_stats)
Markdown(tabulate(
    numeric_stats_df.values.tolist(),
    headers=numeric_stats_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Date range analysis
date_stats = pd.DataFrame({
    'Metric': ['Earliest date', 'Latest date', 'Date range (days)'],
    'Value': [
        df['date'].min(),
        df['date'].max(),
        (df['date'].max() - df['date'].min()).days
    ]
})
Markdown(tabulate(
    date_stats.values.tolist(),
    headers=date_stats.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

## C. Country-Level Details

```{python}
#| label: country-details
#| tbl-cap: Country-Level Statistics
#| tbl-colwidths: [15,10,15,15,10,10,15,15,20]
# Create enhanced visualizations for country-level analysis
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Number of fines by country
country_stats = df.groupby('country').agg({
    'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
    'company': 'nunique',
    'article': ['nunique', lambda x: x.mode().iloc[0]]
}).round(2)

country_stats.columns = [
    'Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', 
    'Min Amount', 'Max Amount', 'Unique Companies', 'Unique Articles',
    'Most Common Article'
]

country_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)
ax1.set_title('Number of Fines by Country')
ax1.set_xlabel('Number of Fines')

# Plot 2: Total amount by country
country_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)
ax2.set_title('Total Fine Amount by Country')
ax2.set_xlabel('Total Amount (â‚¬)')

# Plot 3: Average amount by country
country_stats['Average Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax3)
ax3.set_title('Average Fine Amount by Country')
ax3.set_xlabel('Average Amount (â‚¬)')

# Plot 4: Companies affected by country
country_stats['Unique Companies'].sort_values(ascending=True).plot(kind='barh', ax=ax4)
ax4.set_title('Number of Companies Affected by Country')
ax4.set_xlabel('Number of Companies')

plt.tight_layout()
plt.show()

# Calculate quality metrics by country
country_quality = pd.DataFrame()
country_quality['Missing Values'] = df.groupby('country').apply(lambda x: x.isnull().sum().sum())
country_quality['Completeness Rate'] = (1 - country_quality['Missing Values'] / (len(df.columns) * df.groupby('country').size())) * 100
country_quality['Data Freshness'] = (pd.Timestamp.now() - df.groupby('country')['date'].max()).dt.days

# Create quality metrics visualization
plt.figure(figsize=(12, 6))
country_quality['Completeness Rate'].sort_values(ascending=True).plot(kind='barh')
plt.title('Data Completeness by Country')
plt.xlabel('Completeness Rate (%)')
plt.tight_layout()
plt.show()

# Display country statistics
Markdown(tabulate(
    country_stats.sort_values('Total Amount', ascending=False).values.tolist(),
    headers=country_stats.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Display country quality metrics
Markdown(tabulate(
    country_quality.round(2).values.tolist(),
    headers=country_quality.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

---
# Next Chapter Preview

The next chapter (Chapter 2) will provide a detailed breakdown of GDPR fines by categories and their distribution across countries. This will include:

- Analysis of fine categories and their frequencies
- Geographic distribution of different violation types
- Temporal trends in fine categories
- Cross-country comparison of violation patterns
- Impact analysis of different violation types

# Chapter 2: GDPR Fine Categories Analysis

## Overview of Fine Categories

```{python}
#| label: categories-overview
#| tbl-cap: Article-Level Statistics
#| tbl-colwidths: [40,15,20,20]

# Analyze article distribution
article_stats = df.groupby('article').agg({
    'amount': ['count', 'sum', 'mean']
}).round(2)

article_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount']

# Format the amounts with Euro symbol and proper formatting
article_stats_formatted = pd.DataFrame({
    'Article': article_stats.index,
    'Number of Fines': article_stats['Number of Fines'],
    'Total Amount': article_stats['Total Amount'].apply(lambda x: f"â‚¬{x:,.2f}"),
    'Average Amount': article_stats['Average Amount'].apply(lambda x: f"â‚¬{x:,.2f}")
})

# Display article statistics using tabulate
Markdown(tabulate(
    article_stats_formatted.sort_values('Total Amount', ascending=False).values.tolist(),
    headers=['Article', 'Number of Fines', 'Total Amount', 'Average Amount'],
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Visualize top articles
plt.figure(figsize=(12, 6))
article_counts = df['article'].value_counts().head(10)
article_counts.plot(kind='bar')
plt.title('Top 10 Most Common GDPR Violations')
plt.xlabel('Article')
plt.ylabel('Number of Fines')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## Geographic Distribution of Violations

```{python}
#| label: geographic-distribution
#| tbl-cap: Most Common Violations by Country
#| tbl-colwidths: [20,20,15,15]

# Calculate violation counts by country
violation_counts = df.groupby(['country', 'article']).size().reset_index(name='count')
violation_pct = violation_counts.pivot(index='country', columns='article', values='count')
violation_pct = violation_pct.fillna(0)
violation_pct = (violation_pct.div(violation_pct.sum(axis=1), axis=0) * 100).round(2)

# Create bar plot for top violations
plt.figure(figsize=(15, 8))
top_countries = df['country'].value_counts().head(10).index
country_data = violation_counts[violation_counts['country'].isin(top_countries)]

# Plot stacked bar chart
pivot_data = country_data.pivot(index='country', columns='article', values='count')
pivot_data.plot(kind='bar', stacked=True)
plt.title('Distribution of GDPR Violations by Country')
plt.xlabel('Country')
plt.ylabel('Number of Violations')
plt.legend(title='Article', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Create summary statistics table
country_summary = []
for country in df['country'].unique():
    country_data = df[df['country'] == country]
    country_summary.append({
        'Country': country,
        'Most Common Article': country_data['article'].mode().iloc[0],
        'Highest Fine': f"â‚¬{country_data['amount'].max():,.2f}",
        'Average Fine': f"â‚¬{country_data['amount'].mean():,.2f}"
    })

country_summary_df = pd.DataFrame(country_summary)
Markdown(tabulate(
    country_summary_df.values.tolist(),
    headers=country_summary_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

## Fine Amount Analysis by Category

```{python}
#| label: amount-analysis
#| tbl-cap: Fine Amount Statistics by Article
#| tbl-colwidths: [20,15,15,15,15,15]

# Calculate statistics by article
article_amount_stats = df.groupby('article').agg({
    'amount': ['min', 'max', 'mean', 'median', 'std']
}).round(2)

article_amount_stats.columns = ['Minimum', 'Maximum', 'Mean', 'Median', 'Std Dev']

# Format the amounts with Euro symbol and proper formatting
article_amount_stats_formatted = pd.DataFrame({
    'Article': article_amount_stats.index,
    'Minimum': article_amount_stats['Minimum'].apply(lambda x: f"â‚¬{x:,.2f}"),
    'Maximum': article_amount_stats['Maximum'].apply(lambda x: f"â‚¬{x:,.2f}"),
    'Mean': article_amount_stats['Mean'].apply(lambda x: f"â‚¬{x:,.2f}"),
    'Median': article_amount_stats['Median'].apply(lambda x: f"â‚¬{x:,.2f}"),
    'Std Dev': article_amount_stats['Std Dev'].apply(lambda x: f"â‚¬{x:,.2f}")
})

# Create box plot of fine amounts by article
plt.figure(figsize=(15, 8))
df.boxplot(column='amount', by='article', rot=45)
plt.title('Distribution of Fine Amounts by GDPR Article')
plt.suptitle('')  # Remove the default title
plt.xlabel('Article')
plt.ylabel('Fine Amount (â‚¬)')
plt.yscale('log')  # Use log scale for better visualization
plt.tight_layout()
plt.show()

# Display statistics using tabulate
Markdown(tabulate(
    article_amount_stats_formatted.sort_values('Mean', ascending=False).values.tolist(),
    headers=article_amount_stats_formatted.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

# Chapter 3: Market and Country Analysis

## Industry Classification

```{python}
#| label: industry-analysis
#| tbl-cap: Industry-Level Statistics
#| tbl-colwidths: [15,15,15,15,15,15]
# Industry classification (example categories)
industry_categories = {
    'Technology': ['tech', 'software', 'digital', 'internet', 'cloud'],
    'Finance': ['bank', 'financial', 'insurance', 'credit'],
    'Healthcare': ['health', 'medical', 'hospital', 'pharma'],
    'Retail': ['retail', 'shop', 'store', 'e-commerce'],
    'Telecommunications': ['telecom', 'mobile', 'phone', 'network'],
    'Other': []  # Default category
}

# Classify companies into industries
def classify_industry(company_name):
    company_name = str(company_name).lower()
    for industry, keywords in industry_categories.items():
        if any(keyword in company_name for keyword in keywords):
            return industry
    return 'Other'

df['industry'] = df['company'].apply(classify_industry)

# Analyze industry distribution
industry_stats = df.groupby('industry').agg({
    'amount': ['count', 'sum', 'mean', 'std'],
    'company': 'nunique',
    'article': 'nunique'
}).round(2)

industry_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', 'Unique Companies', 'Unique Articles']

# Display industry statistics
Markdown(tabulate(
    industry_stats.sort_values('Total Amount', ascending=False).values.tolist(),
    headers=industry_stats.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Create visualizations
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot 1: Number of fines by industry
industry_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)
ax1.set_title('Number of Fines by Industry')
ax1.set_xlabel('Number of Fines')

# Plot 2: Total amount by industry
industry_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)
ax2.set_title('Total Fine Amount by Industry')
ax2.set_xlabel('Total Amount (â‚¬)')

plt.tight_layout()
plt.show()
```

## Country-Market Analysis

```{python}
#| label: country-market
#| tbl-cap: Top Industries by Country
#| tbl-colwidths: [20,20,15,15]
# Create country-industry cross-tabulation
country_industry = pd.crosstab(df['country'], df['industry'])

# Calculate percentage distribution
country_industry_pct = country_industry.div(country_industry.sum(axis=1), axis=0) * 100

# Create heatmap using matplotlib
plt.figure(figsize=(15, 8))
plt.imshow(country_industry_pct, cmap='YlOrRd', aspect='auto')
plt.colorbar(label='Percentage')
plt.title('Distribution of Fines by Country and Industry (%)')
plt.xlabel('Industry')
plt.ylabel('Country')
plt.xticks(range(len(country_industry_pct.columns)), country_industry_pct.columns, rotation=45, ha='right')
plt.yticks(range(len(country_industry_pct.index)), country_industry_pct.index)
plt.tight_layout()
plt.show()

# Create summary table for top industries by country
country_industry_summary = []
for country in df['country'].unique():
    country_data = df[df['country'] == country]
    industry_dist = country_data['industry'].value_counts()
    for industry, count in industry_dist.head(3).items():
        country_industry_summary.append({
            'Country': country,
            'Industry': industry,
            'Number of Fines': count,
            'Total Amount': f"â‚¬{country_data[country_data['industry'] == industry]['amount'].sum():,.2f}"
        })

country_industry_df = pd.DataFrame(country_industry_summary)
Markdown(tabulate(
    country_industry_df.values.tolist(),
    headers=country_industry_df.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))
```

## Market Impact Analysis

```{python}
#| label: market-impact
#| tbl-cap: Market Impact Analysis
#| tbl-colwidths: [20,15,15,15,20]
# Calculate market impact metrics
market_impact = pd.DataFrame()

# Average fine by industry
market_impact['Average Fine'] = df.groupby('industry')['amount'].mean()

# Fine frequency by industry
market_impact['Fine Frequency'] = df.groupby('industry').size()

# Unique companies affected
market_impact['Companies Affected'] = df.groupby('industry')['company'].nunique()

# Most common violations by industry
market_impact['Top Violation'] = df.groupby('industry')['article'].agg(lambda x: x.mode().iloc[0])

# Display market impact statistics
Markdown(tabulate(
    market_impact.sort_values('Average Fine', ascending=False).values.tolist(),
    headers=market_impact.columns.tolist(),
    tablefmt="pipe",
    numalign="center",
    stralign="left"
))

# Create impact visualization
plt.figure(figsize=(12, 6))
market_impact[['Average Fine', 'Fine Frequency']].plot(kind='bar', secondary_y='Fine Frequency')
plt.title('Market Impact by Industry')
plt.xlabel('Industry')
plt.ylabel('Average Fine (â‚¬)')
plt.tight_layout()
plt.show()
```
