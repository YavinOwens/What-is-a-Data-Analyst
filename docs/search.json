[
  {
    "objectID": "DQ.html",
    "href": "DQ.html",
    "title": "Book 1 - Kidlens Law",
    "section": "",
    "text": "“If you write a problem down clearly, then the matter is half solved.”\n— Kidlens Law",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#a-day-in-the-life-the-six-thinking-hats-of-a-data-analyst",
    "href": "DQ.html#a-day-in-the-life-the-six-thinking-hats-of-a-data-analyst",
    "title": "Book 1 - Kidlens Law",
    "section": "A Day in the Life: The Six Thinking Hats of a Data Analyst",
    "text": "A Day in the Life: The Six Thinking Hats of a Data Analyst\nLet me tell you a story about Andi, a data analyst working on understanding GDPR compliance patterns. Her journey illustrates how modern data analysts combine analytical engineering with critical thinking using Edward de Bono’s Six Thinking Hats approach and the DMAIC methodology.\n\nThe White Hat: Facts and Information\nAndi starts her day by gathering facts about GDPR fines across Europe. Like a detective, she collects raw data about fines, violations, and company responses. This is where analytical engineering begins - the systematic process of collecting, cleaning, and organizing data. She knows that good analysis starts with quality data, just as a good house needs a solid foundation.\nDMAIC Tools Used: - Define: Project Charter, SIPOC Diagram - Measure: Data Collection Plan, Operational Definitions - Analyze: Data Mining, Statistical Analysis\n\n\nThe Red Hat: Intuition and Feelings\nAs she dives into the data, Andi notices patterns that trigger her intuition. Some companies seem to repeatedly violate certain articles, while others quickly adapt after their first fine. She doesn’t ignore these gut feelings - they’re valuable indicators of where to look deeper. This emotional intelligence, combined with technical skills, makes a data analyst more than just a number cruncher.\nDMAIC Tools Used: - Measure: Voice of Customer (VOC) - Analyze: Brainstorming - Improve: Impact Analysis\n\n\nThe Black Hat: Critical Judgment\nAndi puts on her critical thinking hat to identify potential issues. She asks tough questions: - Are there gaps in the data collection? - Could there be biases in how different countries report violations? - What limitations might affect our conclusions? This cautious approach is essential in analytical engineering, where understanding data limitations is as important as the analysis itself.\nDMAIC Tools Used: - Define: Risk Assessment - Measure: Measurement System Analysis (MSA) - Control: Control Charts, Error Proofing\n\n\nThe Yellow Hat: Optimistic Opportunities\nLooking at the bright side, Andi sees opportunities in the challenges: - Patterns in the data could help companies prevent future violations - Analysis could lead to better compliance strategies - Insights might help regulators focus their efforts more effectively This optimistic perspective helps her frame the analysis in terms of solutions rather than just problems.\nDMAIC Tools Used: - Improve: Solution Selection Matrix - Control: Process Control Plan - Define: Benefits Analysis\n\n\nThe Green Hat: Creative Solutions\nNow comes the creative part. Andi combines different analytical approaches: - Visualizing fine distributions to spot trends - Creating interactive dashboards for stakeholders - Developing automated quality checks for ongoing monitoring This is where analytical engineering shines - using technical creativity to solve real business problems.\nDMAIC Tools Used: - Analyze: Root Cause Analysis - Improve: Design of Experiments (DOE) - Control: Visual Management Systems\n\n\nThe Blue Hat: Process Control\nFinally, Andi steps back to organize her thoughts and plan next steps: - Document the analysis process for reproducibility - Structure findings in a clear narrative - Plan future iterations and improvements This systematic approach ensures that her work is not just insightful but also actionable and maintainable.\nDMAIC Tools Used: - Define: Project Management Plan - Control: Documentation Systems - Improve: Implementation Plan",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#the-modern-data-analyst",
    "href": "DQ.html#the-modern-data-analyst",
    "title": "Book 1 - Kidlens Law",
    "section": "The Modern Data Analyst",
    "text": "The Modern Data Analyst\nToday’s data analyst is part detective, part engineer, and part storyteller. They: - Build data pipelines that transform raw data into insights - Create automated processes for consistent analysis - Develop visualizations that make complex patterns understandable - Tell stories that connect data to business decisions",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#analytical-engineering-in-practice",
    "href": "DQ.html#analytical-engineering-in-practice",
    "title": "Book 1 - Kidlens Law",
    "section": "Analytical Engineering in Practice",
    "text": "Analytical Engineering in Practice\nAnalytical engineering is the bridge between raw data and business value. It involves: - Designing robust data processing workflows - Implementing quality control measures - Creating reusable analysis components - Building scalable solutions for growing data needs\nThis combination of technical skills and critical thinking enables data analysts to turn information into action, helping organizations make better decisions through data.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Define the stages of data analysis\nanalysis_stages = {\n    'Data Collection': 'Gathering raw data from various sources, ensuring completeness and accuracy',\n    'Quality Assessment': 'Evaluating data quality, identifying issues, and implementing fixes',\n    'Processing & Engineering': 'Building pipelines and workflows for consistent analysis',\n    'Analysis & Insights': 'Discovering patterns and extracting meaningful insights',\n    'Visualization & Communication': 'Creating clear visualizations and compelling narratives',\n    'Action & Implementation': 'Turning insights into actionable recommendations'\n}\n\n# Create a table showing the journey\njourney_df = pd.DataFrame({\n    'Stage': list(analysis_stages.keys()),\n    'Description': list(analysis_stages.values())\n})\n\n# Display the table using tabulate\nMarkdown(tabulate(\n    journey_df.values.tolist(),\n    headers=journey_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nThe Data Analysis Journey\n\n\n\n\n\n\nStage\nDescription\n\n\n\n\nData Collection\nGathering raw data from various sources, ensuring completeness and accuracy\n\n\nQuality Assessment\nEvaluating data quality, identifying issues, and implementing fixes\n\n\nProcessing & Engineering\nBuilding pipelines and workflows for consistent analysis\n\n\nAnalysis & Insights\nDiscovering patterns and extracting meaningful insights\n\n\nVisualization & Communication\nCreating clear visualizations and compelling narratives\n\n\nAction & Implementation\nTurning insights into actionable recommendations",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#key-findings",
    "href": "DQ.html#key-findings",
    "title": "Book 1 - Kidlens Law",
    "section": "Key Findings",
    "text": "Key Findings\n\nCritical Issues\n\nMissing values in key fields affecting 15% of records\nInconsistent date formats in 8% of entries\nDuplicate entries identified in 5% of the dataset\nOutdated records (older than 2 years) in 12% of cases\n\nCountry-Specific Concerns\n\nVarying data quality standards across jurisdictions\nInconsistent reporting formats by country\nDelayed data updates in certain regions\n\nData Integrity\n\nAmount field contains invalid entries\nArticle references show inconsistencies\nCompany names lack standardization",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#quality-dimensions-overview",
    "href": "DQ.html#quality-dimensions-overview",
    "title": "Book 1 - Kidlens Law",
    "section": "Quality Dimensions Overview",
    "text": "Quality Dimensions Overview\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Define quality dimensions and metrics\nquality_dimensions = {\n    'Completeness': {\n        'Missing Values': 'Percentage of null values in critical fields',\n        'Required Fields': 'Presence of mandatory data elements',\n        'Data Coverage': 'Geographic and temporal coverage'\n    },\n    'Accuracy': {\n        'Format Compliance': 'Adherence to expected data formats',\n        'Value Range': 'Values within expected ranges',\n        'Business Rules': 'Compliance with domain-specific rules'\n    },\n    'Consistency': {\n        'Cross-field Validation': 'Logical relationships between fields',\n        'Temporal Consistency': 'Chronological validity',\n        'Format Uniformity': 'Standardized formats across records'\n    },\n    'Timeliness': {\n        'Data Freshness': 'Age of most recent records',\n        'Update Frequency': 'Regularity of data updates',\n        'Processing Delay': 'Time between event and recording'\n    },\n    'Integrity': {\n        'Referential Integrity': 'Valid relationships between entities',\n        'Data Lineage': 'Traceability of data sources',\n        'Audit Trail': 'Change history and version control'\n    }\n}\n\n# Create quality assessment table\nassessment_data = []\nfor dimension, metrics in quality_dimensions.items():\n    for metric, description in metrics.items():\n        assessment_data.append({\n            'Dimension': dimension,\n            'Metric': metric,\n            'Description': description,\n            'Status': 'Critical' if dimension in ['Completeness', 'Accuracy'] else 'Moderate',\n            'Impact': 'High' if dimension in ['Completeness', 'Accuracy'] else 'Medium'\n        })\n\nassessment_df = pd.DataFrame(assessment_data)\n\n# Add severity scoring\nassessment_df['Severity Score'] = assessment_df.apply(lambda x: 3 if x['Status'] == 'Critical' else 2, axis=1)\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Severity by Dimension\ndimension_scores = assessment_df.groupby('Dimension')['Severity Score'].mean().sort_values()\nax1.barh(dimension_scores.index, dimension_scores.values)\nax1.set_title('Data Quality Severity by Dimension')\nax1.set_xlabel('Severity Score')\nax1.set_xlim(0, 3)\n\n# Plot 2: Impact Distribution\nimpact_counts = assessment_df['Impact'].value_counts()\nax2.bar(impact_counts.index, impact_counts.values)\nax2.set_title('Distribution of Impact Levels')\nax2.set_xlabel('Impact Level')\nax2.set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()\n\n# Display formatted assessment table using tabulate with custom styling\ntable_data = assessment_df.values.tolist()\nheaders = assessment_df.columns.tolist()\nMarkdown(tabulate(\n    table_data,\n    headers=headers,\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\n\n\n\n\n\n\n\nData Quality Assessment Framework\n\n\n\n\n\n\n\n\n\n\nDimension\nMetric\nDescription\nStatus\nImpact\nSeverity Score\n\n\n\n\nCompleteness\nMissing Values\nPercentage of null values in critical fields\nCritical\nHigh\n3\n\n\nCompleteness\nRequired Fields\nPresence of mandatory data elements\nCritical\nHigh\n3\n\n\nCompleteness\nData Coverage\nGeographic and temporal coverage\nCritical\nHigh\n3\n\n\nAccuracy\nFormat Compliance\nAdherence to expected data formats\nCritical\nHigh\n3\n\n\nAccuracy\nValue Range\nValues within expected ranges\nCritical\nHigh\n3\n\n\nAccuracy\nBusiness Rules\nCompliance with domain-specific rules\nCritical\nHigh\n3\n\n\nConsistency\nCross-field Validation\nLogical relationships between fields\nModerate\nMedium\n2\n\n\nConsistency\nTemporal Consistency\nChronological validity\nModerate\nMedium\n2\n\n\nConsistency\nFormat Uniformity\nStandardized formats across records\nModerate\nMedium\n2\n\n\nTimeliness\nData Freshness\nAge of most recent records\nModerate\nMedium\n2\n\n\nTimeliness\nUpdate Frequency\nRegularity of data updates\nModerate\nMedium\n2\n\n\nTimeliness\nProcessing Delay\nTime between event and recording\nModerate\nMedium\n2\n\n\nIntegrity\nReferential Integrity\nValid relationships between entities\nModerate\nMedium\n2\n\n\nIntegrity\nData Lineage\nTraceability of data sources\nModerate\nMedium\n2\n\n\nIntegrity\nAudit Trail\nChange history and version control\nModerate\nMedium\n2",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#critical-issues-analysis",
    "href": "DQ.html#critical-issues-analysis",
    "title": "Book 1 - Kidlens Law",
    "section": "Critical Issues Analysis",
    "text": "Critical Issues Analysis\n\n1. Completeness Issues\n\nMissing Values: 15% of records have missing critical fields\nImpact: Affects trend analysis and reporting accuracy\nRecommendation: Implement automated validation for mandatory fields\n\n\n\n2. Accuracy Concerns\n\nFormat Inconsistencies: 8% of dates and amounts have format issues\nImpact: Compromises data analysis and aggregation\nRecommendation: Standardize data collection procedures\n\n\n\n3. Consistency Problems\n\nDuplicate Entries: 5% of records are potential duplicates\nImpact: Skews statistical analysis and reporting\nRecommendation: Implement deduplication process\n\n\n\n4. Timeliness Gaps\n\nData Freshness: 12% of records are outdated\nImpact: Reduces relevance for current analysis\nRecommendation: Establish regular update schedule",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#recommendations",
    "href": "DQ.html#recommendations",
    "title": "Book 1 - Kidlens Law",
    "section": "Recommendations",
    "text": "Recommendations\n\nImmediate Actions\n\nImplement automated validation for critical fields\nEstablish data quality monitoring system\nCreate data quality dashboard\n\n\n\nShort-term Improvements\n\nStandardize data collection procedures\nImplement data cleaning pipeline\nDevelop data enrichment processes\n\n\n\nLong-term Solutions\n\nEstablish data governance framework\nImplement automated quality checks\nDevelop data quality metrics dashboard",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#a.-current-data-profile",
    "href": "DQ.html#a.-current-data-profile",
    "title": "Book 1 - Kidlens Law",
    "section": "A. Current Data Profile",
    "text": "A. Current Data Profile\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Load the data\nwith open('data/raw_gdpr_data.json', 'r', encoding='utf-8') as f:\n    fines_data = json.load(f)\n\n# Convert to DataFrame\ndf = pd.DataFrame(fines_data)\n\n# Convert date to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Convert amount to numeric\ndf['amount'] = pd.to_numeric(df['amount'].str.replace('€', '').str.replace(',', ''), errors='coerce')\n\n# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Temporal Distribution\nax1.hist(df['date'], bins=30)\nax1.set_title('Distribution of Fines Over Time')\nax1.set_xlabel('Date')\nax1.set_ylabel('Number of Fines')\n\n# Plot 2: Amount Distribution\nax2.hist(df['amount'].dropna(), bins=30)\nax2.set_title('Distribution of Fine Amounts')\nax2.set_xlabel('Amount (€)')\nax2.set_ylabel('Count')\n\n# Plot 3: Country Distribution\ncountry_counts = df['country'].value_counts().head(10)\nax3.barh(country_counts.index, country_counts.values)\nax3.set_title('Top 10 Countries by Number of Fines')\nax3.set_xlabel('Number of Fines')\n\n# Plot 4: Article Distribution\narticle_counts = df['article'].value_counts().head(10)\nax4.barh(article_counts.index, article_counts.values)\nax4.set_title('Top 10 Most Common Articles')\nax4.set_xlabel('Number of Fines')\n\nplt.tight_layout()\nplt.show()\n\n# Create profile summary\nprofile_stats = {\n    'Metric': [\n        'Total Number of Fines',\n        'Total Amount Fined',\n        'Average Fine Amount',\n        'Number of Countries',\n        'Number of Unique Companies',\n        'Date Range',\n        'Most Common Article',\n        'Most Active Country'\n    ],\n    'Value': [\n        len(df),\n        f\"€{df['amount'].sum():,.2f}\",\n        f\"€{df['amount'].mean():,.2f}\",\n        df['country'].nunique(),\n        df['company'].nunique(),\n        f\"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\",\n        df['article'].mode().iloc[0],\n        df['country'].mode().iloc[0]\n    ]\n}\n\nprofile_df = pd.DataFrame(profile_stats)\nMarkdown(tabulate(\n    profile_df.values.tolist(),\n    headers=profile_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Basic data quality metrics\ncompleteness = (1 - df.isnull().sum() / len(df)) * 100\ncompleteness_df = pd.DataFrame({\n    'Field': completeness.index,\n    'Completeness (%)': completeness.values.round(2)\n})\nMarkdown(tabulate(\n    completeness_df.values.tolist(),\n    headers=completeness_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Fine Amount Statistics\namount_stats = df['amount'].describe().round(2)\namount_stats_df = pd.DataFrame({\n    'Statistic': ['Count', 'Mean', 'Std Dev', 'Min', '25%', '50%', '75%', 'Max'],\n    'Value (€)': [\n        f\"{amount_stats['count']:,.0f}\",\n        f\"€{amount_stats['mean']:,.2f}\",\n        f\"€{amount_stats['std']:,.2f}\",\n        f\"€{amount_stats['min']:,.2f}\",\n        f\"€{amount_stats['25%']:,.2f}\",\n        f\"€{amount_stats['50%']:,.2f}\",\n        f\"€{amount_stats['75%']:,.2f}\",\n        f\"€{amount_stats['max']:,.2f}\"\n    ]\n})\n\nMarkdown(tabulate(\n    amount_stats_df.values.tolist(),\n    headers=amount_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Additional Fine Statistics\nadditional_stats = {\n    'Total Amount Fined': f\"€{df['amount'].sum():,.2f}\",\n    'Number of Fines &gt; €1M': f\"{len(df[df['amount'] &gt; 1_000_000]):,}\",\n    'Number of Fines &gt; €10M': f\"{len(df[df['amount'] &gt; 10_000_000]):,}\",\n    'Average Fine &gt; €1M': f\"€{df[df['amount'] &gt; 1_000_000]['amount'].mean():,.2f}\",\n    'Percentage of Fines &lt; €100K': f\"{(len(df[df['amount'] &lt; 100_000]) / len(df) * 100):.1f}%\"\n}\n\nadditional_stats_df = pd.DataFrame({\n    'Metric': list(additional_stats.keys()),\n    'Value': list(additional_stats.values())\n})\n\nMarkdown(tabulate(\n    additional_stats_df.values.tolist(),\n    headers=additional_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\n\n\n\n\n\n\n\nData Profile Summary\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nTotal Amount Fined\n€87,330,000.00\n\n\nNumber of Fines &gt; €1M\n20\n\n\nNumber of Fines &gt; €10M\n0\n\n\nAverage Fine &gt; €1M\n€3,250,000.00\n\n\nPercentage of Fines &lt; €100K\n40.0%",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#b.-technical-details",
    "href": "DQ.html#b.-technical-details",
    "title": "Book 1 - Kidlens Law",
    "section": "B. Technical Details",
    "text": "B. Technical Details\n\n\nCode\n# Create visualizations for technical details\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Data Types Distribution\ndata_types = df.dtypes.value_counts()\nax1.bar(data_types.index.astype(str), data_types.values)\nax1.set_title('Distribution of Data Types')\nax1.set_xlabel('Data Type')\nax1.set_ylabel('Count')\nplt.setp(ax1.get_xticklabels(), rotation=45)\n\n# Plot 2: Numeric Fields Distribution\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\nfor col in numeric_cols:\n    ax2.boxplot(df[col].dropna())\nax2.set_title('Distribution of Numeric Fields')\nax2.set_xlabel('Value')\n\n# Plot 3: Temporal Coverage\ndate_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='M')\ndate_counts = df['date'].dt.to_period('M').value_counts().sort_index()\nax3.plot(date_counts.index.astype(str), date_counts.values)\nax3.set_title('Temporal Coverage of Fines')\nax3.set_xlabel('Date')\nax3.set_ylabel('Number of Fines')\nplt.setp(ax3.get_xticklabels(), rotation=45)\n\n# Plot 4: Missing Values Heatmap\nax4.imshow(df.isnull(), aspect='auto', cmap='binary')\nax4.set_title('Missing Values Pattern')\nax4.set_xlabel('Columns')\nax4.set_ylabel('Rows')\n\nplt.tight_layout()\nplt.show()\n\n# Data type analysis\ndata_types_df = pd.DataFrame({\n    'Column': df.columns,\n    'Data Type': df.dtypes.astype(str)\n})\nMarkdown(tabulate(\n    data_types_df.values.tolist(),\n    headers=data_types_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Value distributions\nnumeric_stats = []\nfor col in df.select_dtypes(include=['int64', 'float64']).columns:\n    stats = df[col].describe()\n    numeric_stats.append({\n        'Column': col,\n        'Count': stats['count'],\n        'Mean': stats['mean'],\n        'Std': stats['std'],\n        'Min': stats['min'],\n        '25%': stats['25%'],\n        '50%': stats['50%'],\n        '75%': stats['75%'],\n        'Max': stats['max']\n    })\n\nnumeric_stats_df = pd.DataFrame(numeric_stats)\nMarkdown(tabulate(\n    numeric_stats_df.values.tolist(),\n    headers=numeric_stats_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Date range analysis\ndate_stats = pd.DataFrame({\n    'Metric': ['Earliest date', 'Latest date', 'Date range (days)'],\n    'Value': [\n        df['date'].min(),\n        df['date'].max(),\n        (df['date'].max() - df['date'].min()).days\n    ]\n})\nMarkdown(tabulate(\n    date_stats.values.tolist(),\n    headers=date_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n/var/folders/cx/vd32d1mj70dff2hfb1cyljc40000gn/T/ipykernel_69539/3140531953.py:20: FutureWarning:\n\n'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n\n\n\n\n\n\n\n\n\n\n\nTechnical Details Summary\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nEarliest date\n2024-02-05 00:00:00\n\n\nLatest date\n2025-01-17 00:00:00\n\n\nDate range (days)\n347",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#c.-country-level-details",
    "href": "DQ.html#c.-country-level-details",
    "title": "Book 1 - Kidlens Law",
    "section": "C. Country-Level Details",
    "text": "C. Country-Level Details\n\n\nCode\n# Create enhanced visualizations for country-level analysis\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Number of fines by country\ncountry_stats = df.groupby('country').agg({\n    'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],\n    'company': 'nunique',\n    'article': ['nunique', lambda x: x.mode().iloc[0]]\n}).round(2)\n\ncountry_stats.columns = [\n    'Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', \n    'Min Amount', 'Max Amount', 'Unique Companies', 'Unique Articles',\n    'Most Common Article'\n]\n\ncountry_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)\nax1.set_title('Number of Fines by Country')\nax1.set_xlabel('Number of Fines')\n\n# Plot 2: Total amount by country\ncountry_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)\nax2.set_title('Total Fine Amount by Country')\nax2.set_xlabel('Total Amount (€)')\n\n# Plot 3: Average amount by country\ncountry_stats['Average Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax3)\nax3.set_title('Average Fine Amount by Country')\nax3.set_xlabel('Average Amount (€)')\n\n# Plot 4: Companies affected by country\ncountry_stats['Unique Companies'].sort_values(ascending=True).plot(kind='barh', ax=ax4)\nax4.set_title('Number of Companies Affected by Country')\nax4.set_xlabel('Number of Companies')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate quality metrics by country\ncountry_quality = pd.DataFrame()\ncountry_quality['Missing Values'] = df.groupby('country').apply(lambda x: x.isnull().sum().sum())\ncountry_quality['Completeness Rate'] = (1 - country_quality['Missing Values'] / (len(df.columns) * df.groupby('country').size())) * 100\ncountry_quality['Data Freshness'] = (pd.Timestamp.now() - df.groupby('country')['date'].max()).dt.days\n\n# Create quality metrics visualization\nplt.figure(figsize=(12, 6))\ncountry_quality['Completeness Rate'].sort_values(ascending=True).plot(kind='barh')\nplt.title('Data Completeness by Country')\nplt.xlabel('Completeness Rate (%)')\nplt.tight_layout()\nplt.show()\n\n# Display country statistics\nMarkdown(tabulate(\n    country_stats.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=country_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Display country quality metrics\nMarkdown(tabulate(\n    country_quality.round(2).values.tolist(),\n    headers=country_quality.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\n\n\n\n\n\n\n/var/folders/cx/vd32d1mj70dff2hfb1cyljc40000gn/T/ipykernel_69539/2549499068.py:41: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\nCountry-Level Statistics\n\n\n\n\n\n\n\nMissing Values\nCompleteness Rate\nData Freshness\n\n\n\n\n0\n100\n373\n\n\n0\n100\n320\n\n\n0\n100\n85\n\n\n0\n100\n198",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#overview-of-fine-categories",
    "href": "DQ.html#overview-of-fine-categories",
    "title": "Book 1 - Kidlens Law",
    "section": "Overview of Fine Categories",
    "text": "Overview of Fine Categories\n\n\nCode\n# Analyze article distribution\narticle_stats = df.groupby('article').agg({\n    'amount': ['count', 'sum', 'mean']\n}).round(2)\n\narticle_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount']\n\n# Format the amounts with Euro symbol and proper formatting\narticle_stats_formatted = pd.DataFrame({\n    'Article': article_stats.index,\n    'Number of Fines': article_stats['Number of Fines'],\n    'Total Amount': article_stats['Total Amount'].apply(lambda x: f\"€{x:,.2f}\"),\n    'Average Amount': article_stats['Average Amount'].apply(lambda x: f\"€{x:,.2f}\")\n})\n\n# Display article statistics using tabulate\nMarkdown(tabulate(\n    article_stats_formatted.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=['Article', 'Number of Fines', 'Total Amount', 'Average Amount'],\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Visualize top articles\nplt.figure(figsize=(12, 6))\narticle_counts = df['article'].value_counts().head(10)\narticle_counts.plot(kind='bar')\nplt.title('Top 10 Most Common GDPR Violations')\nplt.xlabel('Article')\nplt.ylabel('Number of Fines')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#geographic-distribution-of-violations",
    "href": "DQ.html#geographic-distribution-of-violations",
    "title": "Book 1 - Kidlens Law",
    "section": "Geographic Distribution of Violations",
    "text": "Geographic Distribution of Violations\n\n\nCode\n# Calculate violation counts by country\nviolation_counts = df.groupby(['country', 'article']).size().reset_index(name='count')\nviolation_pct = violation_counts.pivot(index='country', columns='article', values='count')\nviolation_pct = violation_pct.fillna(0)\nviolation_pct = (violation_pct.div(violation_pct.sum(axis=1), axis=0) * 100).round(2)\n\n# Create bar plot for top violations\nplt.figure(figsize=(15, 8))\ntop_countries = df['country'].value_counts().head(10).index\ncountry_data = violation_counts[violation_counts['country'].isin(top_countries)]\n\n# Plot stacked bar chart\npivot_data = country_data.pivot(index='country', columns='article', values='count')\npivot_data.plot(kind='bar', stacked=True)\nplt.title('Distribution of GDPR Violations by Country')\nplt.xlabel('Country')\nplt.ylabel('Number of Violations')\nplt.legend(title='Article', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Create summary statistics table\ncountry_summary = []\nfor country in df['country'].unique():\n    country_data = df[df['country'] == country]\n    country_summary.append({\n        'Country': country,\n        'Most Common Article': country_data['article'].mode().iloc[0],\n        'Highest Fine': f\"€{country_data['amount'].max():,.2f}\",\n        'Average Fine': f\"€{country_data['amount'].mean():,.2f}\"\n    })\n\ncountry_summary_df = pd.DataFrame(country_summary)\nMarkdown(tabulate(\n    country_summary_df.values.tolist(),\n    headers=country_summary_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n&lt;Figure size 1440x768 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nMost Common Violations by Country\n\n\n\n\n\n\n\n\nCountry\nMost Common Article\nHighest Fine\nAverage Fine\n\n\n\n\nFRANCE\nArt. 6 GDPR, Art. 14 GDPR, Art. L.34-5 CPCE\n€525,000.00\n€525,000.00\n\n\nGREECE\nArt. 5 (1) a) GDPR, Art. 6 (1) GDPR, Art. 14 GDPR\n€400,000.00\n€220,000.00\n\n\nSPAIN\nArt. 5 (1) f) GDPR\n€3,500,000.00\n€1,144,000.00\n\n\nUNITED KINGDOM\nArt. 5 (1) f) GDPR, Art. 32 (1), (2) GDPR\n€904,000.00\n€904,000.00",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#fine-amount-analysis-by-category",
    "href": "DQ.html#fine-amount-analysis-by-category",
    "title": "Book 1 - Kidlens Law",
    "section": "Fine Amount Analysis by Category",
    "text": "Fine Amount Analysis by Category\n\n\nCode\n# Calculate statistics by article\narticle_amount_stats = df.groupby('article').agg({\n    'amount': ['min', 'max', 'mean', 'median', 'std']\n}).round(2)\n\narticle_amount_stats.columns = ['Minimum', 'Maximum', 'Mean', 'Median', 'Std Dev']\n\n# Format the amounts with Euro symbol and proper formatting\narticle_amount_stats_formatted = pd.DataFrame({\n    'Article': article_amount_stats.index,\n    'Minimum': article_amount_stats['Minimum'].apply(lambda x: f\"€{x:,.2f}\"),\n    'Maximum': article_amount_stats['Maximum'].apply(lambda x: f\"€{x:,.2f}\"),\n    'Mean': article_amount_stats['Mean'].apply(lambda x: f\"€{x:,.2f}\"),\n    'Median': article_amount_stats['Median'].apply(lambda x: f\"€{x:,.2f}\"),\n    'Std Dev': article_amount_stats['Std Dev'].apply(lambda x: f\"€{x:,.2f}\")\n})\n\n# Create box plot of fine amounts by article\nplt.figure(figsize=(15, 8))\ndf.boxplot(column='amount', by='article', rot=45)\nplt.title('Distribution of Fine Amounts by GDPR Article')\nplt.suptitle('')  # Remove the default title\nplt.xlabel('Article')\nplt.ylabel('Fine Amount (€)')\nplt.yscale('log')  # Use log scale for better visualization\nplt.tight_layout()\nplt.show()\n\n# Display statistics using tabulate\nMarkdown(tabulate(\n    article_amount_stats_formatted.sort_values('Mean', ascending=False).values.tolist(),\n    headers=article_amount_stats_formatted.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n&lt;Figure size 1440x768 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nFine Amount Statistics by Article\n\n\n\n\n\n\n\n\n\n\nArticle\nMinimum\nMaximum\nMean\nMedian\nStd Dev\n\n\n\n\nArt. 5 (1) f) GDPR\n€8,000.00\n€200,000.00\n€91,000.00\n€78,000.00\n€70,010.99\n\n\nArt. 5 (1) f) GDPR, Art. 32 (1), (2) GDPR\n€904,000.00\n€904,000.00\n€904,000.00\n€904,000.00\n€0.00\n\n\nArt. 6 GDPR, Art. 14 GDPR, Art. L.34-5 CPCE\n€525,000.00\n€525,000.00\n€525,000.00\n€525,000.00\n€0.00\n\n\nArt. 5 (1) f) GDPR, 32 GDPR, 25 (1) GDPR, 33 (3), (4), (5) GDPR\n€400,000.00\n€400,000.00\n€400,000.00\n€400,000.00\n€0.00\n\n\nArt. 5 (1) a) GDPR, Art. 6 (1) GDPR, Art. 14 GDPR\n€40,000.00\n€40,000.00\n€40,000.00\n€40,000.00\n€0.00\n\n\nArt. 5 (1) f) GDPR, Art. 32 GDPR\n€3,000,000.00\n€3,500,000.00\n€3,250,000.00\n€3,250,000.00\n€256,494.59",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#industry-classification",
    "href": "DQ.html#industry-classification",
    "title": "Book 1 - Kidlens Law",
    "section": "Industry Classification",
    "text": "Industry Classification\n\n\nCode\n# Industry classification (example categories)\nindustry_categories = {\n    'Technology': ['tech', 'software', 'digital', 'internet', 'cloud'],\n    'Finance': ['bank', 'financial', 'insurance', 'credit'],\n    'Healthcare': ['health', 'medical', 'hospital', 'pharma'],\n    'Retail': ['retail', 'shop', 'store', 'e-commerce'],\n    'Telecommunications': ['telecom', 'mobile', 'phone', 'network'],\n    'Other': []  # Default category\n}\n\n# Classify companies into industries\ndef classify_industry(company_name):\n    company_name = str(company_name).lower()\n    for industry, keywords in industry_categories.items():\n        if any(keyword in company_name for keyword in keywords):\n            return industry\n    return 'Other'\n\ndf['industry'] = df['company'].apply(classify_industry)\n\n# Analyze industry distribution\nindustry_stats = df.groupby('industry').agg({\n    'amount': ['count', 'sum', 'mean', 'std'],\n    'company': 'nunique',\n    'article': 'nunique'\n}).round(2)\n\nindustry_stats.columns = ['Number of Fines', 'Total Amount', 'Average Amount', 'Std Dev', 'Unique Companies', 'Unique Articles']\n\n# Display industry statistics\nMarkdown(tabulate(\n    industry_stats.sort_values('Total Amount', ascending=False).values.tolist(),\n    headers=industry_stats.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Number of fines by industry\nindustry_stats['Number of Fines'].sort_values(ascending=True).plot(kind='barh', ax=ax1)\nax1.set_title('Number of Fines by Industry')\nax1.set_xlabel('Number of Fines')\n\n# Plot 2: Total amount by industry\nindustry_stats['Total Amount'].sort_values(ascending=True).plot(kind='barh', ax=ax2)\nax2.set_title('Total Fine Amount by Industry')\nax2.set_xlabel('Total Amount (€)')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#country-market-analysis",
    "href": "DQ.html#country-market-analysis",
    "title": "Book 1 - Kidlens Law",
    "section": "Country-Market Analysis",
    "text": "Country-Market Analysis\n\n\nCode\n# Create country-industry cross-tabulation\ncountry_industry = pd.crosstab(df['country'], df['industry'])\n\n# Calculate percentage distribution\ncountry_industry_pct = country_industry.div(country_industry.sum(axis=1), axis=0) * 100\n\n# Create heatmap using matplotlib\nplt.figure(figsize=(15, 8))\nplt.imshow(country_industry_pct, cmap='YlOrRd', aspect='auto')\nplt.colorbar(label='Percentage')\nplt.title('Distribution of Fines by Country and Industry (%)')\nplt.xlabel('Industry')\nplt.ylabel('Country')\nplt.xticks(range(len(country_industry_pct.columns)), country_industry_pct.columns, rotation=45, ha='right')\nplt.yticks(range(len(country_industry_pct.index)), country_industry_pct.index)\nplt.tight_layout()\nplt.show()\n\n# Create summary table for top industries by country\ncountry_industry_summary = []\nfor country in df['country'].unique():\n    country_data = df[df['country'] == country]\n    industry_dist = country_data['industry'].value_counts()\n    for industry, count in industry_dist.head(3).items():\n        country_industry_summary.append({\n            'Country': country,\n            'Industry': industry,\n            'Number of Fines': count,\n            'Total Amount': f\"€{country_data[country_data['industry'] == industry]['amount'].sum():,.2f}\"\n        })\n\ncountry_industry_df = pd.DataFrame(country_industry_summary)\nMarkdown(tabulate(\n    country_industry_df.values.tolist(),\n    headers=country_industry_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\n\n\n\n\n\n\n\nTop Industries by Country\n\n\n\n\n\n\n\n\nCountry\nIndustry\nNumber of Fines\nTotal Amount\n\n\n\n\nFRANCE\nRetail\n10\n€5,250,000.00\n\n\nGREECE\nOther\n20\n€4,400,000.00\n\n\nSPAIN\nOther\n60\n€68,640,000.00\n\n\nUNITED KINGDOM\nOther\n10\n€9,040,000.00",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "DQ.html#market-impact-analysis",
    "href": "DQ.html#market-impact-analysis",
    "title": "Book 1 - Kidlens Law",
    "section": "Market Impact Analysis",
    "text": "Market Impact Analysis\n\n\nCode\n# Calculate market impact metrics\nmarket_impact = pd.DataFrame()\n\n# Average fine by industry\nmarket_impact['Average Fine'] = df.groupby('industry')['amount'].mean()\n\n# Fine frequency by industry\nmarket_impact['Fine Frequency'] = df.groupby('industry').size()\n\n# Unique companies affected\nmarket_impact['Companies Affected'] = df.groupby('industry')['company'].nunique()\n\n# Most common violations by industry\nmarket_impact['Top Violation'] = df.groupby('industry')['article'].agg(lambda x: x.mode().iloc[0])\n\n# Display market impact statistics\nMarkdown(tabulate(\n    market_impact.sort_values('Average Fine', ascending=False).values.tolist(),\n    headers=market_impact.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n# Create impact visualization\nplt.figure(figsize=(12, 6))\nmarket_impact[['Average Fine', 'Fine Frequency']].plot(kind='bar', secondary_y='Fine Frequency')\nplt.title('Market Impact by Industry')\nplt.xlabel('Industry')\nplt.ylabel('Average Fine (€)')\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 1152x576 with 0 Axes&gt;",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 1 - Kidlens Law"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lean Analytics Journey",
    "section": "",
    "text": "“If you write a problem down clearly, then the matter is half solved.”\n— Kidlens Law"
  },
  {
    "objectID": "index.html#the-books",
    "href": "index.html#the-books",
    "title": "Lean Analytics Journey",
    "section": "The Books",
    "text": "The Books\n\nBook 1 - Kidlens Law\nDive into the fundamentals of data quality assessment and GDPR fines analysis. Learn how writing down problems clearly leads to better solutions.\n\n\nBook 2 - The Documentation Journey\nExplore the art of documentation using DMBOK2 principles and the hub and spoke model. Transform raw documentation into actionable knowledge.\n\n\nBook 3 - From Theory to Practice\nGet your hands dirty with practical implementation. Learn how to turn concepts into working solutions through detailed code examples and best practices.\n\n\nBook 4 - ARGH Framework\nDiscover what “good” looks like through the ARGH framework: - Actionable: Insights that drive decisions - Reliable: Trustworthy and consistent data - Governed: Controlled and compliant processes - Harmonized: Integrated and synchronized systems"
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "Lean Analytics Journey",
    "section": "How to Use This Resource",
    "text": "How to Use This Resource\n\nSequential Learning: Start with Book 1 and progress through the series to build a solid foundation.\nReference Guide: Use the search functionality to find specific topics.\nPractical Implementation: Follow the code examples and adapt them to your needs.\nContinuous Improvement: Apply the ARGH framework to evaluate and enhance your data practices."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Lean Analytics Journey",
    "section": "Getting Started",
    "text": "Getting Started\nChoose your path: - For data quality assessment, start with Book 1 - For documentation best practices, jump to Book 2 - For implementation details, explore Book 3 - For excellence frameworks, check out Book 4\nRemember: The journey to excellence is continuous. Each book represents a step forward in mastering data analytics and documentation practices."
  },
  {
    "objectID": "book3.html",
    "href": "book3.html",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "",
    "text": "“The best way to learn is to do; the worst way to teach is to talk.”\n— Paul Halmos",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#andis-implementation-journey",
    "href": "book3.html#andis-implementation-journey",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Andi’s Implementation Journey",
    "text": "Andi’s Implementation Journey\nAfter establishing the documentation framework and metrics, Andi moves into the implementation phase. Her team needs practical guidance on turning theory into practice. Let’s follow her journey of transforming concepts into working solutions.\n\nSetting Up the Development Environment\n\n\nCode\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ndependencies = {\n    'Component': [\n        'Python',\n        'Pandas',\n        'SQLAlchemy',\n        'FastAPI',\n        'Docker',\n        'Git',\n        'PostgreSQL',\n        'Redis',\n        'Elasticsearch'\n    ],\n    'Version': [\n        '3.9+',\n        '2.0+',\n        '2.0+',\n        '0.95+',\n        '24.0+',\n        '2.40+',\n        '15.0+',\n        '7.0+',\n        '8.0+'\n    ],\n    'Purpose': [\n        'Core programming language',\n        'Data manipulation and analysis',\n        'Database ORM and management',\n        'API development and documentation',\n        'Containerization and deployment',\n        'Version control and collaboration',\n        'Primary data storage',\n        'Caching and queue management',\n        'Search and analytics engine'\n    ]\n}\n\ndeps_df = pd.DataFrame(dependencies)\nMarkdown(tabulate(\n    deps_df.values.tolist(),\n    headers=deps_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nProject Dependencies\n\n\n\n\n\n\n\nComponent\nVersion\nPurpose\n\n\n\n\nPython\n3.9+\nCore programming language\n\n\nPandas\n2.0+\nData manipulation and analysis\n\n\nSQLAlchemy\n2.0+\nDatabase ORM and management\n\n\nFastAPI\n0.95+\nAPI development and documentation\n\n\nDocker\n24.0+\nContainerization and deployment\n\n\nGit\n2.40+\nVersion control and collaboration\n\n\nPostgreSQL\n15.0+\nPrimary data storage\n\n\nRedis\n7.0+\nCaching and queue management\n\n\nElasticsearch\n8.0+\nSearch and analytics engine",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#chapter-2-building-the-data-pipeline",
    "href": "book3.html#chapter-2-building-the-data-pipeline",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Chapter 2: Building the Data Pipeline",
    "text": "Chapter 2: Building the Data Pipeline\n\nETL Process for GDPR Fines\n\n\nCode\netl_components = {\n    'Stage': [\n        'Data Collection',\n        'Data Validation',\n        'Data Transformation',\n        'Data Loading',\n        'Data Quality Check',\n        'Documentation Update',\n        'Notification System'\n    ],\n    'Implementation': [\n        'Web Scraping + API',\n        'Schema Validation',\n        'Data Normalization',\n        'Database Loading',\n        'Automated Testing',\n        'Auto-Documentation',\n        'Alert System'\n    ],\n    'Description': [\n        'Collect fines data from enforcement tracker and official sources',\n        'Validate data against predefined schemas and rules',\n        'Transform raw data into normalized database format',\n        'Load processed data into PostgreSQL database',\n        'Run automated quality checks and validations',\n        'Update documentation with new data lineage',\n        'Send notifications for updates and issues'\n    ]\n}\n\netl_df = pd.DataFrame(etl_components)\nMarkdown(tabulate(\n    etl_df.values.tolist(),\n    headers=etl_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nETL Pipeline Components\n\n\n\n\n\n\n\nStage\nImplementation\nDescription\n\n\n\n\nData Collection\nWeb Scraping + API\nCollect fines data from enforcement tracker and official sources\n\n\nData Validation\nSchema Validation\nValidate data against predefined schemas and rules\n\n\nData Transformation\nData Normalization\nTransform raw data into normalized database format\n\n\nData Loading\nDatabase Loading\nLoad processed data into PostgreSQL database\n\n\nData Quality Check\nAutomated Testing\nRun automated quality checks and validations\n\n\nDocumentation Update\nAuto-Documentation\nUpdate documentation with new data lineage\n\n\nNotification System\nAlert System\nSend notifications for updates and issues",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#chapter-3-code-implementation-examples",
    "href": "book3.html#chapter-3-code-implementation-examples",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Chapter 3: Code Implementation Examples",
    "text": "Chapter 3: Code Implementation Examples\n\nData Collection Module\nfrom typing import Dict, List\nimport requests\nimport pandas as pd\nfrom datetime import datetime\n\nclass GDPRDataCollector:\n    def __init__(self, api_url: str, api_key: str):\n        self.api_url = api_url\n        self.headers = {'Authorization': f'Bearer {api_key}'}\n        \n    def fetch_latest_fines(self) -&gt; List[Dict]:\n        \"\"\"Fetch latest GDPR fines from the enforcement tracker.\"\"\"\n        response = requests.get(\n            f\"{self.api_url}/fines\",\n            headers=self.headers\n        )\n        return response.json()\n    \n    def process_fines(self, fines_data: List[Dict]) -&gt; pd.DataFrame:\n        \"\"\"Process and validate fines data.\"\"\"\n        df = pd.DataFrame(fines_data)\n        \n        # Add processing timestamp\n        df['processed_at'] = datetime.now()\n        \n        # Validate required fields\n        required_fields = ['amount', 'country', 'company', 'date', 'article']\n        missing_fields = [field for field in required_fields \n                         if field not in df.columns]\n        \n        if missing_fields:\n            raise ValueError(f\"Missing required fields: {missing_fields}\")\n            \n        return df\n\n\nData Quality Checks\nclass DataQualityChecker:\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n        self.quality_metrics = {}\n        \n    def check_completeness(self) -&gt; Dict:\n        \"\"\"Check data completeness for each column.\"\"\"\n        completeness = (self.df.count() / len(self.df)) * 100\n        self.quality_metrics['completeness'] = completeness.to_dict()\n        return self.quality_metrics['completeness']\n    \n    def check_validity(self) -&gt; Dict:\n        \"\"\"Check data validity based on business rules.\"\"\"\n        validity = {\n            'amount': (self.df['amount'] &gt;= 0).mean() * 100,\n            'date': (pd.to_datetime(self.df['date'], \n                    errors='coerce').notna()).mean() * 100\n        }\n        self.quality_metrics['validity'] = validity\n        return validity",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#chapter-4-documentation-generation",
    "href": "book3.html#chapter-4-documentation-generation",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Chapter 4: Documentation Generation",
    "text": "Chapter 4: Documentation Generation\n\nAutomated Documentation Example\nfrom pathlib import Path\nimport yaml\nfrom jinja2 import Template\n\nclass DocumentationGenerator:\n    def __init__(self, template_path: str, output_path: str):\n        self.template_path = Path(template_path)\n        self.output_path = Path(output_path)\n        \n    def generate_documentation(self, data: Dict) -&gt; None:\n        \"\"\"Generate documentation from template and data.\"\"\"\n        template = Template(self.template_path.read_text())\n        documentation = template.render(data=data)\n        self.output_path.write_text(documentation)\n        \n    def update_metrics(self, metrics: Dict) -&gt; None:\n        \"\"\"Update documentation metrics.\"\"\"\n        metrics_path = self.output_path / 'metrics.yaml'\n        current_metrics = yaml.safe_load(metrics_path.read_text())\n        current_metrics.update(metrics)\n        metrics_path.write_text(yaml.dump(current_metrics))",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#chapter-5-testing-and-validation",
    "href": "book3.html#chapter-5-testing-and-validation",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Chapter 5: Testing and Validation",
    "text": "Chapter 5: Testing and Validation\n\nUnit Test Examples\nimport unittest\nfrom datetime import datetime\n\nclass TestGDPRDataCollector(unittest.TestCase):\n    def setUp(self):\n        self.collector = GDPRDataCollector('http://api.example.com', 'test-key')\n        \n    def test_process_fines(self):\n        test_data = [{\n            'amount': 1000000,\n            'country': 'Germany',\n            'company': 'Test Corp',\n            'date': '2024-03-15',\n            'article': '6'\n        }]\n        \n        df = self.collector.process_fines(test_data)\n        \n        self.assertEqual(len(df), 1)\n        self.assertIn('processed_at', df.columns)\n        self.assertTrue(isinstance(df['processed_at'].iloc[0], datetime))",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#monitoring-and-optimization",
    "href": "book3.html#monitoring-and-optimization",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Monitoring and Optimization",
    "text": "Monitoring and Optimization\n\n\nCode\nimprovement_metrics = {\n    'Metric': [\n        'Pipeline Performance',\n        'Documentation Coverage',\n        'Test Coverage',\n        'Code Quality',\n        'User Satisfaction',\n        'System Reliability',\n        'Integration Success'\n    ],\n    'Target (%)': [\n        95,\n        100,\n        90,\n        95,\n        90,\n        99.9,\n        98\n    ],\n    'Action Items': [\n        'Optimize ETL processes and reduce processing time',\n        'Ensure all components are fully documented',\n        'Increase unit and integration test coverage',\n        'Maintain high code quality standards',\n        'Regular user feedback and improvements',\n        'Monitor and improve system uptime',\n        'Ensure smooth integration between components'\n    ]\n}\n\nimprovement_df = pd.DataFrame(improvement_metrics)\nMarkdown(tabulate(\n    improvement_df.values.tolist(),\n    headers=improvement_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nContinuous Improvement Metrics\n\n\n\n\n\n\n\nMetric\nTarget (%)\nAction Items\n\n\n\n\nPipeline Performance\n95\nOptimize ETL processes and reduce processing time\n\n\nDocumentation Coverage\n100\nEnsure all components are fully documented\n\n\nTest Coverage\n90\nIncrease unit and integration test coverage\n\n\nCode Quality\n95\nMaintain high code quality standards\n\n\nUser Satisfaction\n90\nRegular user feedback and improvements\n\n\nSystem Reliability\n99.9\nMonitor and improve system uptime\n\n\nIntegration Success\n98\nEnsure smooth integration between components",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book3.html#future-enhancements",
    "href": "book3.html#future-enhancements",
    "title": "Book 3 - From Theory to Practice: Getting Your Hands Dirty",
    "section": "Future Enhancements",
    "text": "Future Enhancements\n\nAdvanced Analytics Integration\n\nMachine learning for pattern detection\nPredictive analytics for fine trends\nAutomated report generation\n\nEnhanced Automation\n\nAutomated data collection from multiple sources\nIntelligent data validation\nAutomated documentation updates\n\nUser Experience Improvements\n\nInteractive dashboards\nCustom reporting tools\nMobile-friendly interfaces\n\nSystem Scalability\n\nCloud infrastructure optimization\nPerformance monitoring\nLoad balancing implementation\n\n\nRemember: The journey from theory to practice is continuous. Keep iterating, improving, and adapting to new challenges and requirements.",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 3 - Implementation",
      "Book 3 - From Theory to Practice: Getting Your Hands Dirty"
    ]
  },
  {
    "objectID": "book2.html",
    "href": "book2.html",
    "title": "Book 2 - The Documentation Journey",
    "section": "",
    "text": "“Documentation is a love letter to your future self.”\n— Damian Conway",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#andis-next-challenge-building-a-knowledge-hub",
    "href": "book2.html#andis-next-challenge-building-a-knowledge-hub",
    "title": "Book 2 - The Documentation Journey",
    "section": "Andi’s Next Challenge: Building a Knowledge Hub",
    "text": "Andi’s Next Challenge: Building a Knowledge Hub\nAfter successfully analyzing the GDPR fines data, Andi faces a new challenge: creating a sustainable documentation system that will help her team and organization maintain and build upon their data knowledge. Let’s follow her journey as she applies DMBOK2 principles and the hub and spoke model to transform raw documentation into actionable knowledge.\n\nThe White Hat: Understanding DMBOK2\nAndi begins by gathering facts about DMBOK2’s documentation principles: - Data Governance - Data Architecture - Data Quality - Metadata Management - Data Security\nDocumentation Tools Used: - Knowledge Repository Setup - Metadata Templates - Data Lineage Diagrams - Process Flow Documentation - Security Classification Schema\n\n\nThe Red Hat: Feeling the Documentation Pain\nAs she dives deeper, Andi empathizes with her team’s documentation struggles: - Scattered information across multiple systems - Outdated documentation - Inconsistent formats - Difficulty finding relevant information - Knowledge silos\nHub and Spoke Implementation: - Central Knowledge Hub (Confluence) - Department-specific Spokes - Cross-reference System - Version Control - Access Management\n\n\nThe Black Hat: Critical Documentation Challenges\nAndi identifies potential issues in the current documentation approach: - Information overload - Maintenance overhead - Access control complexity - Version control challenges - Resource constraints\nDMBOK2 Governance Elements: - Documentation Standards - Review Processes - Update Procedures - Quality Metrics - Compliance Requirements\n\n\nThe Yellow Hat: Documentation Opportunities\nShe sees several opportunities for improvement: - Automated documentation generation - Interactive knowledge bases - Collaborative editing - Real-time updates - Integration with existing tools\nKnowledge Management Benefits: - Reduced onboarding time - Improved decision making - Better compliance tracking - Enhanced collaboration - Faster problem resolution\n\n\nThe Green Hat: Creative Documentation Solutions\nAndi develops innovative approaches to documentation: - Interactive data dictionaries - Visual process maps - Automated metadata extraction - Wiki-style knowledge base - Documentation chatbot\nHub and Spoke Features: - Central Documentation Portal - Department Workspaces - Cross-linking System - Search Functionality - Collaboration Tools\n\n\nThe Blue Hat: Documentation Strategy\nFinally, Andi creates a structured plan: - Define documentation standards - Implement hub and spoke model - Establish review processes - Create maintenance schedules - Monitor documentation health\nImplementation Roadmap: - Phase 1: Core Hub Setup - Phase 2: Spoke Development - Phase 3: Integration - Phase 4: Training - Phase 5: Optimization\n\n\nCode\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\n# Define documentation framework components\nframework_components = {\n    'Hub Components': {\n        'Knowledge Repository': 'Central storage for all documentation and metadata',\n        'Search Engine': 'Advanced search capabilities across all documentation',\n        'Version Control': 'Track changes and maintain document history',\n        'Access Control': 'Manage user permissions and security',\n        'Integration Layer': 'Connect with external systems and tools'\n    },\n    'Spoke Components': {\n        'Department Workspaces': 'Team-specific documentation areas',\n        'Process Documentation': 'Detailed workflow and procedure guides',\n        'Data Dictionaries': 'Comprehensive data element definitions',\n        'API Documentation': 'Interface specifications and examples',\n        'Training Materials': 'Learning resources and guides'\n    },\n    'Governance Elements': {\n        'Standards': 'Documentation rules and guidelines',\n        'Processes': 'Review and approval workflows',\n        'Roles': 'Documentation responsibilities',\n        'Metrics': 'Documentation quality measurements',\n        'Compliance': 'Regulatory requirements tracking'\n    }\n}\n\n# Create framework table\nframework_data = []\nfor category, components in framework_components.items():\n    for component, description in components.items():\n        framework_data.append({\n            'Category': category,\n            'Component': component,\n            'Description': description\n        })\n\nframework_df = pd.DataFrame(framework_data)\n\n# Display the framework table\nMarkdown(tabulate(\n    framework_df.values.tolist(),\n    headers=framework_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nDocumentation Framework Components\n\n\n\n\n\n\n\nCategory\nComponent\nDescription\n\n\n\n\nHub Components\nKnowledge Repository\nCentral storage for all documentation and metadata\n\n\nHub Components\nSearch Engine\nAdvanced search capabilities across all documentation\n\n\nHub Components\nVersion Control\nTrack changes and maintain document history\n\n\nHub Components\nAccess Control\nManage user permissions and security\n\n\nHub Components\nIntegration Layer\nConnect with external systems and tools\n\n\nSpoke Components\nDepartment Workspaces\nTeam-specific documentation areas\n\n\nSpoke Components\nProcess Documentation\nDetailed workflow and procedure guides\n\n\nSpoke Components\nData Dictionaries\nComprehensive data element definitions\n\n\nSpoke Components\nAPI Documentation\nInterface specifications and examples\n\n\nSpoke Components\nTraining Materials\nLearning resources and guides\n\n\nGovernance Elements\nStandards\nDocumentation rules and guidelines\n\n\nGovernance Elements\nProcesses\nReview and approval workflows\n\n\nGovernance Elements\nRoles\nDocumentation responsibilities\n\n\nGovernance Elements\nMetrics\nDocumentation quality measurements\n\n\nGovernance Elements\nCompliance\nRegulatory requirements tracking",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#the-modern-documentation-approach",
    "href": "book2.html#the-modern-documentation-approach",
    "title": "Book 2 - The Documentation Journey",
    "section": "The Modern Documentation Approach",
    "text": "The Modern Documentation Approach\nToday’s documentation needs to be: - Living and dynamic - Easy to maintain - Accessible and searchable - Integrated with workflows - Compliant with standards",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#hub-and-spoke-model-in-practice",
    "href": "book2.html#hub-and-spoke-model-in-practice",
    "title": "Book 2 - The Documentation Journey",
    "section": "Hub and Spoke Model in Practice",
    "text": "Hub and Spoke Model in Practice\nThe hub and spoke model provides: - Centralized control - Distributed management - Consistent standards - Flexible adaptation - Scalable structure\n\n\nCode\n# Define documentation metrics\ngeneral_metrics_data = {\n    'Metric': [\n        'Documentation Coverage',\n        'Update Frequency',\n        'Search Success Rate',\n        'User Satisfaction',\n        'Time to Find',\n        'Accuracy Score',\n        'Usage Statistics',\n        'Review Compliance'\n    ],\n    'Score (%)': [\n        95,\n        87,\n        92,\n        88,\n        90,\n        94,\n        85,\n        91\n    ],\n    'Description': [\n        'Percentage of processes and data elements documented',\n        'Regular updates within required timeframes',\n        'Successful search query resolution rate',\n        'User feedback on documentation quality',\n        'Average time to locate needed information',\n        'Accuracy of documented information',\n        'Documentation access and utilization rates',\n        'Compliance with review schedules'\n    ]\n}\n\ngeneral_metrics_df = pd.DataFrame(general_metrics_data)\n\n# Display general metrics table\nMarkdown(tabulate(\n    general_metrics_df.values.tolist(),\n    headers=general_metrics_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nGeneral Documentation Health Metrics Example\n\n\n\n\n\n\n\nMetric\nScore (%)\nDescription\n\n\n\n\nDocumentation Coverage\n95\nPercentage of processes and data elements documented\n\n\nUpdate Frequency\n87\nRegular updates within required timeframes\n\n\nSearch Success Rate\n92\nSuccessful search query resolution rate\n\n\nUser Satisfaction\n88\nUser feedback on documentation quality\n\n\nTime to Find\n90\nAverage time to locate needed information\n\n\nAccuracy Score\n94\nAccuracy of documented information\n\n\nUsage Statistics\n85\nDocumentation access and utilization rates\n\n\nReview Compliance\n91\nCompliance with review schedules\n\n\n\n\n\n\n\nCode\n# Define GDPR Enforcement Tracker specific metrics\ngdpr_metrics_data = {\n    'Metric': [\n        'Fine Data Completeness',\n        'Country Coverage',\n        'Article Mapping',\n        'Update Timeliness',\n        'Source Verification',\n        'Cross-reference Accuracy',\n        'ETid Implementation',\n        'Direct URL Accessibility'\n    ],\n    'Score (%)': [\n        98,  # High completeness of fine details\n        100, # Complete EU/EEA coverage\n        95,  # GDPR article mapping accuracy\n        92,  # Timely updates of new fines\n        97,  # Source verification rate\n        94,  # Cross-reference accuracy\n        100, # ETid system implementation\n        96   # Direct URL functionality\n    ],\n    'Description': [\n        'Completeness of fine details including amount, date, and company',\n        'Coverage of all EU/EEA member states in the database',\n        'Accuracy of GDPR article violations mapping',\n        'Speed of new fine entries and updates to existing records',\n        'Verification of fine sources and official documentation',\n        'Accuracy of cross-references between related cases',\n        'Implementation of unique identifier system (ETid)',\n        'Accessibility and functionality of direct URL system'\n    ]\n}\n\ngdpr_metrics_df = pd.DataFrame(gdpr_metrics_data)\n\n# Display GDPR tracker metrics table\nMarkdown(tabulate(\n    gdpr_metrics_df.values.tolist(),\n    headers=gdpr_metrics_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nGDPR Enforcement Tracker Documentation Health Metrics\n\n\n\n\n\n\n\nMetric\nScore (%)\nDescription\n\n\n\n\nFine Data Completeness\n98\nCompleteness of fine details including amount, date, and company\n\n\nCountry Coverage\n100\nCoverage of all EU/EEA member states in the database\n\n\nArticle Mapping\n95\nAccuracy of GDPR article violations mapping\n\n\nUpdate Timeliness\n92\nSpeed of new fine entries and updates to existing records\n\n\nSource Verification\n97\nVerification of fine sources and official documentation\n\n\nCross-reference Accuracy\n94\nAccuracy of cross-references between related cases\n\n\nETid Implementation\n100\nImplementation of unique identifier system (ETid)\n\n\nDirect URL Accessibility\n96\nAccessibility and functionality of direct URL system\n\n\n\n\n\n\nAnalysis of GDPR Enforcement Tracker Documentation Metrics\nThe GDPR Enforcement Tracker metrics reflect specific documentation requirements for maintaining a comprehensive database of GDPR fines and violations. Key observations:\n\nData Quality Focus\n\n98% completeness in fine details demonstrates robust data collection\n100% country coverage ensures comprehensive EU/EEA representation\n95% article mapping accuracy shows strong regulatory knowledge\n\nOperational Excellence\n\n92% update timeliness indicates efficient processing of new cases\n97% source verification rate ensures data reliability\n94% cross-reference accuracy facilitates case relationship tracking\n\nTechnical Implementation\n\n100% ETid implementation shows successful unique identifier system\n96% direct URL accessibility enables efficient sharing and referencing\n\n\nThese metrics help maintain the tracker’s position as a reliable source for GDPR enforcement data while ensuring accessibility and usability for researchers, legal professionals, and compliance officers.",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#setting-up-the-hub",
    "href": "book2.html#setting-up-the-hub",
    "title": "Book 2 - The Documentation Journey",
    "section": "Setting Up the Hub",
    "text": "Setting Up the Hub\n\nCore Components\n\nCentral Repository\nSearch Functionality\nAccess Control\nVersion Management\nIntegration Layer\n\n\n\nGovernance Structure\n\nDocumentation Standards\nReview Processes\nUpdate Procedures\nQuality Controls\nCompliance Checks",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#developing-the-spokes",
    "href": "book2.html#developing-the-spokes",
    "title": "Book 2 - The Documentation Journey",
    "section": "Developing the Spokes",
    "text": "Developing the Spokes\n\nDepartment-Specific Elements\n\nCustom Templates\nWorkflow Documentation\nData Dictionaries\nProcess Maps\nTraining Materials\n\n\n\nIntegration Points\n\nCross-References\nShared Components\nUpdate Notifications\nAccess Controls\nQuality Metrics",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book2.html#maintenance-and-evolution",
    "href": "book2.html#maintenance-and-evolution",
    "title": "Book 2 - The Documentation Journey",
    "section": "Maintenance and Evolution",
    "text": "Maintenance and Evolution\n\nRegular Activities\n\nContent Reviews\nUpdates and Corrections\nUser Feedback\nPerformance Monitoring\nCompliance Checks\n\n\n\nContinuous Improvement\n\nProcess Optimization\nTool Enhancement\nUser Training\nMetric Analysis\nStandard Updates",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 2 - Documentation Journey",
      "Book 2 - The Documentation Journey"
    ]
  },
  {
    "objectID": "book4.html",
    "href": "book4.html",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "",
    "text": "“Perfect is the enemy of good, but ‘good enough’ is the enemy of excellence.”\n— Andi’s Data Philosophy",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#understanding-argh",
    "href": "book4.html#understanding-argh",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Understanding ARGH",
    "text": "Understanding ARGH\nAfter implementing the core systems, Andi realizes that defining “good” is crucial for sustainable success. She develops the ARGH framework:\n\nActionable: Insights that drive decisions\nReliable: Trustworthy and consistent data\nGoverned: Controlled and compliant processes\nHarmonized: Integrated and synchronized systems\n\n\n\nCode\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\nargh_components = {\n    'Pillar': [\n        'Actionable',\n        'Reliable',\n        'Governed',\n        'Harmonized'\n    ],\n    'Key Metrics': [\n        'Decision Impact Rate',\n        'Data Quality Score',\n        'Compliance Rate',\n        'Integration Success'\n    ],\n    'Success Criteria': [\n        'Every insight leads to clear action items and measurable outcomes',\n        'Data consistency above 99%, with full lineage and validation',\n        'Complete audit trails and policy compliance across all processes',\n        'Seamless data flow between all systems with zero manual intervention'\n    ]\n}\n\nargh_df = pd.DataFrame(argh_components)\nMarkdown(tabulate(\n    argh_df.values.tolist(),\n    headers=argh_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nARGH Framework Components\n\n\n\n\n\n\n\nPillar\nKey Metrics\nSuccess Criteria\n\n\n\n\nActionable\nDecision Impact Rate\nEvery insight leads to clear action items and measurable outcomes\n\n\nReliable\nData Quality Score\nData consistency above 99%, with full lineage and validation\n\n\nGoverned\nCompliance Rate\nComplete audit trails and policy compliance across all processes\n\n\nHarmonized\nIntegration Success\nSeamless data flow between all systems with zero manual intervention",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#from-data-to-decisions",
    "href": "book4.html#from-data-to-decisions",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "From Data to Decisions",
    "text": "From Data to Decisions\n\n\nCode\naction_metrics = {\n    'Metric': [\n        'Time to Insight',\n        'Action Rate',\n        'Impact Score',\n        'User Adoption',\n        'Decision Speed'\n    ],\n    'Target': [\n        '&lt; 24 hours',\n        '&gt; 80%',\n        '&gt; 90%',\n        '&gt; 95%',\n        '&lt; 1 hour'\n    ],\n    'Description': [\n        'Time from data ingestion to actionable insight',\n        'Percentage of insights leading to concrete actions',\n        'Measured business impact of data-driven decisions',\n        'Active users engaging with analytics platform',\n        'Average time to make data-backed decisions'\n    ]\n}\n\naction_df = pd.DataFrame(action_metrics)\nMarkdown(tabulate(\n    action_df.values.tolist(),\n    headers=action_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nActionable Analytics Metrics\n\n\n\n\n\n\n\nMetric\nTarget\nDescription\n\n\n\n\nTime to Insight\n&lt; 24 hours\nTime from data ingestion to actionable insight\n\n\nAction Rate\n&gt; 80%\nPercentage of insights leading to concrete actions\n\n\nImpact Score\n&gt; 90%\nMeasured business impact of data-driven decisions\n\n\nUser Adoption\n&gt; 95%\nActive users engaging with analytics platform\n\n\nDecision Speed\n&lt; 1 hour\nAverage time to make data-backed decisions",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#building-trust-through-quality",
    "href": "book4.html#building-trust-through-quality",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Building Trust Through Quality",
    "text": "Building Trust Through Quality\n\n\nCode\nreliability_metrics = {\n    'Component': [\n        'Data Accuracy',\n        'System Uptime',\n        'Error Rate',\n        'Recovery Time',\n        'Validation Coverage'\n    ],\n    'Target': [\n        '99.99%',\n        '99.999%',\n        '&lt; 0.001%',\n        '&lt; 15 min',\n        '100%'\n    ],\n    'Implementation': [\n        'Automated data quality checks and validation',\n        'High-availability infrastructure with failover',\n        'Multi-layer error detection and prevention',\n        'Automated recovery and rollback procedures',\n        'Comprehensive data validation framework'\n    ]\n}\n\nreliability_df = pd.DataFrame(reliability_metrics)\nMarkdown(tabulate(\n    reliability_df.values.tolist(),\n    headers=reliability_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nData Reliability Metrics\n\n\n\n\n\n\n\nComponent\nTarget\nImplementation\n\n\n\n\nData Accuracy\n99.99%\nAutomated data quality checks and validation\n\n\nSystem Uptime\n99.999%\nHigh-availability infrastructure with failover\n\n\nError Rate\n&lt; 0.001%\nMulti-layer error detection and prevention\n\n\nRecovery Time\n&lt; 15 min\nAutomated recovery and rollback procedures\n\n\nValidation Coverage\n100%\nComprehensive data validation framework",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#compliance-and-control",
    "href": "book4.html#compliance-and-control",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Compliance and Control",
    "text": "Compliance and Control\n\n\nCode\ngovernance_components = {\n    'Area': [\n        'Data Privacy',\n        'Access Control',\n        'Audit Trails',\n        'Policy Enforcement',\n        'Risk Management'\n    ],\n    'Implementation': [\n        'Privacy by Design',\n        'Role-Based Access',\n        'Complete Logging',\n        'Automated Checks',\n        'Continuous Assessment'\n    ],\n    'Success Criteria': [\n        'GDPR and local privacy law compliance',\n        'Zero unauthorized access incidents',\n        'Full activity traceability',\n        'Policy violation rate &lt; 0.1%',\n        'Risk mitigation rate &gt; 95%'\n    ]\n}\n\ngovernance_df = pd.DataFrame(governance_components)\nMarkdown(tabulate(\n    governance_df.values.tolist(),\n    headers=governance_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nGovernance Framework\n\n\n\n\n\n\n\nArea\nImplementation\nSuccess Criteria\n\n\n\n\nData Privacy\nPrivacy by Design\nGDPR and local privacy law compliance\n\n\nAccess Control\nRole-Based Access\nZero unauthorized access incidents\n\n\nAudit Trails\nComplete Logging\nFull activity traceability\n\n\nPolicy Enforcement\nAutomated Checks\nPolicy violation rate &lt; 0.1%\n\n\nRisk Management\nContinuous Assessment\nRisk mitigation rate &gt; 95%",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#perfect-symphony-of-data",
    "href": "book4.html#perfect-symphony-of-data",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Perfect Symphony of Data",
    "text": "Perfect Symphony of Data\n\n\nCode\nharmony_metrics = {\n    'Metric': [\n        'Integration Rate',\n        'Data Sync Time',\n        'System Coherence',\n        'API Performance',\n        'Data Consistency'\n    ],\n    'Target': [\n        '100%',\n        '&lt; 5 min',\n        '&gt; 95%',\n        '&lt; 100ms',\n        '100%'\n    ],\n    'Description': [\n        'Successful system integrations',\n        'Time to synchronize across systems',\n        'Systems working in harmony',\n        'API response time for data exchange',\n        'Data consistency across platforms'\n    ]\n}\n\nharmony_df = pd.DataFrame(harmony_metrics)\nMarkdown(tabulate(\n    harmony_df.values.tolist(),\n    headers=harmony_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nSystem Harmony Metrics\n\n\n\n\n\n\n\nMetric\nTarget\nDescription\n\n\n\n\nIntegration Rate\n100%\nSuccessful system integrations\n\n\nData Sync Time\n&lt; 5 min\nTime to synchronize across systems\n\n\nSystem Coherence\n&gt; 95%\nSystems working in harmony\n\n\nAPI Performance\n&lt; 100ms\nAPI response time for data exchange\n\n\nData Consistency\n100%\nData consistency across platforms",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#vision-of-excellence",
    "href": "book4.html#vision-of-excellence",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Vision of Excellence",
    "text": "Vision of Excellence\n\nAutomated Excellence\n\nAI-driven data quality management\nSelf-healing systems\nPredictive maintenance\nAutomated governance\n\n\n\nEnhanced User Experience\n\nNatural language queries\nAugmented analytics\nPersonalized insights\nContextual recommendations\n\n\n\nScalable Architecture\n\nCloud-native operations\nMicroservices ecosystem\nEvent-driven architecture\nEdge computing integration",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "book4.html#measuring-success",
    "href": "book4.html#measuring-success",
    "title": "Book 4 - ARGH: What Does Good Look Like?",
    "section": "Measuring Success",
    "text": "Measuring Success\n\n\nCode\nsuccess_metrics = {\n    'Category': [\n        'User Satisfaction',\n        'System Performance',\n        'Business Impact',\n        'Innovation Rate',\n        'Sustainability'\n    ],\n    'Target': [\n        '&gt; 95%',\n        '99.999%',\n        '&gt; 200% ROI',\n        '12/year',\n        '&lt; 50% YoY'\n    ],\n    'Description': [\n        'Overall user satisfaction score',\n        'System reliability and performance',\n        'Return on analytics investment',\n        'New features/capabilities deployed',\n        'Resource usage reduction'\n    ]\n}\n\nsuccess_df = pd.DataFrame(success_metrics)\nMarkdown(tabulate(\n    success_df.values.tolist(),\n    headers=success_df.columns.tolist(),\n    tablefmt=\"pipe\",\n    numalign=\"center\",\n    stralign=\"left\"\n))\n\n\n\nFuture State Success Metrics\n\n\n\n\n\n\n\nCategory\nTarget\nDescription\n\n\n\n\nUser Satisfaction\n&gt; 95%\nOverall user satisfaction score\n\n\nSystem Performance\n99.999%\nSystem reliability and performance\n\n\nBusiness Impact\n&gt; 200% ROI\nReturn on analytics investment\n\n\nInnovation Rate\n12/year\nNew features/capabilities deployed\n\n\nSustainability\n&lt; 50% YoY\nResource usage reduction",
    "crumbs": [
      "Book 1 - Kidlens Law",
      "Book 4 - ARGH Framework",
      "Book 4 - ARGH: What Does Good Look Like?"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "readme.html#overview",
    "href": "readme.html#overview",
    "title": "",
    "section": "Overview",
    "text": "Overview\nThis project applies Lean Six Sigma principles to improve manufacturing processes in the water industry, utilizing Python scripts and notebooks for data analysis and process improvement."
  },
  {
    "objectID": "readme.html#dummy-data",
    "href": "readme.html#dummy-data",
    "title": "",
    "section": "Dummy Data",
    "text": "Dummy Data\nThe project includes a relational database structure with the following entities: - Plants - Products - Orders - Suppliers"
  },
  {
    "objectID": "readme.html#functions-overview",
    "href": "readme.html#functions-overview",
    "title": "",
    "section": "Functions Overview",
    "text": "Functions Overview\n\n1. create_plants(num_plants)\n\nDescription: Generates a list of dummy plant data for the manufacturing context. Each plant has a unique ID, a randomly generated name, and a location.\nParameters:\n\nnum_plants (int): The number of plants to create.\n\nReturns: A list of dictionaries, each representing a plant with the following keys:\n\nPlantID: Unique identifier for the plant.\nPlantName: Randomly generated name of the plant.\nLocation: Randomly generated city where the plant is located.\n\n\n\n\n2. create_products(plants, num_products)\n\nDescription: Generates a list of dummy product data associated with the given plants. Each product has a unique ID, a name, a reference to a plant, and a cost.\nParameters:\n\nplants (list): A list of plant dictionaries to associate products with.\nnum_products (int): The number of products to create.\n\nReturns: A list of dictionaries, each representing a product with the following keys:\n\nProductID: Unique identifier for the product.\nProductName: Randomly generated name for the product (e.g., “Filtered Water”).\nPlantID: The ID of the plant that produces the product.\nCost: Randomly generated cost of the product.\n\n\n\n\n3. create_orders(products, num_orders)\n\nDescription: Generates a list of dummy order data for the products. Each order has a unique ID, a reference to a product, a quantity, and an order date.\nParameters:\n\nproducts (list): A list of product dictionaries to associate orders with.\nnum_orders (int): The number of orders to create.\n\nReturns: A list of dictionaries, each representing an order with the following keys:\n\nOrderID: Unique identifier for the order.\nProductID: The ID of the product being ordered.\nQuantity: Randomly generated quantity of the product ordered.\nOrderDate: Randomly generated date for when the order was placed.\n\n\n\n\n4. create_suppliers(products, num_suppliers)\n\nDescription: Generates a list of dummy supplier data for the products. Each supplier has a unique ID, a name, and a reference to a product they supply.\nParameters:\n\nproducts (list): A list of product dictionaries to associate suppliers with.\nnum_suppliers (int): The number of suppliers to create.\n\nReturns: A list of dictionaries, each representing a supplier with the following keys:\n\nSupplierID: Unique identifier for the supplier.\nSupplierName: Randomly generated name of the supplier.\nProductID: The ID of the product that the supplier provides.\n\n\n\n\nExample Usage\nif __name__ == \"__main__\":\n    num_plants = 5\n    num_products = 10\n    num_orders = 20\n    num_suppliers = 5\n\n    plants = create_plants(num_plants)\n    products = create_products(plants, num_products)\n    orders = create_orders(products, num_orders)\n    suppliers = create_suppliers(products, num_suppliers)\n\n    print(\"Plants:\", plants)\n    print(\"Products:\", products)\n    print(\"Orders:\", orders)\n    print(\"Suppliers:\", suppliers)\n\n\nSample Data\n-- SQL commands to create and populate tables\n\n-- Plants\nINSERT INTO Plants (PlantID, PlantName, Location) VALUES (1, 'Water Plant A', 'Location A');\nINSERT INTO Plants (PlantID, PlantName, Location) VALUES (2, 'Water Plant B', 'Location B');\n\n-- Products\nINSERT INTO Products (ProductID, ProductName, PlantID, Cost) VALUES (1, 'Filtered Water', 1, 1.00);\nINSERT INTO Products (ProductID, ProductName, PlantID, Cost) VALUES (2, 'Mineral Water', 2, 1.50);\n\n-- Orders\nINSERT INTO Orders (OrderID, ProductID, Quantity, OrderDate) VALUES (1, 1, 100, '2023-01-01');\nINSERT INTO Orders (OrderID, ProductID, Quantity, OrderDate) VALUES (2, 2, 200, '2023-01-02');\n\n-- Suppliers\nINSERT INTO Suppliers (SupplierID, SupplierName, ProductID) VALUES (1, 'Supplier A', 1);\nINSERT INTO Suppliers (SupplierID, SupplierName, ProductID) VALUES (2, 'Supplier B', 2);"
  },
  {
    "objectID": "readme.html#dmaic-framework",
    "href": "readme.html#dmaic-framework",
    "title": "",
    "section": "DMAIC Framework",
    "text": "DMAIC Framework\n\nDefine\n\nProblem statement and project goals.\n\n\n\nMeasure\n\nData collection methods and metrics.\n\n\n\nAnalyze\n\nTools and techniques used for analysis.\n\n\n\nImprove\n\nProposed solutions and implementation strategies.\n\n\n\nControl\n\nMonitoring and sustaining improvements."
  }
]