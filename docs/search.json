[
  {
    "objectID": "AndiStory.html",
    "href": "AndiStory.html",
    "title": "Andi’s Data Journey",
    "section": "",
    "text": "“Learn as if you will live forever, earn as if you will die tomorrow, return as if this is your legacy.”\n— Stoic Philosophy\n\nThis is the story of Andi, a data analyst who embarks on a journey to transform raw data into meaningful insights. Through her experiences, we’ll explore the principles of data quality, documentation, implementation, and excellence in data analytics."
  },
  {
    "objectID": "AndiStory.html#the-art-and-science-of-data-analysis",
    "href": "AndiStory.html#the-art-and-science-of-data-analysis",
    "title": "Andi’s Data Journey",
    "section": "The Art and Science of Data Analysis",
    "text": "The Art and Science of Data Analysis\n\nA Day in the Life: The Six Thinking Hats of a Data Analyst\nLet me tell you a story about Andi, a data analyst working on understanding GDPR compliance patterns. Her journey illustrates how modern data analysts combine analytical engineering with critical thinking using Edward de Bono’s Six Thinking Hats approach and the DMAIC methodology.\n\n\nThe White Hat: Facts and Information\nAndi starts her day by gathering facts about GDPR fines across Europe. Like a detective, she collects raw data about fines, violations, and company responses. This is where analytical engineering begins - the systematic process of collecting, cleaning, and organizing data. She knows that good analysis starts with quality data, just as a good house needs a solid foundation.\nDMAIC Tools Used: - Define: Project Charter, SIPOC Diagram - Measure: Data Collection Plan, Operational Definitions - Analyze: Data Mining, Statistical Analysis\n\n\nThe Red Hat: Intuition and Feelings\nAs she dives into the data, Andi notices patterns that trigger her intuition. Some companies seem to repeatedly violate certain articles, while others quickly adapt after their first fine. She doesn’t ignore these gut feelings - they’re valuable indicators of where to look deeper. This emotional intelligence, combined with technical skills, makes a data analyst more than just a number cruncher.\nDMAIC Tools Used: - Measure: Voice of Customer (VOC) - Analyze: Brainstorming - Improve: Impact Analysis\n\n\nThe Black Hat: Critical Judgment\nAndi puts on her critical thinking hat to identify potential issues. She asks tough questions: - Are there gaps in the data collection? - Could there be biases in how different countries report violations? - What limitations might affect our conclusions? This cautious approach is essential in analytical engineering, where understanding data limitations is as important as the analysis itself.\nDMAIC Tools Used: - Define: Risk Assessment - Measure: Measurement System Analysis (MSA) - Control: Control Charts, Error Proofing\n\n\nThe Yellow Hat: Optimistic Opportunities\nLooking at the bright side, Andi sees opportunities in the challenges: - Patterns in the data could help companies prevent future violations - Analysis could lead to better compliance strategies - Insights might help regulators focus their efforts more effectively This optimistic perspective helps her frame the analysis in terms of solutions rather than just problems.\nDMAIC Tools Used: - Improve: Solution Selection Matrix - Control: Process Control Plan - Define: Benefits Analysis\n\n\nThe Green Hat: Creative Solutions\nNow comes the creative part. Andi combines different analytical approaches: - Visualizing fine distributions to spot trends - Creating interactive dashboards for stakeholders - Developing automated quality checks for ongoing monitoring This is where analytical engineering shines - using technical creativity to solve real business problems.\nDMAIC Tools Used: - Analyze: Root Cause Analysis - Improve: Design of Experiments (DOE) - Control: Visual Management Systems\n\n\nThe Blue Hat: Process Control\nFinally, Andi steps back to organize her thoughts and plan next steps: - Document the analysis process for reproducibility - Structure findings in a clear narrative - Plan future iterations and improvements This systematic approach ensures that her work is not just insightful but also actionable and maintainable.\nDMAIC Tools Used: - Define: Project Management Plan - Control: Documentation Systems - Improve: Implementation Plan"
  },
  {
    "objectID": "AndiStory.html#the-modern-data-analyst",
    "href": "AndiStory.html#the-modern-data-analyst",
    "title": "Andi’s Data Journey",
    "section": "The Modern Data Analyst",
    "text": "The Modern Data Analyst\nToday’s data analyst is part detective, part engineer, and part storyteller. They: - Build data pipelines that transform raw data into insights - Create automated processes for consistent analysis - Develop visualizations that make complex patterns understandable - Tell stories that connect data to business decisions"
  },
  {
    "objectID": "AndiStory.html#analytical-engineering-in-practice",
    "href": "AndiStory.html#analytical-engineering-in-practice",
    "title": "Andi’s Data Journey",
    "section": "Analytical Engineering in Practice",
    "text": "Analytical Engineering in Practice\nAnalytical engineering is the bridge between raw data and business value. It involves: - Designing robust data processing workflows - Implementing quality control measures - Creating reusable analysis components - Building scalable solutions for growing data needs\nThis combination of technical skills and critical thinking enables data analysts to turn information into action, helping organizations make better decisions through data."
  },
  {
    "objectID": "AndiStory.html#the-art-of-data-documentation",
    "href": "AndiStory.html#the-art-of-data-documentation",
    "title": "Andi’s Data Journey",
    "section": "The Art of Data Documentation",
    "text": "The Art of Data Documentation\n\nAndi’s Next Challenge: Building a Knowledge Hub\nAfter successfully analyzing the GDPR fines data, Andi faces a new challenge: creating a sustainable documentation system that will help her team and organization maintain and build upon their data knowledge. Let’s follow her journey as she applies DMBOK2 principles and the hub and spoke model to transform raw documentation into actionable knowledge.\n\n\nThe White Hat: Understanding DMBOK2\nAndi begins by gathering facts about DMBOK2’s documentation principles: - Data Governance - Data Architecture - Data Quality - Metadata Management - Data Security\nDocumentation Tools Used: - Knowledge Repository Setup - Metadata Templates - Data Lineage Diagrams - Process Flow Documentation - Security Classification Schema\n\n\nThe Red Hat: Feeling the Documentation Pain\nAs she dives deeper, Andi empathizes with her team’s documentation struggles: - Scattered information across multiple systems - Outdated documentation - Inconsistent formats - Difficulty finding relevant information - Knowledge silos\nHub and Spoke Implementation: - Central Knowledge Hub (Confluence) - Department-specific Spokes - Cross-reference System - Version Control - Access Management\n\n\nThe Black Hat: Critical Documentation Challenges\nAndi identifies potential issues in the current documentation approach: - Information overload - Maintenance overhead - Access control complexity - Version control challenges - Resource constraints\nDMBOK2 Governance Elements: - Documentation Standards - Review Processes - Update Procedures - Quality Metrics - Compliance Requirements\n\n\nThe Yellow Hat: Documentation Opportunities\nShe sees several opportunities for improvement: - Automated documentation generation - Interactive knowledge bases - Collaborative editing - Real-time updates - Integration with existing tools\nKnowledge Management Benefits: - Reduced onboarding time - Improved decision making - Better compliance tracking - Enhanced collaboration - Faster problem resolution\n\n\nThe Green Hat: Creative Documentation Solutions\nAndi develops innovative approaches to documentation: - Interactive data dictionaries - Visual process maps - Automated metadata extraction - Wiki-style knowledge base - Documentation chatbot\nHub and Spoke Features: - Central Documentation Portal - Department Workspaces - Cross-linking System - Search Functionality - Collaboration Tools\n\n\nThe Blue Hat: Documentation Strategy\nFinally, Andi creates a structured plan: - Define documentation standards - Implement hub and spoke model - Establish review processes - Create maintenance schedules - Monitor documentation health\nImplementation Roadmap: - Phase 1: Core Hub Setup - Phase 2: Spoke Development - Phase 3: Integration - Phase 4: Training - Phase 5: Optimization"
  },
  {
    "objectID": "AndiStory.html#implementation-deep-dive",
    "href": "AndiStory.html#implementation-deep-dive",
    "title": "Andi’s Data Journey",
    "section": "Implementation Deep Dive",
    "text": "Implementation Deep Dive\n\nAndi’s Implementation Journey\nAfter establishing the documentation framework and metrics, Andi moves into the implementation phase. Her team needs practical guidance on turning theory into practice. Let’s follow her journey of transforming concepts into working solutions.\n\n\nSetting Up the Development Environment\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#01579b', 'primaryTextColor': '#ffffff', 'primaryBorderColor': '#01579b', 'secondaryColor': '#fff3e0', 'secondaryTextColor': '#000000', 'secondaryBorderColor': '#ff6f00', 'tertiaryColor': '#f5f5f5', 'tertiaryTextColor': '#000000', 'tertiaryBorderColor': '#666' }}}%%\nflowchart LR\n    %% Main Development Environment Node\n    A[Development Environment] --&gt; B[Project Structure]\n    A --&gt; H[Configuration]\n    A --&gt; K[Technology Stack]\n    A --&gt; R[Development Tools]\n    \n    %% Project Structure\n    subgraph Project[Project Structure]\n        direction TB\n        B --&gt; C[data/]\n        C --&gt; C1[raw/]\n        C --&gt; C2[processed/]\n        C --&gt; C3[external/]\n        \n        B --&gt; D[src/]\n        D --&gt; D1[etl/]\n        D --&gt; D2[analysis/]\n        D --&gt; D3[visualization/]\n        D --&gt; D4[utils/]\n        \n        B --&gt; E[tests/]\n        E --&gt; E1[unit/]\n        E --&gt; E2[integration/]\n        E --&gt; E3[e2e/]\n        \n        B --&gt; F[docs/]\n        F --&gt; F1[api/]\n        F --&gt; F2[user_guides/]\n        F --&gt; F3[technical_docs/]\n        \n        B --&gt; G[notebooks/]\n        G --&gt; G1[exploratory/]\n        G --&gt; G2[reporting/]\n    end\n    \n    %% Configuration\n    subgraph Config[Configuration]\n        direction TB\n        H --&gt; I[requirements.txt]\n        H --&gt; J[.env.template]\n        H --&gt; H1[setup.py]\n        H --&gt; H2[config/]\n        H2 --&gt; H2A[dev.yaml]\n        H2 --&gt; H2B[prod.yaml]\n        H2 --&gt; H2C[test.yaml]\n    end\n    \n    %% Technology Stack\n    subgraph Tech[Technology Stack]\n        direction TB\n        K --&gt; L[Backend]\n        L --&gt; L1[\"Python 3.9+&lt;br/&gt;(Core Language)\"]\n        L --&gt; L2[\"FastAPI&lt;br/&gt;(Web Framework)\"]\n        L --&gt; L3[\"SQLAlchemy&lt;br/&gt;(ORM)\"]\n        L --&gt; L4[\"Alembic&lt;br/&gt;(Migrations)\"]\n        \n        K --&gt; M[Data Processing]\n        M --&gt; M1[\"Pandas&lt;br/&gt;(Data Analysis)\"]\n        M --&gt; M2[\"NumPy&lt;br/&gt;(Numerical Ops)\"]\n        M --&gt; M3[\"Scikit-learn&lt;br/&gt;(ML)\"]\n        M --&gt; M4[\"PySpark&lt;br/&gt;(Big Data)\"]\n        \n        K --&gt; N[Visualization]\n        N --&gt; N1[\"Plotly&lt;br/&gt;(Interactive Viz)\"]\n        N --&gt; N2[\"Dash&lt;br/&gt;(Dashboards)\"]\n        N --&gt; N3[\"Streamlit&lt;br/&gt;(Data Apps)\"]\n        N --&gt; N4[\"D3.js&lt;br/&gt;(Custom Viz)\"]\n        \n        K --&gt; O[Database]\n        O --&gt; O1[\"PostgreSQL&lt;br/&gt;(Primary DB)\"]\n        O --&gt; O2[\"Redis&lt;br/&gt;(Caching)\"]\n        O --&gt; O3[\"MongoDB&lt;br/&gt;(Document DB)\"]\n        O --&gt; O4[\"DuckDB&lt;br/&gt;(Analytics)\"]\n        \n        K --&gt; P[Infrastructure]\n        P --&gt; P1[\"Docker&lt;br/&gt;(Containers)\"]\n        P --&gt; P2[\"Kubernetes&lt;br/&gt;(Orchestration)\"]\n        P --&gt; P3[\"AWS&lt;br/&gt;(Cloud)\"]\n        P --&gt; P4[\"GitHub Actions&lt;br/&gt;(CI/CD)\"]\n    end\n    \n    %% Development Tools\n    subgraph Tools[Development Tools]\n        direction TB\n        R --&gt; S[IDEs]\n        S --&gt; S1[\"VS Code&lt;br/&gt;(Primary IDE)\"]\n        S --&gt; S2[\"PyCharm&lt;br/&gt;(Python IDE)\"]\n        S --&gt; S3[\"Jupyter Lab&lt;br/&gt;(Notebooks)\"]\n        \n        R --&gt; T[Version Control]\n        T --&gt; T1[\"Git&lt;br/&gt;(Code Version)\"]\n        T --&gt; T2[\"DVC&lt;br/&gt;(Data Version)\"]\n        T --&gt; T3[\"GitHub&lt;br/&gt;(Repository)\"]\n        \n        R --&gt; U[Documentation]\n        U --&gt; U1[\"Quarto&lt;br/&gt;(Tech Writing)\"]\n        U --&gt; U2[\"Sphinx&lt;br/&gt;(API Docs)\"]\n        U --&gt; U3[\"MkDocs&lt;br/&gt;(Project Docs)\"]\n        \n        R --&gt; V[Quality & Testing]\n        V --&gt; V1[\"pytest&lt;br/&gt;(Testing)\"]\n        V --&gt; V2[\"Black&lt;br/&gt;(Formatting)\"]\n        V --&gt; V3[\"isort&lt;br/&gt;(Import Sort)\"]\n        V --&gt; V4[\"mypy&lt;br/&gt;(Type Check)\"]\n    end\n    \n    %% Styling\n    style A fill:#01579b,stroke:#01579b,stroke-width:2px,color:#ffffff\n    style K fill:#01579b,stroke:#01579b,stroke-width:2px,color:#ffffff\n    style L1,M1,N1,O1,P1,S1,T1,U1,V1 fill:#fff3e0,stroke:#ff6f00,stroke-width:2px,color:#000000\n    \n    %% Subgraph styling\n    style Project fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Config fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Tech fill:#f5f5f5,stroke:#666,stroke-width:2px\n    style Tools fill:#f5f5f5,stroke:#666,stroke-width:2px\n\nclick A \"javascript:void(0);\" \"Click to download as JPEG\"\n\n\n\n\n\n\n\n\nKey Components Legend\nThe highlighted components (in orange) represent the primary tools in each category:\n\n\n\nComponent\nDescription\nRole\n\n\n\n\nL1\nPython 3.9+\nCore programming language for all development\n\n\nM1\nPandas\nPrimary library for data analysis and manipulation\n\n\nN1\nPlotly\nMain tool for creating interactive visualizations\n\n\nO1\nPostgreSQL\nPrimary database for data storage and management\n\n\nP1\nDocker\nContainer platform for consistent environments\n\n\nS1\nVS Code\nPrimary integrated development environment\n\n\nT1\nGit\nVersion control system for code management\n\n\nU1\nQuarto\nTechnical documentation and report generation\n\n\nV1\npytest\nTesting framework for code quality assurance\n\n\n\nThese tools form the foundation of our development stack, each chosen for its reliability, community support, and integration capabilities."
  },
  {
    "objectID": "AndiStory.html#setting-up-the-development-environment-1",
    "href": "AndiStory.html#setting-up-the-development-environment-1",
    "title": "Andi’s Data Journey",
    "section": "Setting Up the Development Environment",
    "text": "Setting Up the Development Environment\nThe development environment is structured to support efficient data analysis and robust software development practices. Here’s a detailed breakdown of each component:\n\nProject Structure\nThe project follows a modular organization: - data/: Manages different data stages (raw, processed, external) - src/: Contains all source code with specialized subdirectories - tests/: Houses different types of tests - docs/: Stores various documentation types - notebooks/: Contains Jupyter notebooks for analysis and reporting\n\n\nConfiguration Management\nEssential configuration files and templates: - requirements.txt: Lists all Python dependencies - .env.template: Environment variable templates - setup.py: Package installation configuration - config/: Environment-specific configurations\n\n\nTechnology Stack\nOur comprehensive stack includes: 1. Backend: Python 3.9+ with FastAPI and SQLAlchemy 2. Data Processing: Pandas, NumPy, Scikit-learn, PySpark 3. Visualization: Plotly, Dash, Streamlit, D3.js 4. Database: PostgreSQL, Redis, MongoDB, DuckDB 5. Infrastructure: Docker, Kubernetes, AWS, GitHub Actions\n\n\nDevelopment Tools\nEssential tools for efficient development: 1. IDEs: VS Code, PyCharm, Jupyter Lab 2. Version Control: Git, DVC, GitHub 3. Documentation: Quarto, Sphinx, MkDocs 4. Quality & Testing: pytest, Black, isort, mypy\n\n\nGetting Started\nTo set up the development environment:\n\nClone the repository:\n\ngit clone &lt;repository-url&gt;\ncd &lt;project-directory&gt;\n\nCreate and activate a virtual environment:\n\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\nInstall dependencies:\n\npip install -r requirements.txt\n\nConfigure environment variables:\n\ncp .env.template .env\n# Edit .env with your specific configurations\n\nInitialize the database:\n\npython src/db/init_db.py\n\nRun tests to verify setup:\n\npytest tests/\n\n\nBest Practices\n\nAlways work in a virtual environment\nKeep dependencies updated\nFollow the coding style guide\nWrite tests for new features\nDocument your code and processes\nUse version control for both code and data"
  },
  {
    "objectID": "AndiStory.html#data-processing-and-analysis",
    "href": "AndiStory.html#data-processing-and-analysis",
    "title": "Andi’s Data Journey",
    "section": "Data Processing and Analysis",
    "text": "Data Processing and Analysis\n\nOverviewTemporal AnalysisGeographic DistributionViolation TypesIndustry Impact\n\n\n\n\n\n### Key Insights\n1. The total GDPR fines collected amount to €787,866,354,349.97, indicating significant financial impact of non-compliance\n2. With 1,461 cases processed, the average fine of €539,265,129.60 suggests a balanced enforcement approach\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n### Key Insights\n1. There is a clear upward trend in fine amounts over time, suggesting increased enforcement activity\n2. Seasonal patterns show peaks in Q4 of each year, possibly due to year-end regulatory reviews\n\n\n\n\n\n\n                            \n                                            \n\n\n\n### Key Insights\n1. Germany leads in total fine amounts, followed by France and Spain, reflecting stricter enforcement\n2. The top 3 countries account for over 60% of total fines, indicating concentrated enforcement in major EU economies\n\n\n\n\n\n\n                            \n                                            \n\n\n\n### Key Insights\n1. Article 5 (Data Processing Principles) is the most frequently violated, highlighting challenges in basic GDPR compliance\n2. Articles 5, 6, and 7 account for 75% of violations, suggesting focus areas for compliance training\n\n\n\n\n\n\n                            \n                                            \n\n\n\n### Key Insights\n1. Technology sector faces the highest fines, reflecting the data-intensive nature of their operations\n2. Financial and Healthcare sectors show similar fine patterns, indicating comparable regulatory scrutiny"
  },
  {
    "objectID": "AndiStory.html#building-the-data-pipeline",
    "href": "AndiStory.html#building-the-data-pipeline",
    "title": "Andi’s Data Journey",
    "section": "Building the Data Pipeline",
    "text": "Building the Data Pipeline\n\nETL Process for GDPR Fines\n\n\n\nETL Workflow Components\n\n\n\n\n\n\n\nStage\nImplementation\nDescription\n\n\n\n\nData Collection\nWeb Scraping + API\nCollect fines data from enforcement tracker and official sources\n\n\nData Validation\nSchema Validation\nValidate data against predefined schemas and rules\n\n\nData Transformation\nData Normalization\nTransform raw data into normalized database format\n\n\nData Loading\nDatabase Loading\nLoad processed data into PostgreSQL database\n\n\nData Quality Check\nAutomated Testing\nRun automated quality checks and validations\n\n\nDocumentation Update\nAuto-Documentation\nUpdate documentation with new data lineage\n\n\nNotification System\nAlert System\nSend notifications for updates and issues"
  },
  {
    "objectID": "AndiStory.html#the-argh-framework",
    "href": "AndiStory.html#the-argh-framework",
    "title": "Andi’s Data Journey",
    "section": "The ARGH Framework",
    "text": "The ARGH Framework\n\nUnderstanding ARGH\nAfter implementing the core systems, Andi realizes that defining “good” is crucial for sustainable success. She develops the ARGH framework:\n\nActionable: Insights that drive decisions\nReliable: Trustworthy and consistent data\nGoverned: Controlled and compliant processes\nHarmonized: Integrated and synchronized systems\n\n\n\n\nARGH Framework Components\n\n\nPillar\nKey Metrics\nSuccess Criteria\n\n\n\n\nActionable\nDecision Impact Rate\nEvery insight leads to clear action items and measurable outcomes\n\n\nReliable\nData Quality Score\nData consistency above 99%, with full lineage and validation\n\n\nGoverned\nCompliance Rate\nComplete audit trails and policy compliance across all processes\n\n\nHarmonized\nIntegration Success\nSeamless data flow between all systems with zero manual intervention"
  },
  {
    "objectID": "AndiStory.html#the-never-ending-journey",
    "href": "AndiStory.html#the-never-ending-journey",
    "title": "Andi’s Data Journey",
    "section": "The Never-Ending Journey",
    "text": "The Never-Ending Journey\nAndi’s journey teaches us that “good” is not a destination but a continuous journey of improvement. The ARGH framework provides a compass for this journey:\n\nActionable: Every insight should drive meaningful change\nReliable: Trust is built on consistent quality\nGoverned: Control enables freedom\nHarmonized: Integration creates value\n\nRemember: &gt; “The goal is not to be perfect at everything, but to be excellent at what matters most to your organization.”\nThe future of data analytics is not just about technology—it’s about creating value through actionable insights, reliable systems, governed processes, and harmonized operations. As Andi would say, “ARGH!” might sound like frustration, but it’s actually the sound of excellence in the making."
  },
  {
    "objectID": "AndiStory.html#methodologies-and-frameworks",
    "href": "AndiStory.html#methodologies-and-frameworks",
    "title": "Andi’s Data Journey",
    "section": "Methodologies and Frameworks",
    "text": "Methodologies and Frameworks\n\nDMAIC\nDefinition: A data-driven improvement cycle used to improve, optimize and stabilize business processes and designs. - Define: Identify the problem and project goals - Measure: Collect data to understand the current state - Analyze: Identify root causes of problems - Improve: Implement and verify solutions - Control: Maintain the improvements\n\n\nSix Thinking Hats\nDefinition: A thinking tool for group discussion and individual thinking involving six colored hats: - White Hat: Facts and information - Red Hat: Feelings and intuition - Black Hat: Critical judgment - Yellow Hat: Positive aspects - Green Hat: Creativity - Blue Hat: Process control\n\n\nARGH Framework\nDefinition: A framework for achieving excellence in data analytics: - Actionable: Insights that drive decisions - Reliable: Trustworthy and consistent data - Governed: Controlled and compliant processes - Harmonized: Integrated and synchronized systems\n\n\nDMBOK2\nDefinition: Data Management Body of Knowledge, a comprehensive framework for data management: - Data Governance - Data Architecture - Data Quality - Metadata Management - Data Security"
  },
  {
    "objectID": "AndiStory.html#technical-terms",
    "href": "AndiStory.html#technical-terms",
    "title": "Andi’s Data Journey",
    "section": "Technical Terms",
    "text": "Technical Terms\n\nData Quality Dimensions\n\nCompleteness: The degree to which all required data is present\nAccuracy: The degree to which data correctly describes the real-world object or event\nConsistency: The degree to which data maintains consistency across the data set\nTimeliness: The degree to which data represents reality from the required point in time\nRelevance: The degree to which data is applicable and helpful for the task at hand\n\n\n\nAnalytical Engineering\nDefinition: The practice of designing, implementing, and maintaining systems that transform raw data into actionable insights: - Data Pipeline Development - Quality Control Implementation - Process Automation - Scalable Solutions Design"
  },
  {
    "objectID": "AndiStory.html#books-and-publications",
    "href": "AndiStory.html#books-and-publications",
    "title": "Andi’s Data Journey",
    "section": "Books and Publications",
    "text": "Books and Publications\n\nDe Bono, E. (1985). Six Thinking Hats: An Essential Approach to Business Management. Little, Brown and Company.\nDAMA International. (2017). DAMA-DMBOK: Data Management Body of Knowledge (2nd Edition). Technics Publications.\nPyzdek, T., & Keller, P. (2018). The Six Sigma Handbook (5th Edition). McGraw-Hill Education."
  },
  {
    "objectID": "AndiStory.html#online-resources",
    "href": "AndiStory.html#online-resources",
    "title": "Andi’s Data Journey",
    "section": "Online Resources",
    "text": "Online Resources\n\nInternational Organization for Standardization. (2015). ISO 9001:2015 Quality Management Systems. https://www.iso.org/standard/62085.html\nGDPR.eu. (2018). Complete guide to GDPR compliance. https://gdpr.eu/\nData Management Association (DAMA). https://www.dama.org/"
  },
  {
    "objectID": "AndiStory.html#industry-standards-and-frameworks",
    "href": "AndiStory.html#industry-standards-and-frameworks",
    "title": "Andi’s Data Journey",
    "section": "Industry Standards and Frameworks",
    "text": "Industry Standards and Frameworks\n\nCOBIT (Control Objectives for Information and Related Technologies)\n\nFramework for IT management and IT governance\n\nITIL (Information Technology Infrastructure Library)\n\nSet of detailed practices for IT service management\n\nCRISP-DM (Cross-Industry Standard Process for Data Mining)\n\nIndustry-standard process model for data mining projects"
  },
  {
    "objectID": "AndiStory.html#additional-reading",
    "href": "AndiStory.html#additional-reading",
    "title": "Andi’s Data Journey",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nAgile Data Warehouse Design\n\nLawrence Corr & Jim Stagnitto\nCollaborative dimensional modeling\n\nData Quality Assessment Framework\n\nWorld Bank Group\nStatistical capacity building\n\nThe Data Warehouse Toolkit\n\nRalph Kimball & Margy Ross\nDimensional modeling fundamentals"
  },
  {
    "objectID": "AndiStory.html#interactive-gdpr-fines-dashboard",
    "href": "AndiStory.html#interactive-gdpr-fines-dashboard",
    "title": "Andi’s Data Journey",
    "section": "Interactive GDPR Fines Dashboard",
    "text": "Interactive GDPR Fines Dashboard\n\nOverviewTemporal AnalysisGeographic DistributionViolation TypesIndustry Impact\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nTotal Fines\n€87,330,000.00\n\n\nTotal Cases\n100\n\n\nAverage Fine\n€873,300.00"
  },
  {
    "objectID": "AndiStory.html#gdpr-fines-analysis-summary",
    "href": "AndiStory.html#gdpr-fines-analysis-summary",
    "title": "Andi’s Data Journey",
    "section": "GDPR Fines Analysis Summary",
    "text": "GDPR Fines Analysis Summary\n\nOverview\nThe analysis of GDPR fines reveals several key patterns and insights:\n\nTemporal Trends\n\nIncreasing trend in enforcement actions since GDPR implementation\nSeasonal variations in fine amounts and frequency\nNotable increase in large fines over time\n\nGeographic Distribution\n\nHigher enforcement activity in certain jurisdictions\nVarying fine amounts across different countries\nRegional patterns in types of violations\n\nFine Amount Patterns\n\nWide range of fine amounts, from minor to significant penalties\nCorrelation between violation severity and fine amounts\nIndustry-specific patterns in fine amounts\n\nCommon Violations\n\nMost frequently violated GDPR articles\nPatterns in violation types across industries\nRecurring compliance challenges\n\nIndustry Impact\n\nSector-specific compliance challenges\nVariation in fine severity across industries\nIndustry-specific risk patterns\n\nSeverity Analysis\n\nCorrelation between violation types and fine amounts\nIndustry-specific severity patterns\nImpact of multiple violations on fine amounts\n\n\n\nDisclaimer: The data used in these visualizations is generated dummy data for illustrative purposes only. For access to real GDPR enforcement data and up-to-date information about fines, please visit GDPR Enforcement Tracker."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lean Analytics Journey",
    "section": "",
    "text": "“If you write a problem down clearly, then the matter is half solved.”\n— Kidlens Law"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Lean Analytics Journey",
    "section": "Books",
    "text": "Books\n\nBook 1 - Kidlens Law\nExplore the fundamentals of data quality and analysis through the lens of Kidlens Law.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nData Quality Assessment\nGDPR Fines Analysis\nProblem Definition\n\n\n\n\n\nBook 2 - Documentation Journey\nLearn about the importance of documentation and knowledge management in data projects.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nDMBOK2 Principles\nHub and Spoke Model\nDocumentation Best Practices\n\n\n\n\n\nBook 3 - Getting Your Hands Dirty\nDive into practical implementation and hands-on data engineering.\n\n\n\n\n\n\nKey Topics\n\n\n\n\nImplementation Guide\nCode Examples\nBest Practices\n\n\n\n\n\nBook 4 - ARGH Framework\nDiscover the ARGH framework for defining and achieving excellence in data projects.\n\n\n\n\n\n\nFramework Components\n\n\n\n\nActionable: Insights that drive decisions\nReliable: Trustworthy and consistent data\nGoverned: Controlled and compliant processes\nHarmonized: Integrated and synchronized systems\n\n\n\n\n\nAndi’s Complete Story\nFollow Andi’s journey through all four books in a single narrative."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Lean Analytics Journey",
    "section": "Getting Started",
    "text": "Getting Started\n\nStart with Book 1\nExplore Documentation in Book 2\nGet Practical in Book 3\nDefine Excellence in Book 4"
  },
  {
    "objectID": "index.html#key-topics-3",
    "href": "index.html#key-topics-3",
    "title": "Lean Analytics Journey",
    "section": "Key Topics",
    "text": "Key Topics\n\nData Quality → Book 1\nDocumentation → Book 2\nImplementation → Book 3\nExcellence → Book 4\n\nRemember: The journey to excellence is continuous. Each book represents a step forward in mastering data analytics and documentation practices."
  }
]